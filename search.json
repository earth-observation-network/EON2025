[
  {
    "objectID": "ml_session/ML_AOA.html",
    "href": "ml_session/ML_AOA.html",
    "title": "Machine learning for remote sensing applications",
    "section": "",
    "text": "Slides and data are on https://github.com/earth-observation-network/EON2025/tree/main/ml_session",
    "crumbs": [
      "Block 3: Machine Learning",
      "Machine learning for remote sensing applications"
    ]
  },
  {
    "objectID": "ml_session/ML_AOA.html#how-to-start",
    "href": "ml_session/ML_AOA.html#how-to-start",
    "title": "Machine learning for remote sensing applications",
    "section": "How to start",
    "text": "How to start\nFor this tutorial we need the terra package for processing of the satellite data as well as the caret package as a wrapper for machine learning (here: randomForest) algorithms. Sf is used for handling of the training data available as vector data (polygons). Mapview is used for spatial visualization of the data. CAST will be used to account for spatial dependencies during model validation as well as for the estimation of the AOA.\n\nlibrary(CAST)\nlibrary(terra)\nlibrary(caret)\nlibrary(randomForest)\nlibrary(mapview)\nlibrary(sf)\nlibrary(CAST)\nlibrary(tmap)",
    "crumbs": [
      "Block 3: Machine Learning",
      "Machine learning for remote sensing applications"
    ]
  },
  {
    "objectID": "ml_session/ML_AOA.html#data-preparation",
    "href": "ml_session/ML_AOA.html#data-preparation",
    "title": "Machine learning for remote sensing applications",
    "section": "Data preparation",
    "text": "Data preparation\nTo start with, let’s load and explore the remote sensing raster data as well as the vector data that include the training sites.\n\nRaster data (predictor variables)\n\nmof_sen &lt;- rast(\"data/sentinel_uniwald.grd\")\nprint(mof_sen)\n\nclass       : SpatRaster \ndimensions  : 522, 588, 10  (nrow, ncol, nlyr)\nresolution  : 10, 10  (x, y)\nextent      : 474200, 480080, 5629540, 5634760  (xmin, xmax, ymin, ymax)\ncoord. ref. : +proj=utm +zone=32 +datum=WGS84 +units=m +no_defs \nsource      : sentinel_uniwald.grd \nnames       : T32UM~1_B02, T32UM~1_B03, T32UM~1_B04, T32UM~1_B05, T32UM~1_B06, T32UM~1_B07, ... \nmin values  :         723,         514,         294,    341.8125,    396.9375,    440.8125, ... \nmax values  :        8325,        9087,       13810,   7368.7500,   8683.8125,   9602.3125, ... \n\n\nThe raster data contain a subset of the optical data from Sentinel-2 (see band information here: https://en.wikipedia.org/wiki/Sentinel-2) given in scaled reflectances (B02-B11). In addition,the NDVI was calculated. Let’s plot the data to get an idea how the variables look like.\n\nplot(mof_sen)\n\n\n\n\n\n\n\nplotRGB(mof_sen,r=3,g=2,b=1,stretch=\"lin\")\n\n\n\n\n\n\n\n\n\n\nVector data (Response variable)\nThe vector file is read as sf object. It contains the training sites that will be regarded here as a ground truth for the land cover classification.\n\ntrainSites &lt;- read_sf(\"data/trainingsites_LUC.gpkg\")\n\nUsing mapview we can visualize the aerial image channels in the geographical context and overlay it with the polygons. Click on the polygons to see which land cover class is assigned to a respective polygon.\n\nmapview(mof_sen[[1]], map.types = \"Esri.WorldImagery\") +\n  mapview(trainSites)\n\n\n\n\n\n\n\nDraw training samples and extract raster information\nIn order to train a machine learning model between the spectral properties and the land cover class, we first need to create a data frame that contains the predictor variables at the location of the training sites as well as the corresponding class information. However, using each pixel overlapped by a polygon would lead to a overly huge dataset, therefore, we first draw training samples from the polygon. Let’s use 1000 randomly sampled (within the polygons) pixels as training data set.\n\ntrainlocations &lt;- st_sample(trainSites,1000)\ntrainlocations &lt;- st_join(st_sf(trainlocations), trainSites)\nmapview(trainlocations)\n\n\n\n\n\nNext, we can extract the raster values for these locations. The resulting data frame contains the predictor variables for each training location that we can merged with the information on the land cover class from the sf object.\n\ntrainDat &lt;- extract(mof_sen, trainlocations, df=TRUE)\ntrainDat &lt;- data.frame(trainDat, trainlocations)\nhead(trainDat)\n\n  ID T32UMB_20170510T103031_B02 T32UMB_20170510T103031_B03\n1  1                       1243                       1169\n2  2                        885                        784\n3  3                        827                        785\n4  4                        915                        848\n5  5                        804                        689\n6  6                        923                       1054\n  T32UMB_20170510T103031_B04 T32UMB_20170510T103031_B05\n1                        926                   1143.500\n2                        568                    935.000\n3                        501                   1077.750\n4                        731                   1147.438\n5                        445                    901.500\n6                        733                   1260.812\n  T32UMB_20170510T103031_B06 T32UMB_20170510T103031_B07\n1                   2252.312                   2552.812\n2                   2242.188                   2766.938\n3                   2148.500                   2483.250\n4                   1893.750                   2129.688\n5                   1747.125                   2002.500\n6                   3734.625                   4639.625\n  T32UMB_20170510T103031_B08 T32UMB_20170510T103031_B11\n1                       2925                   1878.812\n2                       2644                   1523.062\n3                       2385                   1359.500\n4                       2114                   1788.312\n5                       1812                   1183.062\n6                       4769                   1205.312\n  T32UMB_20170510T103031_B12      NDVI id  LN   Type           trainlocations\n1                  1303.5625 0.5190859 64 503 Wasser POINT (476609.7 5632431)\n2                   824.1250 0.6463263 NA 106 Felder POINT (478523.8 5631097)\n3                   635.2500 0.6528066 NA   2  Buche POINT (477490.4 5632300)\n4                  1003.6250 0.4861160 NA   2  Buche POINT (477605.7 5631966)\n5                   558.5625 0.6056712 NA   1  Eiche POINT (476782.9 5632857)\n6                   523.8125 0.7335514 NA 102 Felder POINT (478600.8 5631513)",
    "crumbs": [
      "Block 3: Machine Learning",
      "Machine learning for remote sensing applications"
    ]
  },
  {
    "objectID": "ml_session/ML_AOA.html#model-training",
    "href": "ml_session/ML_AOA.html#model-training",
    "title": "Machine learning for remote sensing applications",
    "section": "Model training",
    "text": "Model training\n\nPredictors and response\nFor model training we need to define the predictor and response variables. As predictors we can use basically all information from the raster stack as we might assume they could all be meaningful for the differentiation between the land cover classes. As response variable we use the “Label” column of the data frame.\n\npredictors &lt;- names(mof_sen)\nresponse &lt;- \"Type\"\n\n\n\nA first “default” model\nWe then train a Random Forest model to lean how the classes can be distinguished based on the predictors (note: other algorithms would work as well. See https://topepo.github.io/caret/available-models.html for a list of algorithms available in caret). Caret’s train function is doing this job.\nSo let’s see how we can then train a “default” random forest model. We specify “rf” as method, indicating that a Random Forest is applied. We reduce the number of trees (ntree) to 75 to speed things up. Note that usually a larger number (&gt;250) is appropriate.\n\nmodel &lt;- train(trainDat[,predictors],\n               trainDat[,response],\n               method=\"rf\",\n               ntree=75)\nmodel\n\nRandom Forest \n\n1000 samples\n  10 predictor\n  10 classes: 'Buche', 'Duglasie', 'Eiche', 'Felder', 'Fichte', 'Laerche', 'Siedlung', 'Strasse', 'Wasser', 'Wiese' \n\nNo pre-processing\nResampling: Bootstrapped (25 reps) \nSummary of sample sizes: 1000, 1000, 1000, 1000, 1000, 1000, ... \nResampling results across tuning parameters:\n\n  mtry  Accuracy   Kappa    \n   2    0.8309292  0.7895740\n   6    0.8341677  0.7942385\n  10    0.8271947  0.7855348\n\nAccuracy was used to select the optimal model using the largest value.\nThe final value used for the model was mtry = 6.\n\n\nTo perform the classification we can then use the trained model and apply it to each pixel of the raster stack using the predict function.\n\nprediction &lt;- predict(mof_sen,model)\n\nThen we can then create a map with meaningful colors of the predicted land cover using the tmap package.\n\ncols &lt;- rev(c(\"palegreen\", \"blue\", \"grey\", \"red\", \"lightgreen\", \"forestgreen\", \"beige\",\"brown\",\"darkgreen\",\"yellowgreen\"))\n\ntm_shape(prediction) +\n  tm_raster(palette = cols,title = \"LUC\")+\n  tm_scale_bar(bg.color=\"white\",bg.alpha=0.75)+\n  tm_layout(legend.bg.color = \"white\",\n            legend.bg.alpha = 0.75)\n\n\n\n\n\n\n\n\nBased on this we can now discuss more advanced aspects of cross-validation for performance assessment as well as spatial variable selection strategies.\n\n\nModel training with spatial CV and variable selection\nBefore starting model training we can specify some control settings using trainControl. For hyperparameter tuning (mtry) as well as for error assessment we use a spatial cross-validation. Here, the training data are split into 5 folds by trying to resemble the geographic distance distribution required when predicting the entire area from the trainign data,\n\nindices &lt;- knndm(trainlocations,mof_sen,k=5)\ngd &lt;- geodist(trainlocations,mof_sen,cvfolds = indices$indx_train)\nplot(gd)+ scale_x_log10(labels=round)\n\n\n\n\n\n\n\nctrl &lt;- trainControl(method=\"cv\", \n                     index = indices$indx_train,\n                     indexOut = indices$indx_test,\n                     savePredictions = TRUE)\n\nModel training is then again performed using caret’s train function. However we use a wrapper around it that is selecting the predictor variables which are relevant for making predictions to new spatial locations (forward feature selection, fss). We use the Kappa index as metric to select the best model.\n\n# train the model\nset.seed(100)\nmodel &lt;- ffs(trainDat[,predictors],\n             trainDat[,response],\n             method=\"rf\",\n             metric=\"Kappa\",\n             trControl=ctrl,\n             importance=TRUE,\n             ntree=100,\n             verbose=FALSE)\n\n\nprint(model)\n\nSelected Variables: \nT32UMB_20170510T103031_B05 T32UMB_20170510T103031_B06 T32UMB_20170510T103031_B02 T32UMB_20170510T103031_B11 T32UMB_20170510T103031_B12\n---\nRandom Forest \n\n1000 samples\n   5 predictor\n  10 classes: 'Buche', 'Duglasie', 'Eiche', 'Felder', 'Fichte', 'Laerche', 'Siedlung', 'Strasse', 'Wasser', 'Wiese' \n\nNo pre-processing\nResampling: Cross-Validated (10 fold) \nSummary of sample sizes: 768, 791, 712, 854, 875 \nResampling results across tuning parameters:\n\n  mtry  Accuracy   Kappa    \n  2     0.7562707  0.5637556\n  3     0.7492029  0.5575426\n  5     0.7344652  0.5369017\n\nKappa was used to select the optimal model using the largest value.\nThe final value used for the model was mtry = 2.\n\nplot(varImp(model))\n\n\n\n\n\n\n\n\n\n\nModel validation\nWhen we print the model (see above) we get a summary of the prediction performance as the average Kappa and Accuracy of the three spatial folds. Looking at all cross-validated predictions together we can get the “global” model performance.\n\n# get all cross-validated predictions:\ncvPredictions &lt;- model$pred[model$pred$mtry==model$bestTune$mtry,]\n# calculate cross table:\ntable(cvPredictions$pred,cvPredictions$obs)\n\n          \n           Buche Duglasie Eiche Felder Fichte Laerche Siedlung Strasse Wasser\n  Buche        4        0     2      2     11       0        0       0      0\n  Duglasie     0        0     0      1      3       0        0       0      0\n  Eiche        0        0     1      2      2       0        0       0      0\n  Felder       6        0     0    259      0       0        0       6      0\n  Fichte       0        0     0      3      0       0        0       0      0\n  Laerche      0        0     0      0      0       0        0       0      0\n  Siedlung     0        0     0      0      0       0        0       0      0\n  Strasse      0        0     0     16      0       0        0      25      0\n  Wasser       0        0     0      2      3       0        0       0      0\n  Wiese        0        0     0     30      0       0        0       9      0\n          \n           Wiese\n  Buche        0\n  Duglasie     0\n  Eiche        1\n  Felder       6\n  Fichte       0\n  Laerche      0\n  Siedlung     0\n  Strasse      8\n  Wasser       0\n  Wiese       78\n\n\n\n\nVisualize the final model predictions\n\nprediction &lt;- predict(mof_sen,model)\ncols &lt;- rev(c(\"palegreen\", \"blue\", \"grey\", \"red\", \"lightgreen\", \"forestgreen\", \"beige\",\"brown\",\"darkgreen\",\"yellowgreen\"))\n\ntm_shape(prediction) +\n  tm_raster(palette = cols,title = \"LUC\")+\n  tm_scale_bar(bg.color=\"white\",bg.alpha=0.75)+\n  tm_layout(legend.bg.color = \"white\",\n            legend.bg.alpha = 0.75)",
    "crumbs": [
      "Block 3: Machine Learning",
      "Machine learning for remote sensing applications"
    ]
  },
  {
    "objectID": "ml_session/ML_AOA.html#area-of-applicability",
    "href": "ml_session/ML_AOA.html#area-of-applicability",
    "title": "Machine learning for remote sensing applications",
    "section": "Area of Applicability",
    "text": "Area of Applicability\nWe have seen that technically, the trained model can be applied to the entire area of interest (and beyond…as long as the sentinel predictors are available which they are, even globally). But we should assess if we SHOULD apply our model to the entire area. The model should only be applied to locations that feature predictor properties that are comparable to those of the training data. If dissimilarity to the training data is larger than the dissimmilarity within the training data, the model should not be applied to this location.\n\nAOA &lt;- aoa(mof_sen,model,LPD=TRUE, verbose=FALSE)\nplot(AOA$AOA)\n\n\n\n\n\n\n\n\nThe result of the aoa function has two layers: the dissimilarity index (DI) and the area of applicability (AOA). The DI can take values from 0 to Inf, where 0 means that a location has predictor properties that are identical to properties observed in the training data. With increasing values the dissimilarity increases. The AOA has only two values: 0 and 1. 0 means that a location is outside the area of applicability, 1 means that the model is inside the area of applicability. As an option, we cal also calculate the Local Point Density (LPD), which tells us, for a prediction location, how MANY similar training data points were used during modle training.\n\nError profiles\nLet’s assume there is a relationship between the density of training data points in the predictor space (LPD) and the model performance. Let’s analyze that and use that to predict the prediction performance.\n\nplot(AOA$LPD)\n\n\n\n\n\n\n\nep &lt;- errorProfiles(model,AOA,variable=\"LPD\")\nplot(ep)\n\n\n\n\n\n\n\nplot(predict(AOA$LPD,ep))",
    "crumbs": [
      "Block 3: Machine Learning",
      "Machine learning for remote sensing applications"
    ]
  },
  {
    "objectID": "ml_session/ML_AOA.html#prepare-data",
    "href": "ml_session/ML_AOA.html#prepare-data",
    "title": "Machine learning for remote sensing applications",
    "section": "Prepare data",
    "text": "Prepare data\n\nmof_sen &lt;- rast(\"data/sentinel_uniwald.grd\")\nLAIdat &lt;- st_read(\"data/trainingsites_LAI.gpkg\")\n\nReading layer `trainingsites_LAI' from data source \n  `/home/hanna/Documents/Github/earth-observation-network/EON2025/ml_session/data/trainingsites_LAI.gpkg' \n  using driver `GPKG'\nSimple feature collection with 67 features and 10 fields\nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: 476350 ymin: 5631537 xmax: 478075 ymax: 5632765\nProjected CRS: WGS 84 / UTM zone 32N\n\ntrainDat &lt;- extract(mof_sen,LAIdat,na.rm=TRUE)\ntrainDat$LAI &lt;- LAIdat$LAI\n\n\nmeanmodel &lt;- mof_sen[[1]]\nvalues(meanmodel) &lt;- mean(trainDat$LAI)\nplot(meanmodel)\n\n\n\n\n\n\n\nrandommodel &lt;- mof_sen[[1]]\nvalues(randommodel)&lt;- runif(ncell(randommodel),min = 0,4)\n\nplot(randommodel)",
    "crumbs": [
      "Block 3: Machine Learning",
      "Machine learning for remote sensing applications"
    ]
  },
  {
    "objectID": "ml_session/ML_AOA.html#a-simple-linear-model",
    "href": "ml_session/ML_AOA.html#a-simple-linear-model",
    "title": "Machine learning for remote sensing applications",
    "section": "A simple linear model",
    "text": "A simple linear model\nAs a simple first approach we might develop a linear model. Let’s assume a linear relationship between the NDVI and the LAI\n\nplot(trainDat$NDVI,trainDat$LAI)\nmodel_lm &lt;- lm(LAI~NDVI,data=trainDat)\nsummary(model_lm)\n\n\nCall:\nlm(formula = LAI ~ NDVI, data = trainDat)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-1.87314 -0.52143 -0.03363  0.63668  2.25252 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)   \n(Intercept)  -0.8518     1.4732  -0.578  0.56515   \nNDVI          6.8433     2.3160   2.955  0.00435 **\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.8887 on 65 degrees of freedom\nMultiple R-squared:  0.1184,    Adjusted R-squared:  0.1049 \nF-statistic: 8.731 on 1 and 65 DF,  p-value: 0.004354\n\nabline(model_lm,col=\"red\")\n\n\n\n\n\n\n\nprediction_LAI &lt;- predict(mof_sen,model_lm,na.rm=T)\nplot(prediction_LAI)\n\n\n\n\n\n\n\nlimodelpred &lt;- -0.8518+mof_sen$NDVI*6.8433\nmapview(limodelpred)",
    "crumbs": [
      "Block 3: Machine Learning",
      "Machine learning for remote sensing applications"
    ]
  },
  {
    "objectID": "ml_session/ML_AOA.html#the-machine-learning-way",
    "href": "ml_session/ML_AOA.html#the-machine-learning-way",
    "title": "Machine learning for remote sensing applications",
    "section": "The machine learning way",
    "text": "The machine learning way\n\nDefine CV folds\nLet’s use the NNDM cross-validation approach.\n\nnndm_folds &lt;- knndm(LAIdat,mof_sen,k=3)\n\nLet’s explore the geodistance\n\ngd &lt;- geodist(LAIdat,mof_sen,cvfolds = nndm_folds$indx_test)\nplot(gd)\n\n\n\n\n\n\n\n\n\n\nModel training\n\nctrl &lt;- trainControl(method=\"cv\",\n                     index=nndm_folds$indx_train,\n                     indexOut = nndm_folds$indx_test,\n                    savePredictions = \"all\")\n\n\nmodel &lt;- ffs(trainDat[,predictors],\n             trainDat$LAI,\n             method=\"rf\",\n             trControl = ctrl,\n             importance=TRUE,\n             verbose=FALSE)\n\n\nmodel\n\nSelected Variables: \nT32UMB_20170510T103031_B07 T32UMB_20170510T103031_B08 NDVI\n---\nRandom Forest \n\n67 samples\n 3 predictor\n\nNo pre-processing\nResampling: Cross-Validated (10 fold) \nSummary of sample sizes: 41, 44, 49 \nResampling results across tuning parameters:\n\n  mtry  RMSE       Rsquared   MAE      \n  2     0.8477533  0.2158171  0.7015259\n  3     0.8612522  0.2068717  0.7236176\n\nRMSE was used to select the optimal model using the smallest value.\nThe final value used for the model was mtry = 2.\n\n\n\n\nLAI prediction\nLet’s then use the trained model for prediction.\n\nLAIprediction &lt;- predict(mof_sen,model)\nplot(LAIprediction)\n\n\n\n\n\n\n\n\n\nQuestion?! Why does it look so different than the linear model?\n\n\n\nAOA estimation\n\nAOA &lt;- aoa(mof_sen,model,LPD = TRUE, verbose=FALSE)\nplot(AOA$AOA)\n\n\n\n\n\n\n\nplot(AOA$LPD)\n\n\n\n\n\n\n\n\n\n\nError profiles\nLet’s assume there is a relationship between the density of training data points in the predictor space (LPD) and the model performance. Let’s analyze that and use that to predict the prediction performance.\n\nep &lt;- errorProfiles(model,AOA,variable=\"DI\")\nplot(ep)\n\n\n\n\n\n\n\nplot(predict(AOA$DI,ep))",
    "crumbs": [
      "Block 3: Machine Learning",
      "Machine learning for remote sensing applications"
    ]
  },
  {
    "objectID": "block6/index.html",
    "href": "block6/index.html",
    "title": "Understanding spatial patterns: how to measure and compare them",
    "section": "",
    "text": "The workshop slides are at https://jakubnowosad.com/eon2025/.\n\n\n\nParticipants are expected to have a working recent version of R and RStudio installed, along with several R packages listed below.\n\nR: https://cloud.r-project.org/\nRStudio: https://posit.co/download/rstudio-desktop/#download\n\ninstall.packages(\"remotes\")\npkg_list = c(\"terra\", \"sf\", \"landscapemetrics\", \"motif\", \"tidyr\", \"dplyr\")\nremotes::install_cran(pkg_list)\n\n\n\nThe slides are accompanied by practical exercises. The best way to get them is to download the exercises repository as a ZIP file from https://github.com/Nowosad/eon2025-exercises/archive/refs/heads/main.zip and unpack it on your computer. Then, you can open the .Rproj file and start working on the exercises in RStudio.",
    "crumbs": [
      "Block 5: Spatial patterns",
      "Understanding spatial patterns: how to measure and compare them"
    ]
  },
  {
    "objectID": "block6/index.html#slides",
    "href": "block6/index.html#slides",
    "title": "Understanding spatial patterns: how to measure and compare them",
    "section": "",
    "text": "The workshop slides are at https://jakubnowosad.com/eon2025/.",
    "crumbs": [
      "Block 5: Spatial patterns",
      "Understanding spatial patterns: how to measure and compare them"
    ]
  },
  {
    "objectID": "block6/index.html#prerequisites",
    "href": "block6/index.html#prerequisites",
    "title": "Understanding spatial patterns: how to measure and compare them",
    "section": "",
    "text": "Participants are expected to have a working recent version of R and RStudio installed, along with several R packages listed below.\n\nR: https://cloud.r-project.org/\nRStudio: https://posit.co/download/rstudio-desktop/#download\n\ninstall.packages(\"remotes\")\npkg_list = c(\"terra\", \"sf\", \"landscapemetrics\", \"motif\", \"tidyr\", \"dplyr\")\nremotes::install_cran(pkg_list)",
    "crumbs": [
      "Block 5: Spatial patterns",
      "Understanding spatial patterns: how to measure and compare them"
    ]
  },
  {
    "objectID": "block6/index.html#exercises",
    "href": "block6/index.html#exercises",
    "title": "Understanding spatial patterns: how to measure and compare them",
    "section": "",
    "text": "The slides are accompanied by practical exercises. The best way to get them is to download the exercises repository as a ZIP file from https://github.com/Nowosad/eon2025-exercises/archive/refs/heads/main.zip and unpack it on your computer. Then, you can open the .Rproj file and start working on the exercises in RStudio.",
    "crumbs": [
      "Block 5: Spatial patterns",
      "Understanding spatial patterns: how to measure and compare them"
    ]
  },
  {
    "objectID": "block4_5/mc_2025_pipemodel_explain.html",
    "href": "block4_5/mc_2025_pipemodel_explain.html",
    "title": "PipeModel Idealized valley microclimate sandbox",
    "section": "",
    "text": "The PipeModel is a deliberately idealized yet physically plausible valley scenario. It distills terrain to the essentials (parabolic cross-valley profile) and optional features (left-side hill, right-side pond or hollow), so that dominant microclimate drivers become visible and testable:\n\nRadiation via terrain exposure cos(i) from slope & aspect\nElevation: daytime negative lapse; pre-dawn weak inversion\nCold-air pooling along the valley axis (Gaussian trough)\nSurface type / land-cover (grass / forest / water / bare soil / maize) alters heating, shading, roughness and nocturnal behaviour\n\nYou can sample synthetic stations, train interpolators (IDW, Kriging variants, RF, GAM), and assess them with spatial LBO-CV.\n\n🔧 This document keeps the previous behaviour but extends the physics with a modular land-cover layer that feeds into both daytime and night fields."
  },
  {
    "objectID": "block4_5/mc_2025_pipemodel_explain.html#d.1-generated-rasters-derived-fields",
    "href": "block4_5/mc_2025_pipemodel_explain.html#d.1-generated-rasters-derived-fields",
    "title": "PipeModel Idealized valley microclimate sandbox",
    "section": "2.1 D.1 Generated rasters & derived fields",
    "text": "2.1 D.1 Generated rasters & derived fields\n\n\n\n\n\n\n\n\n\nName\nUnit\nWhat it is\nHow it’s built\n\n\n\n\nE (elev)\nm\nGround elevation\nParabolic “half-pipe” across y; + optional hill; − optional pond/hollow\n\n\nslp, asp\nrad\nSlope, aspect\nterra::terrain(E, \"slope\"/\"aspect\", \"radians\")\n\n\nI14, I05\n–\nCosine solar incidence at 14/05 UTC\ncosi_fun(alt, az, slp, asp), clamped to [0,1]\n\n\nlc\ncat\nLand-cover class\n{Forest, Water, Bare Soil, Maize}; rules from hill/slope/water masks\n\n\nhillW\n0–1\nHill weight (1 inside footprint)\nDisk/Gaussian on left third; combines main + optional micro-hills\n\n\nlake\n0/1\nWater mask\n1 only when lake_choice == \"water\" (disk on right third)\n\n\nI14_eff\n–\nShaded incidence (day)\nI14 * shade_fac_by_lc[lc]\n\n\nαI(lc)\n–\nDaytime solar sensitivity by LC\nLook-up from alpha_I_by_lc\n\n\ndawn_bias(lc)\n°C\nAdditive pre-dawn bias by LC\nLook-up from dawn_bias_by_lc\n\n\npool_fac(lc)\n–\nPooling multiplier by LC\nLook-up from pool_fac_by_lc\n\n\nR14 (T14)\n°C\nDaytime “truth” temperature field\nEq. (below)\n\n\nR05 (T05)\n°C\nPre-dawn “truth” temperature field\nEq. (below)"
  },
  {
    "objectID": "block4_5/mc_2025_pipemodel_explain.html#d.2-governing-equations",
    "href": "block4_5/mc_2025_pipemodel_explain.html#d.2-governing-equations",
    "title": "PipeModel Idealized valley microclimate sandbox",
    "section": "2.2 D.2 Governing equations",
    "text": "2.2 D.2 Governing equations\nLet $$ be the domain-mean elevation. Define the cross-valley cold-pool kernel\n\\[\n\\texttt{pool\\_base} \\;=\\; A \\exp\\!\\left[-(d_y/w)^2\\right],\\quad d_y=|y-y_0|,\n\\]\nblocked over the hill by (1 − pool_block_gain * hillW).\nDay (14 UTC)\n\\[\nT_{14} \\;=\\; T0_{14} \\;+\\; \\texttt{lapse\\_14}\\,(E-\\overline{E})\n\\;+\\; \\alpha_I(\\texttt{lc})\\, I_{14}^{\\text{eff}}\n\\;+\\; \\varepsilon_{14},\n\\quad\nI_{14}^{\\text{eff}} = I_{14}\\cdot \\texttt{shade\\_fac}(\\texttt{lc}).\n\\]\nPre-dawn (05 UTC)\n\\[\nT_{05} \\;=\\; T0_{05} \\;+\\; \\texttt{inv\\_05}\\,(E-\\overline{E})\n\\;+\\; \\eta_{\\text{slope}}\\;\\texttt{slp}\n\\;-\\; \\texttt{pool\\_base}\\cdot(1-\\texttt{pool\\_block\\_gain}\\cdot\\texttt{hillW})\\cdot \\texttt{pool\\_fac}(\\texttt{lc})\n\\;+\\; \\texttt{dawn\\_bias}(\\texttt{lc})\n\\;+\\; \\varepsilon_{05}.\n\\]\nNoise $_{14},_{05} (0,,0.3^2)$ i.i.d.\n\nNote vs. predecessor: the former warm_bias_water_dawn * lake term is now folded into dawn_bias(lc) (class “Water”); daytime α_map became αI(lc) * I14_eff with explicit canopy shading."
  },
  {
    "objectID": "block4_5/mc_2025_pipemodel_explain.html#d.3-dials",
    "href": "block4_5/mc_2025_pipemodel_explain.html#d.3-dials",
    "title": "PipeModel Idealized valley microclimate sandbox",
    "section": "2.3 D.3 Dials",
    "text": "2.3 D.3 Dials\n\n2.3.1 Global scalars\n\n\n\n\n\n\n\n\n\n\nParameter\nDefault\nSensible range\nAffects\nVisual signature (+)\n\n\n\n\nT0_14\n26.0 °C\n20–35\nT14 baseline\nUniform warming\n\n\nlapse_14\n−0.0065 °C/m\n−0.01…−0.002\nT14 vs elevation\nCooler rims, warmer floor\n\n\nT0_05\n8.5 °C\n3–15\nT05 baseline\nUniform warming\n\n\ninv_05\n+0.003 °C/m\n0–0.008\nT05 vs elevation\nRims warmer vs floor\n\n\nη_slope\n0.6\n0–1.5\nT05 slope flow proxy\nSteeper slopes a bit warmer at dawn\n\n\npool_base amplitude\n4.0 K\n1–8\nT05 pooling depth\nStronger blue band on valley axis\n\n\nw_pool\n70 m\n40–150\nT05 pooling width\nNarrower/broader cold band\n\n\npool_block_gain\n0.4\n0–1\nHill blocking\nWarm “tongue” over hill at dawn\n\n\nnoise σ\n0.3 K\n0–1\nBoth\nFine speckle/random texture\n\n\n\n\n\n2.3.2 Land-cover coefficients (by class)\nDefaults used in the code:\n\n\n\n\n\n\n\n\n\n\nLC class\nalpha_I_by_lc\nshade_fac_by_lc\ndawn_bias_by_lc (°C)\npool_fac_by_lc\n\n\n\n\nForest\n3.5\n0.6\n+0.3\n0.7\n\n\nWater\n1.5\n1.0\n+1.2\n0.8\n\n\nBare Soil\n6.0\n1.0\n−0.5\n1.1\n\n\nMaize\n4.5\n0.9\n+0.1\n1.0\n\n\n\nInterpretation: Bare Soil heats most by day and enhances pooling (factor &gt; 1) and cool bias at dawn; Forest damps radiation by day (shading) and reduces pooling (factor &lt; 1); Water heats little by day, gets a positive dawn bias and reduced pooling; Maize sits between grass and forest.\n\n\n2.3.3 Geometry/toggles\n\n\n\n\n\n\n\n\n\nParameter\nDefault\nOptions / range\nEffect\n\n\n\n\nlake_choice\n\"water\"\n\"none\", \"water\", \"hollow\"\nControls depression; only \"water\" sets LC=Water (thermal effects).\n\n\nhill_choice\n\"bump\"\n\"none\", \"bump\"\nAdds blocking & relief.\n\n\nlake_diam_m\n80\n40–150\nSize of pond/hollow.\n\n\nlake_depth_m\n10\n5–30\nDepression depth.\n\n\nhill_diam_m\n80\n40–150\nHill footprint.\n\n\nhill_height_m\n50\n10–120\nHill relief.\n\n\nsmooth_edges\nFALSE\nbool\nSoft pond rim if TRUE.\n\n\nhill_smooth\nFALSE\nbool\nGaussian hill if TRUE.\n\n\n(optional) micro-hills\noff\nrandom_hills, micro_*\nAdds sub-footprint relief; included in hillW."
  },
  {
    "objectID": "block4_5/mc_2025_pipemodel_explain.html#d.4-quick-recipes",
    "href": "block4_5/mc_2025_pipemodel_explain.html#d.4-quick-recipes",
    "title": "PipeModel Idealized valley microclimate sandbox",
    "section": "2.4 D.4 Quick “recipes”",
    "text": "2.4 D.4 Quick “recipes”\n\nCloud/haze day → ↓ alpha_I_by_lc (all classes, esp. Bare/Maize) → daytime LC contrasts fade; models lean on elevation/smoothness.\nHotter afternoon → ↑ T0_14 (+1…+3 K) → uniform bias shift; rankings unchanged.\nStronger pooling → ↑ pool_base and/or ↓ w_pool → sharper, deeper trough; drift-aware models gain.\nWater vs hollow → \"water\" sets LC=Water → ↓ daytime heating, ↑ dawn warm bias, ↓ pooling; \"hollow\" keeps only geometry (no water thermals).\nHill blocking → ↑ pool_block_gain → warm dawn tongue over hill; harder CV across blocks.\nCover swaps (what if): set a patch to Bare Soil → warmer day, colder dawn & stronger pooling; to Forest → cooler day, weaker pooling & slight dawn warm-up."
  },
  {
    "objectID": "block4_5/mc_2025_pipemodel_explain.html#scaled-demo-compact-physics-dossier",
    "href": "block4_5/mc_2025_pipemodel_explain.html#scaled-demo-compact-physics-dossier",
    "title": "PipeModel Idealized valley microclimate sandbox",
    "section": "2.5 Scaled demo: Compact Physics Dossier",
    "text": "2.5 Scaled demo: Compact Physics Dossier\nHere’s a clear, didactic walkthrough of the “scaled teaching” scenario and exactly what R_true14 and R_true05 are."
  },
  {
    "objectID": "block4_5/mc_2025_pipemodel_explain.html#lakebumpdense-compact-physics-dossier",
    "href": "block4_5/mc_2025_pipemodel_explain.html#lakebumpdense-compact-physics-dossier",
    "title": "PipeModel Idealized valley microclimate sandbox",
    "section": "2.6 Lake–Bump–Dense: Compact Physics Dossier",
    "text": "2.6 Lake–Bump–Dense: Compact Physics Dossier\nGoal. A clear, didactic synthetic scenario that (a) looks realistic, (b) drives temperature with topography + land-cover + sun, and (c) plays nicely with blocked CV and R*-tuning. Class 4 is meadows (not maize)."
  },
  {
    "objectID": "block4_5/mc_2025_pipemodel_explain.html#g-diagnostic",
    "href": "block4_5/mc_2025_pipemodel_explain.html#g-diagnostic",
    "title": "PipeModel Idealized valley microclimate sandbox",
    "section": "2.7 G Diagnostic",
    "text": "2.7 G Diagnostic\nlet’s read your baseline (no R*) results explicitly through the lens of process (what drives T) and scale (over what distances the drivers operate), model-by-model and time-by-time, then close with a scale+process summary and concrete upgrades."
  },
  {
    "objectID": "block4_5/mc_2025_pipemodel_explain.html#summary",
    "href": "block4_5/mc_2025_pipemodel_explain.html#summary",
    "title": "PipeModel Idealized valley microclimate sandbox",
    "section": "6.1 Summary",
    "text": "6.1 Summary\nDaytime temperature is controlled by very local facet and LC effects layered over a gentle lapse; models that encode those drivers at the right (small) scale—notably GAM, then RF—generalize across blocks with low error.\nPre-dawn temperature is anisotropic with a short cross-valley pooling scale, slope, and LC offsets; RF captures these thresholdy interactions best, with GAM second. Purely spatial smoothers (OK/IDW/Voronoi) underperform because their smoothing scale and mean process are mismatched.\nBring kriging back into contention by giving it the right drifts (cos(i), LC, distance-to-axis, hill-block) at tuned feature scales (R*), and by acknowledging anisotropy at night; if you want the best of both worlds, use regression-kriging with the learned mean from GAM/RF and an anisotropic residual field."
  },
  {
    "objectID": "block4_5/mc_2025_pipemodel_explain.html#i.-scale-analysis-l50l95-tuned-ked-drift-r",
    "href": "block4_5/mc_2025_pipemodel_explain.html#i.-scale-analysis-l50l95-tuned-ked-drift-r",
    "title": "PipeModel Idealized valley microclimate sandbox",
    "section": "11.1 I. Scale analysis — L50/L95 & tuned KED drift (R*)",
    "text": "11.1 I. Scale analysis — L50/L95 & tuned KED drift (R*)\nThis section adds a four-stage pipeline:\n\nScale inference: global variogram → L50/L95\n\nScale-matched predictors: drift from smoothed E at radius R\n\nTune R* with blocked CV (U-curve)\n\nDiagnostics: full benchmark + simple error budget\n\n\nWhy: Matching the model scale to the process scale reduces scale-mismatch error and makes gains attributable to scale rather than algorithm choice.\n\n\n11.1.1 Reading the outputs\n\nVariogram: dotted sill; dashed L50/L95 → scale anchors for smoothing and block sizes.\n\nU-curve: R* at lowest blocked-CV RMSE; include R = 0 so the tuner can prefer the raw drift.\n\nBenchmark: compare OK / KED / GAM / RF / IDW / Voronoi under the same blocked CV; document block size and R*.\n\nError budget (illustrative): OK → KED(base) → KED(R*) shows gains from drift and from scale matching.\n\n\nFrom concept to practice (pipeline mapping).\n\nEstimate scales: variogram \\(\\rightarrow\\) \\(\\sigma_{\\text{proc}}^2\\), \\(L_{50}\\), \\(L_{95}\\).\nCouple scales: smooth predictors / choose grids according to \\(R_{\\text{micro}}\\), \\(R_{\\text{local}}\\).\nTune \\(R^*\\): block‑CV, U‑curve \\(\\rightarrow\\) stable drift radius.\nBenchmark methods: compare OK/KED/GAM/RF/Trend/IDW/Voronoi at \\(R^*\\) (RMSE/MAE/Bias, document block size).\nProducts: write maps/grids at \\(R^*\\) (and optionally \\(L_{95}\\)); report the error budget.\n\n\n\nKey takeaway: The “smartest” algorithm doesn’t win — the one whose scale matches the process does.\n\n\n\n11.1.2 I.5 Reading the outputs (tables & plots)\nThis section explains how to interpret the key tables and figures produced by the pipeline and how to turn them into a model choice and a scale statement.\n\n11.1.2.1 1) Variogram & scale table (chunk scale-Ls)\n\nWhat you see: Empirical variogram points/line, horizontal dotted line at the (structural) sill, and vertical dashed lines at L50 and L95.\nHow to read it:\n\nNugget (near‑zero intercept) ≈ measurement/microscale noise. A large nugget means close points differ substantially; no method can beat this noise floor.\nSill (plateau) ≈ total variance once pairs are effectively uncorrelated.\nL50 / L95 ≈ pragmatic correlation distances (half vs. ~all structure spent). They are your scale anchors for smoothing radii, neighborhood ranges, and CV block sizes.\n\nQuality checks:\n\nIf no clear plateau: trend/non‑stationarity is likely → consider a drift (elev/sun terms) or a larger domain.\nIf L95 is near the domain size: scales are long; block sizes should be generous to avoid leakage.\nIf the variogram is noisy at large lags: rely more on L50 and the U‑curve outcome.\n\n\n\n\n11.1.2.2 2) U‑curve for tuned drift (chunk scale-tune)\n\nWhat you see: A line plot of RMSE vs. smoothing radius R for KED under blocked CV.\nDecision rule: R* is the radius with the lowest CV‑RMSE.\nWhat shapes mean:\n\nLeft side high (too small R): drift carries microscale noise → overfitting → higher CV error.\nRight side high (too large R): drift is oversmoothed → loses meaningful gradient → bias ↑.\nFlat bottom/plateau: a range of R values are equivalent → pick the smallest R on the plateau for parsimony.\n\nEdge cases: If the minimum sits at the search boundary, widen the R grid and re‑run; if still at the boundary, the field may be trend‑dominated or the covariate is weak.\n\n\n\n11.1.2.3 3) LBO‑CV metrics table (res$metrics)\nFor each model (Voronoi, IDW, OK, KED, GAM, RF) we report:\n\nRMSE (primary): square‑error penalty; most sensitive to outliers. Use this to rank models.\nMAE: median‑like robustness; a useful tie‑breaker alongside RMSE.\nBias (mean error): systematic over/under‑prediction; prefer |Bias| close to 0.\nR²: variance explained in held‑out blocks; interpret cautiously under spatial CV.\nn: number of held‑out predictions contributing.\n\nChoosing a winner:\n\nRank by lowest RMSE under the tuned configuration.\nIf RMSEs are within ~5–10%: prefer the model with lower MAE, lower |Bias|, and more stable block‑wise errors (see next point).\nIf KED (R*) ≈ OK: the drift adds little; the covariate is weak or the process is long‑range. If GAM/RF wins, the relationship is nonlinear or interaction‑rich.\n\n\n\n11.1.2.4 4) Block‑wise diagnostics\n\nBlock error boxes/scatter: Look for narrow distributions (stable across space). Large spread or outliers indicate location‑dependent performance.\nStability index (optional): CV_rmse = sd(RMSE_block) / mean(RMSE_block). Values &lt; 0.25 are typically stable; &gt; 0.4 suggests uneven performance.\nObs vs Pred scatter: Slope ~1 and tight cloud = good calibration; bowed patterns imply bias or missing drift terms.\n\n\n\n11.1.2.5 5) Error budget table (make_simple_error_budget)\nThree rows show how error decreases as structure is added and matched:\n\nBaseline (OK): no drift; sets a structure‑free reference.\nAdd drift (KED base): uses raw covariate; improvement here quantifies signal in the covariate.\nScale‑match drift (KED R*): covariate smoothed at R*; additional gain isolates scale alignment. The Gain_vs_prev column is the incremental improvement at each step.\n\n\nIf KED base ~ KED R*, scale matching adds little (either the raw drift is already at a compatible scale, or the field is insensitive to R). If OK &gt; KED base, the covariate may inject noise or the drift term is mis‑specified.\n\n\n\n\n11.1.3 I.6 Deciding on the best model (and documenting the scale)\nUse this practical, auditable rule set:\n\nPrimary criterion: Lowest CV‑RMSE under blocked CV.\nTie‑breakers: Lower MAE, smaller |Bias|, and better block‑stability.\nParsimony: If multiple models tie, choose the simplest (OK/KED &lt; GAM &lt; RF).\nScale sanity check: Report L50/L95 and verify that R* lies roughly in [L50, 1.5·L95]. If not, discuss why (e.g., strong trend, weak covariate, anisotropy).\nReproducibility: Record the block size, R grid, winning R*, and the full metrics table.\n\n\n\n11.1.4 I.7 Typical patterns & what they imply\n\nHigh nugget, short L50: Expect modest absolute accuracy; prefer coarser R and conservative models. IDW/OK with tight neighborhoods can perform on par with KED.\nLong L95, clear sill: Favor larger neighborhoods and smoother drifts; KED (R*) often dominates.\nGAM/RF &gt; KED: Nonlinear covariate effects or interactions (e.g., slope×aspect). Still align covariates to R* to avoid noise chasing.\nOK ~ KED: Elevation (or chosen drift) is weak for this synthetic setup; consider enriching covariates (slope/aspect/TRI) at matched scales.\n\n\n\n11.1.5 I.8 Checklist before you trust the numbers\n\nBlock size reflects correlation scale (≈ L95).\nU‑curve scanned a broad enough R range; minimum not at boundary.\nR* reported along with L50/L95.\nWinner chosen by blocked CV (not random folds).\nBias near zero; residuals pattern‑free in space.\nFigures/tables archived for reproducibility."
  },
  {
    "objectID": "block4_5/mc_2025_microclimate_viewer_merged.html",
    "href": "block4_5/mc_2025_microclimate_viewer_merged.html",
    "title": "Ecowitt Air Temperature — End-to-End applied Workflow",
    "section": "",
    "text": "This document demonstrates and explains a six-stage spatial pipeline for Ecowitt temperature data:\n\nIngest & clean: load two loggers, harmonize names, and aggregate to 3-hourly.\nInterpolation preview: per-timestep KED (universal kriging) and a multi-panel plot.\nScale inference (L): fit a global variogram to get L50 / L95 (spatial correlation ranges).\nScale-matched predictors: build DEM-derived rasters (optionally slope/aspect/TRI) at the right scale.\nTune \\(R^*\\): select an optimal drift radius \\(R\\) with block-CV (U-curve), then benchmark methods.\nDiagnostics: export products, report scales, and compute an optional error budget.\n\nKey concept: We use two DEMs on purpose — DEM_scale at native/coarser resolution drives scales, tuning, folds, CV, and error budget. DEM_render is an upsampled/aggregated DEM for pretty maps and interpolation output. This separation prevents the tuner from collapsing to 10 m just because pixels are tiny.\n\n\n\n# Global setup + packages\nset.seed(42)\noptions(width = 100)\n\npkgs &lt;- c(\n  \"sf\",\"terra\",\"raster\",\"dplyr\",\"automap\",\"gstat\",\"mapview\",\"stars\",\n  \"readxl\",\"stringr\",\"tidyr\",\"purrr\",\"lubridate\",\"rprojroot\",\n  \"exactextractr\",\"zoo\",\"ggplot2\",\"viridis\",\"mgcv\",\"randomForest\",\"fields\",\"sp\",\"deldir\",\n  \"leaflet\",\"DT\",\"htmltools\",\"jsonlite\",\"shiny\" # viewer deps\n)\nneed &lt;- setdiff(pkgs, rownames(installed.packages()))\nif (length(need)) install.packages(need, dependencies = TRUE)\ninvisible(lapply(pkgs, function(p) suppressPackageStartupMessages(library(p, character.only = TRUE))))\n\n\n# Small utilities (kept identical to your working script where applicable)\n# SpatRaster sicher \"pinnen\" (Datei-basiert) und als gültiges Objekt zurückgeben\n.pin_rast &lt;- function(r, crs = NULL, dir = NULL, name = \"pinned\") {\n  stopifnot(inherits(r, \"SpatRaster\"))\n  if (is.null(dir)) dir &lt;- file.path(getwd(), \"run_cache\")\n  if (!dir.exists(dir)) dir.create(dir, recursive = TRUE)\n  f &lt;- file.path(dir, paste0(name, \".tif\"))\n  # immer schreiben → garantiert datei-gestützt und voll materialisiert\n  terra::writeRaster(r, f, overwrite = TRUE)\n  rp &lt;- terra::rast(f)\n  if (!is.null(crs)) {\n    # nur projezieren, wenn noch nicht identisch\n    same &lt;- try(terra::crs(rp, proj=TRUE) == as.character(crs), silent = TRUE)\n    if (!isTRUE(same)) rp &lt;- terra::project(rp, as.character(crs), method = \"near\")\n  }\n  # Sanity check\n  invisible(terra::nlyr(rp))\n  rp\n}\n\n# Prüfer: „lebt“ der Pointer?\n.is_alive_spatr &lt;- function(r) {\n  inherits(r, \"SpatRaster\") && !inherits(try(terra::nlyr(r), silent = TRUE), \"try-error\")\n}\n\n# Safe, URL-friendly file slug\nslug &lt;- function(x) { \n  x &lt;- gsub(\"[^0-9A-Za-z_-]+\",\"-\", x)\n  x &lt;- gsub(\"-+\",\"-\", x)\n  gsub(\"(^-|-$)\",\"\", x)\n}\n\n# Human-readable time labels from AYYYY... keys\npretty_time &lt;- function(x) {\n  vapply(x, function(s) {\n    if (grepl(\"^A\\\\d{14}$\", s)) {\n      ts &lt;- as.POSIXct(substr(s, 2, 15), format = \"%Y%m%d%H%M%S\", tz = \"UTC\")\n      format(ts, \"%Y-%m-%d %H:%M\")\n    } else if (grepl(\"^A\\\\d{8}(_D)?$\", s)) {\n      ts &lt;- as.Date(substr(s, 2, 9), format = \"%Y%m%d\")\n      format(ts, \"%Y-%m-%d\")\n    } else s\n  }, character(1))\n}\n\n# Pick the most data-dense time-slice (max number of finite observations)\npick_densest_index &lt;- function(sf_wide, var_names) {\n  nn &lt;- sapply(var_names, function(v) sum(is.finite(sf_wide[[v]])))\n  which.max(nn)\n}\n\n# Build figure descriptions (viewer)\nbuild_explanations &lt;- function(fig_dir, pick_ts) {\n  ts_label &lt;- slug(pretty_time(pick_ts))\n  files &lt;- c(\n    \"timeseries_panel_grid.png\",\n    \"timeseries_panel_grid.pdf\",\n    sprintf(\"u_curve_%s.png\", ts_label),\n    sprintf(\"u_curve_extras_%s.png\", ts_label),\n    sprintf(\"benchmark_%s.png\", ts_label),\n    sprintf(\"benchmark_extras_%s.png\", ts_label)\n  )\n  paths &lt;- file.path(fig_dir, files)\n  desc  &lt;- c(\n    \"Per-timestep KED previews; dots=stations; red=plot boundary.\",\n    \"Same as PNG, vector PDF.\",\n    \"U-curve for tuning R via block-CV (drift-only).\",\n    \"U-curve with extra predictors.\",\n    \"Method comparison at R* (lower RMSE is better).\",\n    \"Benchmark with extras at R*.\"\n  )\n  keep &lt;- file.exists(paths)\n  out &lt;- as.list(desc[keep]); names(out) &lt;- basename(paths[keep])\n  out\n}\n\n\nWhy this matters: using fine pixels for scale estimation will cap \\(R\\) at a few pixels, resulting in the “10 m fallback”. Keep DEM_scale at a realistic native/coarser resolution for L/R.\n\n\n\n\n\n# Robust project root finder and paths\nwd &lt;- rprojroot::find_rstudio_root_file()\n\nsource(file.path(wd, \"block4_5/all_functions_1.R\"))   # consolidated toolkit\n\nfn_DTM        &lt;- file.path(wd, \"block4_5/data_2024/copernicus_DEM.tif\")\nfn_stations   &lt;- file.path(wd, \"block4_5/data_2024/stations_prelim_modifiziert.gpkg\")\nfn_area       &lt;- file.path(wd, \"block4_5/data_2024/plot.shp\")\nfn_temp_FC29  &lt;- file.path(wd, \"block4_5/data_2024/all_GW1000A-WIFIFC29.xlsx\")\nfn_temp_DB2F  &lt;- file.path(wd, \"block4_5/data_2024/all_GW1000A-WIFIDB2F.xlsx\")\ncleandata_rds &lt;- file.path(wd, \"block4_5/data_2024/climdata.RDS\")\n\nout_dir    &lt;- file.path(wd, \"block4_5/interpolated\")\nfig_dir    &lt;- file.path(out_dir, \"fig\")\nmethod_dir &lt;- file.path(out_dir, \"methods_compare\")\nreport_dir &lt;- file.path(out_dir, \"report\")\nfor (d in c(out_dir, fig_dir, method_dir, report_dir)) if (!dir.exists(d)) dir.create(d, recursive = TRUE)\n\n# CRS\nepsg &lt;- \"EPSG:32633\"  # UTM zone 33N\nsf_crs_utm33 &lt;- sf::st_crs(epsg)\n\n\n\n\n\n# DEMs\nDEM_scale  &lt;- terra::rast(\"data_2024/DEM.tif\") |&gt; terra::project(epsg)\nDEM_scale  &lt;- terra::aggregate(DEM_scale, c(20, 20))  # coarsen ~20–25 m (as in your working code)\nnames(DEM_scale) &lt;- \"altitude\"\nDEM_render &lt;- DEM_scale |&gt; terra::aggregate(fact = c(10, 10))  # for rendering products\n\ncat(\"DEM_scale res (m): \", paste(terra::res(DEM_scale),  collapse=\" x \"), \"\\n\")\n\nDEM_scale res (m):  2.00223686801611 x 2.0022368680127 \n\ncat(\"DEM_render res (m):\", paste(terra::res(DEM_render), collapse=\" x \"), \"\\n\")\n\nDEM_render res (m): 20.0223686801606 x 20.022368680127 \n\n# Stations and plot boundary → same CRS\nstations_pos &lt;- sf::st_read(fn_stations, quiet = TRUE) |&gt; sf::st_transform(sf_crs_utm33)\nplot_area    &lt;- sf::st_read(fn_area, quiet = TRUE)     |&gt; sf::st_transform(sf_crs_utm33) |&gt; sf::st_make_valid()\n\n# Altitude from DEM_scale (not the upsampled one)\nstations_pos &lt;- stations_pos %&gt;%\n  dplyr::mutate(altitude = exactextractr::exact_extract(DEM_scale, sf::st_buffer(stations_pos, 1), \"mean\"))\n\n\n  |                                                                                                \n  |                                                                                          |   0%\n  |                                                                                                \n  |======                                                                                    |   7%\n  |                                                                                                \n  |=============                                                                             |  14%\n  |                                                                                                \n  |===================                                                                       |  21%\n  |                                                                                                \n  |==========================                                                                |  29%\n  |                                                                                                \n  |================================                                                          |  36%\n  |                                                                                                \n  |=======================================                                                   |  43%\n  |                                                                                                \n  |=============================================                                             |  50%\n  |                                                                                                \n  |===================================================                                       |  57%\n  |                                                                                                \n  |==========================================================                                |  64%\n  |                                                                                                \n  |================================================================                          |  71%\n  |                                                                                                \n  |=======================================================================                   |  79%\n  |                                                                                                \n  |=============================================================================             |  86%\n  |                                                                                                \n  |====================================================================================      |  93%\n  |                                                                                                \n  |==========================================================================================| 100%\n\n\n\n\n\n\n\n\n\n\n\n\ntemp_FC29 &lt;- extract_ecowitt_core_vars(fn_temp_FC29)\n\nNew names:\nNew names:\n• `` -&gt; `...1`\n• `` -&gt; `...2`\n• `` -&gt; `...3`\n• `` -&gt; `...4`\n• `` -&gt; `...5`\n• `` -&gt; `...6`\n• `` -&gt; `...7`\n• `` -&gt; `...8`\n• `` -&gt; `...9`\n• `` -&gt; `...10`\n• `` -&gt; `...11`\n• `` -&gt; `...12`\n• `` -&gt; `...13`\n• `` -&gt; `...14`\n• `` -&gt; `...15`\n• `` -&gt; `...16`\n• `` -&gt; `...17`\n• `` -&gt; `...18`\n• `` -&gt; `...19`\n• `` -&gt; `...20`\n• `` -&gt; `...21`\n• `` -&gt; `...22`\n• `` -&gt; `...23`\n• `` -&gt; `...24`\n• `` -&gt; `...25`\n• `` -&gt; `...26`\n• `` -&gt; `...27`\n• `` -&gt; `...28`\n• `` -&gt; `...29`\n• `` -&gt; `...30`\n• `` -&gt; `...31`\n• `` -&gt; `...32`\n• `` -&gt; `...33`\n• `` -&gt; `...34`\n• `` -&gt; `...35`\n• `` -&gt; `...36`\n• `` -&gt; `...37`\n• `` -&gt; `...38`\n• `` -&gt; `...39`\n• `` -&gt; `...40`\n• `` -&gt; `...41`\n• `` -&gt; `...42`\n• `` -&gt; `...43`\n• `` -&gt; `...44`\n• `` -&gt; `...45`\n• `` -&gt; `...46`\n• `` -&gt; `...47`\n• `` -&gt; `...48`\n• `` -&gt; `...49`\n• `` -&gt; `...50`\n• `` -&gt; `...51`\n• `` -&gt; `...52`\n• `` -&gt; `...53`\n• `` -&gt; `...54`\n• `` -&gt; `...55`\n• `` -&gt; `...56`\n• `` -&gt; `...57`\n• `` -&gt; `...58`\n• `` -&gt; `...59`\n• `` -&gt; `...60`\n• `` -&gt; `...61`\n• `` -&gt; `...62`\n• `` -&gt; `...63`\n• `` -&gt; `...64`\n• `` -&gt; `...65`\n• `` -&gt; `...66`\n• `` -&gt; `...67`\n• `` -&gt; `...68`\n• `` -&gt; `...69`\n• `` -&gt; `...70`\n• `` -&gt; `...71`\n• `` -&gt; `...72`\n• `` -&gt; `...73`\n• `` -&gt; `...74`\n• `` -&gt; `...75`\n• `` -&gt; `...76`\n• `` -&gt; `...77`\n• `` -&gt; `...78`\n• `` -&gt; `...79`\n• `` -&gt; `...80`\n\ntemp_DB2F &lt;- extract_ecowitt_core_vars(fn_temp_DB2F)\n\nNew names:\nNew names:\n• `` -&gt; `...1`\n• `` -&gt; `...2`\n• `` -&gt; `...3`\n• `` -&gt; `...4`\n• `` -&gt; `...5`\n• `` -&gt; `...6`\n• `` -&gt; `...7`\n• `` -&gt; `...8`\n• `` -&gt; `...9`\n• `` -&gt; `...10`\n• `` -&gt; `...11`\n• `` -&gt; `...12`\n• `` -&gt; `...13`\n• `` -&gt; `...14`\n• `` -&gt; `...15`\n• `` -&gt; `...16`\n• `` -&gt; `...17`\n• `` -&gt; `...18`\n• `` -&gt; `...19`\n• `` -&gt; `...20`\n• `` -&gt; `...21`\n• `` -&gt; `...22`\n• `` -&gt; `...23`\n• `` -&gt; `...24`\n• `` -&gt; `...25`\n• `` -&gt; `...26`\n• `` -&gt; `...27`\n• `` -&gt; `...28`\n• `` -&gt; `...29`\n• `` -&gt; `...30`\n• `` -&gt; `...31`\n• `` -&gt; `...32`\n• `` -&gt; `...33`\n• `` -&gt; `...34`\n• `` -&gt; `...35`\n• `` -&gt; `...36`\n• `` -&gt; `...37`\n• `` -&gt; `...38`\n• `` -&gt; `...39`\n• `` -&gt; `...40`\n• `` -&gt; `...41`\n• `` -&gt; `...42`\n• `` -&gt; `...43`\n• `` -&gt; `...44`\n• `` -&gt; `...45`\n• `` -&gt; `...46`\n• `` -&gt; `...47`\n• `` -&gt; `...48`\n• `` -&gt; `...49`\n• `` -&gt; `...50`\n• `` -&gt; `...51`\n• `` -&gt; `...52`\n• `` -&gt; `...53`\n• `` -&gt; `...54`\n• `` -&gt; `...55`\n• `` -&gt; `...56`\n• `` -&gt; `...57`\n\nt_rh_all  &lt;- merge_ecowitt_logger_vars(temp_FC29, temp_DB2F)\n\n# Clean display names and map to verbose station names\nfor (meas in c(\"temperature\",\"humidity\")) {\n  t_rh_all[[meas]] &lt;- t_rh_all[[meas]] %&gt;%\n    dplyr::rename_with(~ to_verbose(.x, ifelse(meas==\"temperature\",\"Temperature\",\"Humidity\")), -Time) %&gt;%\n    clean_names()\n}\n\n# Aggregate to 3-hour steps\ntemp_agg &lt;- t_rh_all$temperature %&gt;%\n  dplyr::mutate(time = lubridate::floor_date(Time, \"3 hours\")) %&gt;%\n  dplyr::group_by(time) %&gt;%\n  dplyr::summarise(across(where(is.numeric), ~ mean(.x, na.rm = TRUE)), .groups = \"drop\")\nnames(temp_agg) &lt;- clean_ids(names(temp_agg))\n\n# long → wide matrix: station rows, time columns\ntemp_matrix &lt;- temp_agg %&gt;%\n  tidyr::pivot_longer(cols = -time, names_to = \"stationid\", values_to = \"value\") %&gt;%\n  tidyr::pivot_wider(names_from = time, values_from = value)\n\n# Join to station geometry and altitude\nstations_pos &lt;- stations_pos %&gt;% dplyr::mutate(stationid = to_verbose(stationid))\nm &lt;- dplyr::left_join(stations_pos, temp_matrix, by = \"stationid\")\n\n# Hygiene\nstations_pos$stationid &lt;- gsub(\"\\\\(℃\\\\)|\\\\(％\\\\)|\\\\(\\\\%\\\\)\", \"\", stations_pos$stationid)\nm$stationid            &lt;- gsub(\"\\\\(℃\\\\)|\\\\(％\\\\)|\\\\(\\\\%\\\\)\", \"\", m$stationid)\nnames(m)               &lt;- fix_names(names(m))\nsaveRDS(m, cleandata_rds)\n\n\n\n\n\n\n\n\n\n\n\nmin_pts &lt;- 5\nvars &lt;- as.list(grep(\"^A\\\\d{8,14}\", names(m), value = TRUE))\n\nkriged_list &lt;- lapply(vars, function(v) {\n  interpolate_kriging(v, m, DEM_render, output_dir = out_dir, label = \"pretty\")\n})\n\nInterpolating: A20230827150000\n\n\nWarning in interpolate_kriging(v, m, DEM_render, output_dir = out_dir, label = \"pretty\"): Variogram\nfailed for A20230827150000\n\n\nInterpolating: A20230827180000\n\n\nWarning in interpolate_kriging(v, m, DEM_render, output_dir = out_dir, label = \"pretty\"): Variogram\nfailed for A20230827180000\n\n\nInterpolating: A20230828150000\n\n\nWarning in interpolate_kriging(v, m, DEM_render, output_dir = out_dir, label = \"pretty\"): Variogram\nfailed for A20230828150000\n\n\nInterpolating: A20230828180000\n\n\nWarning in interpolate_kriging(v, m, DEM_render, output_dir = out_dir, label = \"pretty\"): Variogram\nfailed for A20230828180000\n\n\nInterpolating: A20230828210000\n\n\nWarning in interpolate_kriging(v, m, DEM_render, output_dir = out_dir, label = \"pretty\"): Variogram\nfailed for A20230828210000\n\n\nInterpolating: A20230829\n\n\nWarning in interpolate_kriging(v, m, DEM_render, output_dir = out_dir, label = \"pretty\"): Variogram\nfailed for A20230829\n\n\nInterpolating: A20230829030000\n\n\nWarning in interpolate_kriging(v, m, DEM_render, output_dir = out_dir, label = \"pretty\"): Variogram\nfailed for A20230829030000\n\n\nInterpolating: A20230829060000\n\n\nWarning in interpolate_kriging(v, m, DEM_render, output_dir = out_dir, label = \"pretty\"): Variogram\nfailed for A20230829060000\n\n\nInterpolating: A20230829090000\n\n\n[using universal kriging]\n\n\n✔ Written: /home/creu/edu/gisma-courses/EON2025/block4_5/interpolated/A20230829090000_interpolated.tif\n\n\nInterpolating: A20230829120000\n\n\n[using universal kriging]\n\n\n✔ Written: /home/creu/edu/gisma-courses/EON2025/block4_5/interpolated/A20230829120000_interpolated.tif\n\n\nInterpolating: A20230829150000\n\n\n[using universal kriging]\n\n\n✔ Written: /home/creu/edu/gisma-courses/EON2025/block4_5/interpolated/A20230829150000_interpolated.tif\n\n\nInterpolating: A20230829180000\n\n\n[using universal kriging]\n\n\n✔ Written: /home/creu/edu/gisma-courses/EON2025/block4_5/interpolated/A20230829180000_interpolated.tif\n\n\nInterpolating: A20230829210000\n\n\n[using universal kriging]\n\n\n✔ Written: /home/creu/edu/gisma-courses/EON2025/block4_5/interpolated/A20230829210000_interpolated.tif\n\n\nInterpolating: A20230830\n\n\n[using universal kriging]\n\n\n✔ Written: /home/creu/edu/gisma-courses/EON2025/block4_5/interpolated/A20230830_interpolated.tif\n\n\nInterpolating: A20230830030000\n\n\nWarning in interpolate_kriging(v, m, DEM_render, output_dir = out_dir, label = \"pretty\"): Variogram\nfailed for A20230830030000\n\n\nInterpolating: A20230830060000\n\n\nWarning in interpolate_kriging(v, m, DEM_render, output_dir = out_dir, label = \"pretty\"): Variogram\nfailed for A20230830060000\n\n\nInterpolating: A20230830090000\n\n\n[using universal kriging]\n\n\n✔ Written: /home/creu/edu/gisma-courses/EON2025/block4_5/interpolated/A20230830090000_interpolated.tif\n\n\nInterpolating: A20230830120000\n\n\n[using universal kriging]\n\n\n✔ Written: /home/creu/edu/gisma-courses/EON2025/block4_5/interpolated/A20230830120000_interpolated.tif\n\n\nInterpolating: A20230830150000\n\n\nWarning in interpolate_kriging(v, m, DEM_render, output_dir = out_dir, label = \"pretty\"): Variogram\nfailed for A20230830150000\n\n\nInterpolating: A20230830180000\n\n\nWarning in interpolate_kriging(v, m, DEM_render, output_dir = out_dir, label = \"pretty\"): Variogram\nfailed for A20230830180000\n\n\nInterpolating: A20230830210000\n\n\n[using universal kriging]\n\n\n✔ Written: /home/creu/edu/gisma-courses/EON2025/block4_5/interpolated/A20230830210000_interpolated.tif\n\n\nInterpolating: A20230831\n\n\n[using universal kriging]\n\n\n✔ Written: /home/creu/edu/gisma-courses/EON2025/block4_5/interpolated/A20230831_interpolated.tif\n\n\nInterpolating: A20230831030000\n\n\nWarning in interpolate_kriging(v, m, DEM_render, output_dir = out_dir, label = \"pretty\"): Variogram\nfailed for A20230831030000\n\n\nInterpolating: A20230831060000\n\n\nWarning in interpolate_kriging(v, m, DEM_render, output_dir = out_dir, label = \"pretty\"): Variogram\nfailed for A20230831060000\n\n\nInterpolating: A20230831090000\n\n\n[using universal kriging]\n\n\n✔ Written: /home/creu/edu/gisma-courses/EON2025/block4_5/interpolated/A20230831090000_interpolated.tif\n\n\nInterpolating: A20230831120000\n\n\n[using universal kriging]\n\n\n✔ Written: /home/creu/edu/gisma-courses/EON2025/block4_5/interpolated/A20230831120000_interpolated.tif\n\n\nInterpolating: A20230831150000\n\n\nWarning in interpolate_kriging(v, m, DEM_render, output_dir = out_dir, label = \"pretty\"): Variogram\nfailed for A20230831150000\n\n\nInterpolating: A20230831180000\n\n\nWarning in interpolate_kriging(v, m, DEM_render, output_dir = out_dir, label = \"pretty\"): Variogram\nfailed for A20230831180000\n\n\nInterpolating: A20230831210000\n\n\n[using universal kriging]\n\n\n✔ Written: /home/creu/edu/gisma-courses/EON2025/block4_5/interpolated/A20230831210000_interpolated.tif\n\n\nInterpolating: A20230901\n\n\nWarning in interpolate_kriging(v, m, DEM_render, output_dir = out_dir, label = \"pretty\"): Variogram\nfailed for A20230901\n\nnames(kriged_list) &lt;- vars\n\n\n# Shared color scale across all timestamps\npanel &lt;- timeseries_panel(\n  kriged_list        = kriged_list,\n  plot_area          = plot_area,\n  stations_pos       = stations_pos,\n  cells_target       = 150000,\n  max_cols           = 4,\n  label_pretty_time  = TRUE,\n  out_png            = file.path(fig_dir, \"timeseries_panel_grid.png\"),\n  out_pdf            = file.path(fig_dir, \"timeseries_panel_grid.pdf\"),\n  fill_label         = \"Temperature\"\n)\n\nCoordinate system already present. Adding new coordinate system, which will replace the existing\none.\n\npanel$plot\n\n\n\n\n\n\n\n\n\n\n\n\n# Densest timestamp\npick_idx &lt;- pick_densest_index(m, vars)\npick_ts  &lt;- names(m)[pick_idx]\n\n# Extra predictors (kept as in your working code)\nextra_list &lt;- list(\n  slope  = terra::terrain(DEM_scale, v = \"slope\",  unit = \"degrees\"),\n  aspect = terra::terrain(DEM_scale, v = \"aspect\"),\n  tri    = terra::terrain(DEM_scale, v = \"TRI\")\n)\n\n# One timestamp (densest), with extras\nres_one &lt;- run_one(\n  v           = vars[[pick_idx]],\n  m           = m,\n  DEM_render  = DEM_render,\n  DEM_scale   = DEM_scale,\n  method_dir  = method_dir,\n  fig_dir     = fig_dir,\n  report_dir  = report_dir,\n  extra_preds = extra_list,\n  save_figs   = TRUE\n)\n\nWarning in newton(lsp = lsp, X = G$X, y = G$y, Eb = G$Eb, UrS = G$UrS, L = G$L, : Fitting\nterminated with step failure - check results carefully\n\n\nWarning in log(b$scale.est): NaNs produced\nWarning in log(b$scale.est): NaNs produced\n\n\nWarning in newton(lsp = lsp, X = G$X, y = G$y, Eb = G$Eb, UrS = G$UrS, L = G$L, : Fitting\nterminated with step failure - check results carefully\n\n\nWarning in log(b$scale.est): NaNs produced\n\n\nWarning in newton(lsp = lsp, X = G$X, y = G$y, Eb = G$Eb, UrS = G$UrS, L = G$L, : Fitting\nterminated with step failure - check results carefully\nWarning in newton(lsp = lsp, X = G$X, y = G$y, Eb = G$Eb, UrS = G$UrS, L = G$L, : Fitting\nterminated with step failure - check results carefully\nWarning in newton(lsp = lsp, X = G$X, y = G$y, Eb = G$Eb, UrS = G$UrS, L = G$L, : Fitting\nterminated with step failure - check results carefully\n\n\nWarning in log(b$scale.est): NaNs produced\n\n\nWarning in newton(lsp = lsp, X = G$X, y = G$y, Eb = G$Eb, UrS = G$UrS, L = G$L, : Fitting\nterminated with step failure - check results carefully\n\n\nWarning in log(b$scale.est): NaNs produced\n\n\nWarning in newton(lsp = lsp, X = G$X, y = G$y, Eb = G$Eb, UrS = G$UrS, L = G$L, : Fitting\nterminated with step failure - check results carefully\n\n\nWarning in log(b$scale.est): NaNs produced\n\n\nWarning in newton(lsp = lsp, X = G$X, y = G$y, Eb = G$Eb, UrS = G$UrS, L = G$L, : Fitting\nterminated with step failure - check results carefully\nWarning in newton(lsp = lsp, X = G$X, y = G$y, Eb = G$Eb, UrS = G$UrS, L = G$L, : Fitting\nterminated with step failure - check results carefully\nWarning in newton(lsp = lsp, X = G$X, y = G$y, Eb = G$Eb, UrS = G$UrS, L = G$L, : Fitting\nterminated with step failure - check results carefully\nWarning in newton(lsp = lsp, X = G$X, y = G$y, Eb = G$Eb, UrS = G$UrS, L = G$L, : Fitting\nterminated with step failure - check results carefully\n\n\nWarning in log(b$scale.est): NaNs produced\n\n\nWarning in newton(lsp = lsp, X = G$X, y = G$y, Eb = G$Eb, UrS = G$UrS, L = G$L, : Fitting\nterminated with step failure - check results carefully\nWarning in newton(lsp = lsp, X = G$X, y = G$y, Eb = G$Eb, UrS = G$UrS, L = G$L, : Fitting\nterminated with step failure - check results carefully\n\n\n[inverse distance weighted interpolation]\n[using ordinary kriging]\n[using universal kriging]\n[inverse distance weighted interpolation]\n[using ordinary kriging]\n[using universal kriging]\n\n\nR*: tuner failed; falling back to 56.8593 m.\n\n\nWarning in x@pntr$rastDistance(target, exclude, keepNA, tolower(unit), TRUE, : GDAL Message 1:\nPixels not square, distances will be inaccurate.\n\n\n[using ordinary kriging]\n[inverse distance weighted interpolation]\n[using universal kriging]\n[using ordinary kriging]\n[inverse distance weighted interpolation]\n[using universal kriging]\n[using ordinary kriging]\n[inverse distance weighted interpolation]\n[using universal kriging]\n[using ordinary kriging]\n[inverse distance weighted interpolation]\n[using universal kriging]\n[using ordinary kriging]\n[inverse distance weighted interpolation]\n[using universal kriging]\n[using ordinary kriging]\n[inverse distance weighted interpolation]\n[using universal kriging]\n\n\nR*: tuner failed; falling back to 56.8593 m.\n\n\nWarning in x@pntr$rastDistance(target, exclude, keepNA, tolower(unit), TRUE, : GDAL Message 1:\nPixels not square, distances will be inaccurate.\n\n\n[using ordinary kriging]\n[inverse distance weighted interpolation]\n[using universal kriging]\n[using ordinary kriging]\n[inverse distance weighted interpolation]\n[using universal kriging]\n[using ordinary kriging]\n[inverse distance weighted interpolation]\n[using universal kriging]\n[using ordinary kriging]\n[inverse distance weighted interpolation]\n[using universal kriging]\n[using ordinary kriging]\n[inverse distance weighted interpolation]\n[using universal kriging]\n[using ordinary kriging]\n[inverse distance weighted interpolation]\n[using universal kriging]\n\n\n[using ordinary kriging]\n[using ordinary kriging]\n[using ordinary kriging]\n[using ordinary kriging]\n[using ordinary kriging]\n[using ordinary kriging]\n[using ordinary kriging]\n[using ordinary kriging]\n[using ordinary kriging]\n[using ordinary kriging]\n[using ordinary kriging]\n[using ordinary kriging]\n[using ordinary kriging]\n[using ordinary kriging]\n[using ordinary kriging]\n[using ordinary kriging]\n\n\n\n\n\n\ncompute_all &lt;- FALSE\n\nif (isTRUE(compute_all)) {\n  req &lt;- c(\"m\",\"DEM_render\",\"DEM_scale\",\"method_dir\",\"fig_dir\",\"report_dir\")\n  miss &lt;- req[!vapply(req, exists, logical(1), inherits = TRUE)]\n  if (length(miss)) stop(\"Fehlende Objekte im Environment: \", paste(miss, collapse = \", \"))\n\n  .best_from_bench &lt;- function(bench_obj) {\n    if (is.null(bench_obj) || !is.data.frame(bench_obj$table) || nrow(bench_obj$table) &lt; 1)\n      return(NULL)\n    b &lt;- bench_obj$table\n    b &lt;- b[is.finite(b$RMSE), , drop = FALSE]\n    if (!nrow(b)) return(NULL)\n    b &lt;- b[order(b$RMSE), , drop = FALSE]\n    b[1, c(\"method\",\"RMSE\"), drop = FALSE]\n  }\n\n  message(sprintf(\"Starte compute_all für %d Zeitschritte …\", length(vars)))\n\n  res_all &lt;- setNames(lapply(vars, function(vv) {\n    message(\"→ run_one: \", pretty_time(vv))\n    tryCatch(\n      run_one(\n        v           = vv,\n        m           = m,\n        DEM_render  = DEM_render,\n        DEM_scale   = DEM_scale,\n        method_dir  = method_dir,\n        fig_dir     = fig_dir,\n        report_dir  = report_dir,\n        extra_preds = extra_list,\n        save_figs   = TRUE,\n        save_tables = TRUE\n      ),\n      error = function(e) {\n        warning(\"run_one fehlgeschlagen für \", vv, \": \", conditionMessage(e))\n        NULL\n      }\n    )\n  }), vars)\n\n  saveRDS(res_all, file.path(report_dir, \"all_results.RDS\"))\n\n  summ &lt;- do.call(rbind, lapply(names(res_all), function(k) {\n    r &lt;- res_all[[k]]\n    if (is.null(r)) {\n      return(data.frame(\n        ts_key      = k, \n        stamp       = pretty_time(k),\n        R_star      = NA_real_,\n        best_source = NA_character_,  # \"no_extras\"/\"with_extras\"\n        best_method = NA_character_,\n        best_RMSE   = NA_real_\n      ))\n    }\n    rstar &lt;- suppressWarnings(as.numeric(r$tune$R_star))\n    if (!is.finite(rstar)) rstar &lt;- NA_real_\n\n    b0 &lt;- .best_from_bench(r$bench)\n    bE &lt;- .best_from_bench(r$bench_ex)\n\n    score0 &lt;- if (!is.null(b0) && isTRUE(is.finite(b0$RMSE))) b0$RMSE else Inf\n    scoreE &lt;- if (!is.null(bE) && isTRUE(is.finite(bE$RMSE))) bE$RMSE else Inf\n\n    if (is.infinite(score0) && is.infinite(scoreE)) {\n      src &lt;- NA_character_; bm &lt;- NA_character_; br &lt;- NA_real_\n    } else if (score0 &lt;= scoreE) {\n      src &lt;- \"no_extras\"; bm &lt;- b0$method; br &lt;- score0\n    } else {\n      src &lt;- \"with_extras\"; bm &lt;- bE$method; br &lt;- scoreE\n    }\n\n    data.frame(\n      ts_key      = k,\n      stamp       = pretty_time(k),\n      R_star      = rstar,\n      best_source = src,\n      best_method = bm,\n      best_RMSE   = br\n    )\n  }))\n\n  utils::write.csv(summ, file.path(report_dir, \"summary_Rstar_bestmethod.csv\"), row.names = FALSE)\n  message(\"✔ Fertig: summary_Rstar_bestmethod.csv geschrieben.\")\n}\n\n\n\n\n\nts_label &lt;- pretty_time(pick_ts)\nbench_base_csv &lt;- file.path(report_dir, sprintf(\"benchmark_%s.csv\",        slug(ts_label)))\nbench_ex_csv   &lt;- file.path(report_dir, sprintf(\"benchmark_extras_%s.csv\", slug(ts_label)))\neb_base_csv    &lt;- file.path(report_dir, sprintf(\"error_budget_%s.csv\",     slug(ts_label)))\neb_ex_csv      &lt;- file.path(report_dir, sprintf(\"error_budget_extras_%s.csv\", slug(ts_label)))\n\nif (is.list(res_one$bench)    && is.data.frame(res_one$bench$table))    write.csv(res_one$bench$table,    bench_base_csv, row.names = FALSE)\nif (is.list(res_one$bench_ex) && is.data.frame(res_one$bench_ex$table)) write.csv(res_one$bench_ex$table, bench_ex_csv,   row.names = FALSE)\nif (is.data.frame(res_one$errtab))    write.csv(res_one$errtab,    eb_base_csv, row.names = FALSE)\nif (is.data.frame(res_one$errtab_ex)) write.csv(res_one$errtab_ex, eb_ex_csv,   row.names = FALSE)\n\n\n\n\n\nn_stations &lt;- nrow(stations_pos)\nn_pts_ts   &lt;- sum(is.finite(m[[pick_ts]]))\nLs         &lt;- get_Ls(res_one$wf$L)\nLs_e       &lt;- if (!is.null(res_one$wf_ex)) get_Ls(res_one$wf_ex$L) else NULL\nRstar_base &lt;- suppressWarnings(as.numeric(res_one$tune$R_star))\nRstar_ex   &lt;- suppressWarnings(as.numeric(res_one$tune_ex$R_star))\n\ncat(sprintf(\"Chosen R (micro/local): %s / %s m\\n\",\n            ifelse(is.finite(res_one$wf$R['micro']), round(res_one$wf$R['micro']), \"NA\"),\n            ifelse(is.finite(res_one$wf$R['local']), round(res_one$wf$R['local']), \"NA\")\n))\n\nChosen R (micro/local): 19 / 57 m\n\n\n\n\n\n\nThe viewer launches a Shiny app; to not block Quarto rendering, the launch is wrapped in if (interactive()).\n\n\n# Minimal viewer injection: use your working run_mc_viewer() with robust file matching\nraster_path &lt;- function(method, ts) {\n  stopifnot(length(method) == 1, length(ts) == 1)\n  m &lt;- tolower(method)\n  .ts_tokens &lt;- function(ts_key) {\n    raw &lt;- tolower(as.character(ts_key))\n    pty &lt;- tolower(pretty_time(ts_key))\n    slug_pt &lt;- gsub(\"[^0-9A-Za-z_-]+\",\"-\", pty)\n    d14 &lt;- sub(\"^a\", \"\", raw)\n    ymd  &lt;- if (nchar(d14) &gt;= 8) substr(d14,1,8) else NA_character_\n    hhmm &lt;- if (nchar(d14) &gt;= 12) substr(d14,9,12) else NA_character_\n    comp1 &lt;- if (!is.na(ymd) && !is.na(hhmm))\n      paste0(substr(ymd,1,4),\"-\",substr(ymd,5,6),\"-\",substr(ymd,7,8),\"-\",\n             substr(hhmm,1,2),\"-\",substr(hhmm,3,4)) else NA_character_\n    comp2 &lt;- gsub(\"-\", \"\", comp1)\n    ymd_dash &lt;- if (!is.na(ymd)) paste0(substr(ymd,1,4),\"-\",substr(ymd,5,6),\"-\",substr(ymd,7,8)) else NA_character_\n    unique(na.omit(c(raw, slug_pt, comp1, comp2, ymd_dash, ymd)))\n  }\n  toks &lt;- .ts_tokens(ts)\n  tok_rx &lt;- gsub(\"[-_]\", \"[-_]\", toks)\n\n  all_files &lt;- list.files(method_dir, pattern = \"\\\\.tif$\", full.names = TRUE, ignore.case = TRUE)\n  if (!length(all_files)) return(NA_character_)\n  b &lt;- tolower(basename(all_files))\n\n  keep_pref &lt;- grepl(paste0(\"^\", m, \"_\"), b)\n  files_m &lt;- all_files[keep_pref]; b_m &lt;- b[keep_pref]\n  if (length(files_m)) {\n    score &lt;- vapply(seq_along(b_m), function(i) {\n      max(c(0, vapply(tok_rx, function(rx) if (grepl(rx, b_m[i], perl = TRUE)) nchar(rx) else 0L, integer(1))))\n    }, numeric(1))\n\n    if (any(score &gt; 0)) {\n      best &lt;- files_m[score == max(score)]\n      bbest &lt;- tolower(basename(best))\n      idxR &lt;- grep(\"_rstar\\\\.tif$\", bbest)\n      if (length(idxR)) return(best[idxR[1]])\n      idxL &lt;- grep(\"_l95\\\\.tif$\", bbest)\n      if (length(idxL)) return(best[idxL[1]])\n      return(best[1])\n    }\n  }\n\n  if (toupper(method) %in% c(\"KED\",\"PREVIEW\")) {\n    out_dir_local &lt;- dirname(method_dir)\n    prev &lt;- list.files(out_dir_local, pattern = \"\\\\.tif$\", full.names = TRUE, ignore.case = TRUE)\n    if (length(prev)) {\n      bp &lt;- tolower(basename(prev))\n      rx_prev &lt;- paste0(\"^(\", paste0(tok_rx, collapse = \"|\"), \")_interpolated(_wgs84)?\\\\.tif$\")\n      hit &lt;- grepl(rx_prev, bp, perl = TRUE)\n      if (any(hit)) return(prev[which(hit)[1]])\n    }\n  }\n  NA_character_\n}\n\n\n# Only launch interactively\nif (interactive()) {\n  explanations &lt;- build_explanations(fig_dir = fig_dir, pick_ts = vars[[pick_idx]])\n  run_mc_viewer(\n    vars         = vars,\n    method_dir   = method_dir,\n    fig_dir      = fig_dir,\n    stations_pos = stations_pos,\n    plot_area    = plot_area,\n    wf           = res_one$wf,\n    wf_ex        = res_one$wf_ex,\n    tune         = res_one$tune,\n    tune_ex      = res_one$tune_ex,\n    bench        = res_one$bench,\n    bench_ex     = res_one$bench_ex,\n    tab_err      = res_one$errtab,\n    tab_err_ex   = res_one$errtab_ex,\n    explanations = explanations\n  )\n}",
    "crumbs": [
      "Block 4: Microclimate Data Retrieval & Spatial Data Interpolation",
      "Ecowitt Air Temperature — End-to-End applied Workflow"
    ]
  },
  {
    "objectID": "block4_5/mc_2025_microclimate_viewer_merged.html#requirements-helpers",
    "href": "block4_5/mc_2025_microclimate_viewer_merged.html#requirements-helpers",
    "title": "Ecowitt Air Temperature — End-to-End applied Workflow",
    "section": "",
    "text": "# Global setup + packages\nset.seed(42)\noptions(width = 100)\n\npkgs &lt;- c(\n  \"sf\",\"terra\",\"raster\",\"dplyr\",\"automap\",\"gstat\",\"mapview\",\"stars\",\n  \"readxl\",\"stringr\",\"tidyr\",\"purrr\",\"lubridate\",\"rprojroot\",\n  \"exactextractr\",\"zoo\",\"ggplot2\",\"viridis\",\"mgcv\",\"randomForest\",\"fields\",\"sp\",\"deldir\",\n  \"leaflet\",\"DT\",\"htmltools\",\"jsonlite\",\"shiny\" # viewer deps\n)\nneed &lt;- setdiff(pkgs, rownames(installed.packages()))\nif (length(need)) install.packages(need, dependencies = TRUE)\ninvisible(lapply(pkgs, function(p) suppressPackageStartupMessages(library(p, character.only = TRUE))))\n\n\n# Small utilities (kept identical to your working script where applicable)\n# SpatRaster sicher \"pinnen\" (Datei-basiert) und als gültiges Objekt zurückgeben\n.pin_rast &lt;- function(r, crs = NULL, dir = NULL, name = \"pinned\") {\n  stopifnot(inherits(r, \"SpatRaster\"))\n  if (is.null(dir)) dir &lt;- file.path(getwd(), \"run_cache\")\n  if (!dir.exists(dir)) dir.create(dir, recursive = TRUE)\n  f &lt;- file.path(dir, paste0(name, \".tif\"))\n  # immer schreiben → garantiert datei-gestützt und voll materialisiert\n  terra::writeRaster(r, f, overwrite = TRUE)\n  rp &lt;- terra::rast(f)\n  if (!is.null(crs)) {\n    # nur projezieren, wenn noch nicht identisch\n    same &lt;- try(terra::crs(rp, proj=TRUE) == as.character(crs), silent = TRUE)\n    if (!isTRUE(same)) rp &lt;- terra::project(rp, as.character(crs), method = \"near\")\n  }\n  # Sanity check\n  invisible(terra::nlyr(rp))\n  rp\n}\n\n# Prüfer: „lebt“ der Pointer?\n.is_alive_spatr &lt;- function(r) {\n  inherits(r, \"SpatRaster\") && !inherits(try(terra::nlyr(r), silent = TRUE), \"try-error\")\n}\n\n# Safe, URL-friendly file slug\nslug &lt;- function(x) { \n  x &lt;- gsub(\"[^0-9A-Za-z_-]+\",\"-\", x)\n  x &lt;- gsub(\"-+\",\"-\", x)\n  gsub(\"(^-|-$)\",\"\", x)\n}\n\n# Human-readable time labels from AYYYY... keys\npretty_time &lt;- function(x) {\n  vapply(x, function(s) {\n    if (grepl(\"^A\\\\d{14}$\", s)) {\n      ts &lt;- as.POSIXct(substr(s, 2, 15), format = \"%Y%m%d%H%M%S\", tz = \"UTC\")\n      format(ts, \"%Y-%m-%d %H:%M\")\n    } else if (grepl(\"^A\\\\d{8}(_D)?$\", s)) {\n      ts &lt;- as.Date(substr(s, 2, 9), format = \"%Y%m%d\")\n      format(ts, \"%Y-%m-%d\")\n    } else s\n  }, character(1))\n}\n\n# Pick the most data-dense time-slice (max number of finite observations)\npick_densest_index &lt;- function(sf_wide, var_names) {\n  nn &lt;- sapply(var_names, function(v) sum(is.finite(sf_wide[[v]])))\n  which.max(nn)\n}\n\n# Build figure descriptions (viewer)\nbuild_explanations &lt;- function(fig_dir, pick_ts) {\n  ts_label &lt;- slug(pretty_time(pick_ts))\n  files &lt;- c(\n    \"timeseries_panel_grid.png\",\n    \"timeseries_panel_grid.pdf\",\n    sprintf(\"u_curve_%s.png\", ts_label),\n    sprintf(\"u_curve_extras_%s.png\", ts_label),\n    sprintf(\"benchmark_%s.png\", ts_label),\n    sprintf(\"benchmark_extras_%s.png\", ts_label)\n  )\n  paths &lt;- file.path(fig_dir, files)\n  desc  &lt;- c(\n    \"Per-timestep KED previews; dots=stations; red=plot boundary.\",\n    \"Same as PNG, vector PDF.\",\n    \"U-curve for tuning R via block-CV (drift-only).\",\n    \"U-curve with extra predictors.\",\n    \"Method comparison at R* (lower RMSE is better).\",\n    \"Benchmark with extras at R*.\"\n  )\n  keep &lt;- file.exists(paths)\n  out &lt;- as.list(desc[keep]); names(out) &lt;- basename(paths[keep])\n  out\n}\n\n\nWhy this matters: using fine pixels for scale estimation will cap \\(R\\) at a few pixels, resulting in the “10 m fallback”. Keep DEM_scale at a realistic native/coarser resolution for L/R.",
    "crumbs": [
      "Block 4: Microclimate Data Retrieval & Spatial Data Interpolation",
      "Ecowitt Air Temperature — End-to-End applied Workflow"
    ]
  },
  {
    "objectID": "block4_5/mc_2025_microclimate_viewer_merged.html#project-paths",
    "href": "block4_5/mc_2025_microclimate_viewer_merged.html#project-paths",
    "title": "Ecowitt Air Temperature — End-to-End applied Workflow",
    "section": "",
    "text": "# Robust project root finder and paths\nwd &lt;- rprojroot::find_rstudio_root_file()\n\nsource(file.path(wd, \"block4_5/all_functions_1.R\"))   # consolidated toolkit\n\nfn_DTM        &lt;- file.path(wd, \"block4_5/data_2024/copernicus_DEM.tif\")\nfn_stations   &lt;- file.path(wd, \"block4_5/data_2024/stations_prelim_modifiziert.gpkg\")\nfn_area       &lt;- file.path(wd, \"block4_5/data_2024/plot.shp\")\nfn_temp_FC29  &lt;- file.path(wd, \"block4_5/data_2024/all_GW1000A-WIFIFC29.xlsx\")\nfn_temp_DB2F  &lt;- file.path(wd, \"block4_5/data_2024/all_GW1000A-WIFIDB2F.xlsx\")\ncleandata_rds &lt;- file.path(wd, \"block4_5/data_2024/climdata.RDS\")\n\nout_dir    &lt;- file.path(wd, \"block4_5/interpolated\")\nfig_dir    &lt;- file.path(out_dir, \"fig\")\nmethod_dir &lt;- file.path(out_dir, \"methods_compare\")\nreport_dir &lt;- file.path(out_dir, \"report\")\nfor (d in c(out_dir, fig_dir, method_dir, report_dir)) if (!dir.exists(d)) dir.create(d, recursive = TRUE)\n\n# CRS\nepsg &lt;- \"EPSG:32633\"  # UTM zone 33N\nsf_crs_utm33 &lt;- sf::st_crs(epsg)",
    "crumbs": [
      "Block 4: Microclimate Data Retrieval & Spatial Data Interpolation",
      "Ecowitt Air Temperature — End-to-End applied Workflow"
    ]
  },
  {
    "objectID": "block4_5/mc_2025_microclimate_viewer_merged.html#base-data-dem-strategy",
    "href": "block4_5/mc_2025_microclimate_viewer_merged.html#base-data-dem-strategy",
    "title": "Ecowitt Air Temperature — End-to-End applied Workflow",
    "section": "",
    "text": "# DEMs\nDEM_scale  &lt;- terra::rast(\"data_2024/DEM.tif\") |&gt; terra::project(epsg)\nDEM_scale  &lt;- terra::aggregate(DEM_scale, c(20, 20))  # coarsen ~20–25 m (as in your working code)\nnames(DEM_scale) &lt;- \"altitude\"\nDEM_render &lt;- DEM_scale |&gt; terra::aggregate(fact = c(10, 10))  # for rendering products\n\ncat(\"DEM_scale res (m): \", paste(terra::res(DEM_scale),  collapse=\" x \"), \"\\n\")\n\nDEM_scale res (m):  2.00223686801611 x 2.0022368680127 \n\ncat(\"DEM_render res (m):\", paste(terra::res(DEM_render), collapse=\" x \"), \"\\n\")\n\nDEM_render res (m): 20.0223686801606 x 20.022368680127 \n\n# Stations and plot boundary → same CRS\nstations_pos &lt;- sf::st_read(fn_stations, quiet = TRUE) |&gt; sf::st_transform(sf_crs_utm33)\nplot_area    &lt;- sf::st_read(fn_area, quiet = TRUE)     |&gt; sf::st_transform(sf_crs_utm33) |&gt; sf::st_make_valid()\n\n# Altitude from DEM_scale (not the upsampled one)\nstations_pos &lt;- stations_pos %&gt;%\n  dplyr::mutate(altitude = exactextractr::exact_extract(DEM_scale, sf::st_buffer(stations_pos, 1), \"mean\"))\n\n\n  |                                                                                                \n  |                                                                                          |   0%\n  |                                                                                                \n  |======                                                                                    |   7%\n  |                                                                                                \n  |=============                                                                             |  14%\n  |                                                                                                \n  |===================                                                                       |  21%\n  |                                                                                                \n  |==========================                                                                |  29%\n  |                                                                                                \n  |================================                                                          |  36%\n  |                                                                                                \n  |=======================================                                                   |  43%\n  |                                                                                                \n  |=============================================                                             |  50%\n  |                                                                                                \n  |===================================================                                       |  57%\n  |                                                                                                \n  |==========================================================                                |  64%\n  |                                                                                                \n  |================================================================                          |  71%\n  |                                                                                                \n  |=======================================================================                   |  79%\n  |                                                                                                \n  |=============================================================================             |  86%\n  |                                                                                                \n  |====================================================================================      |  93%\n  |                                                                                                \n  |==========================================================================================| 100%",
    "crumbs": [
      "Block 4: Microclimate Data Retrieval & Spatial Data Interpolation",
      "Ecowitt Air Temperature — End-to-End applied Workflow"
    ]
  },
  {
    "objectID": "block4_5/mc_2025_microclimate_viewer_merged.html#ecowitt-ingestion-cleaning-aggregation",
    "href": "block4_5/mc_2025_microclimate_viewer_merged.html#ecowitt-ingestion-cleaning-aggregation",
    "title": "Ecowitt Air Temperature — End-to-End applied Workflow",
    "section": "",
    "text": "temp_FC29 &lt;- extract_ecowitt_core_vars(fn_temp_FC29)\n\nNew names:\nNew names:\n• `` -&gt; `...1`\n• `` -&gt; `...2`\n• `` -&gt; `...3`\n• `` -&gt; `...4`\n• `` -&gt; `...5`\n• `` -&gt; `...6`\n• `` -&gt; `...7`\n• `` -&gt; `...8`\n• `` -&gt; `...9`\n• `` -&gt; `...10`\n• `` -&gt; `...11`\n• `` -&gt; `...12`\n• `` -&gt; `...13`\n• `` -&gt; `...14`\n• `` -&gt; `...15`\n• `` -&gt; `...16`\n• `` -&gt; `...17`\n• `` -&gt; `...18`\n• `` -&gt; `...19`\n• `` -&gt; `...20`\n• `` -&gt; `...21`\n• `` -&gt; `...22`\n• `` -&gt; `...23`\n• `` -&gt; `...24`\n• `` -&gt; `...25`\n• `` -&gt; `...26`\n• `` -&gt; `...27`\n• `` -&gt; `...28`\n• `` -&gt; `...29`\n• `` -&gt; `...30`\n• `` -&gt; `...31`\n• `` -&gt; `...32`\n• `` -&gt; `...33`\n• `` -&gt; `...34`\n• `` -&gt; `...35`\n• `` -&gt; `...36`\n• `` -&gt; `...37`\n• `` -&gt; `...38`\n• `` -&gt; `...39`\n• `` -&gt; `...40`\n• `` -&gt; `...41`\n• `` -&gt; `...42`\n• `` -&gt; `...43`\n• `` -&gt; `...44`\n• `` -&gt; `...45`\n• `` -&gt; `...46`\n• `` -&gt; `...47`\n• `` -&gt; `...48`\n• `` -&gt; `...49`\n• `` -&gt; `...50`\n• `` -&gt; `...51`\n• `` -&gt; `...52`\n• `` -&gt; `...53`\n• `` -&gt; `...54`\n• `` -&gt; `...55`\n• `` -&gt; `...56`\n• `` -&gt; `...57`\n• `` -&gt; `...58`\n• `` -&gt; `...59`\n• `` -&gt; `...60`\n• `` -&gt; `...61`\n• `` -&gt; `...62`\n• `` -&gt; `...63`\n• `` -&gt; `...64`\n• `` -&gt; `...65`\n• `` -&gt; `...66`\n• `` -&gt; `...67`\n• `` -&gt; `...68`\n• `` -&gt; `...69`\n• `` -&gt; `...70`\n• `` -&gt; `...71`\n• `` -&gt; `...72`\n• `` -&gt; `...73`\n• `` -&gt; `...74`\n• `` -&gt; `...75`\n• `` -&gt; `...76`\n• `` -&gt; `...77`\n• `` -&gt; `...78`\n• `` -&gt; `...79`\n• `` -&gt; `...80`\n\ntemp_DB2F &lt;- extract_ecowitt_core_vars(fn_temp_DB2F)\n\nNew names:\nNew names:\n• `` -&gt; `...1`\n• `` -&gt; `...2`\n• `` -&gt; `...3`\n• `` -&gt; `...4`\n• `` -&gt; `...5`\n• `` -&gt; `...6`\n• `` -&gt; `...7`\n• `` -&gt; `...8`\n• `` -&gt; `...9`\n• `` -&gt; `...10`\n• `` -&gt; `...11`\n• `` -&gt; `...12`\n• `` -&gt; `...13`\n• `` -&gt; `...14`\n• `` -&gt; `...15`\n• `` -&gt; `...16`\n• `` -&gt; `...17`\n• `` -&gt; `...18`\n• `` -&gt; `...19`\n• `` -&gt; `...20`\n• `` -&gt; `...21`\n• `` -&gt; `...22`\n• `` -&gt; `...23`\n• `` -&gt; `...24`\n• `` -&gt; `...25`\n• `` -&gt; `...26`\n• `` -&gt; `...27`\n• `` -&gt; `...28`\n• `` -&gt; `...29`\n• `` -&gt; `...30`\n• `` -&gt; `...31`\n• `` -&gt; `...32`\n• `` -&gt; `...33`\n• `` -&gt; `...34`\n• `` -&gt; `...35`\n• `` -&gt; `...36`\n• `` -&gt; `...37`\n• `` -&gt; `...38`\n• `` -&gt; `...39`\n• `` -&gt; `...40`\n• `` -&gt; `...41`\n• `` -&gt; `...42`\n• `` -&gt; `...43`\n• `` -&gt; `...44`\n• `` -&gt; `...45`\n• `` -&gt; `...46`\n• `` -&gt; `...47`\n• `` -&gt; `...48`\n• `` -&gt; `...49`\n• `` -&gt; `...50`\n• `` -&gt; `...51`\n• `` -&gt; `...52`\n• `` -&gt; `...53`\n• `` -&gt; `...54`\n• `` -&gt; `...55`\n• `` -&gt; `...56`\n• `` -&gt; `...57`\n\nt_rh_all  &lt;- merge_ecowitt_logger_vars(temp_FC29, temp_DB2F)\n\n# Clean display names and map to verbose station names\nfor (meas in c(\"temperature\",\"humidity\")) {\n  t_rh_all[[meas]] &lt;- t_rh_all[[meas]] %&gt;%\n    dplyr::rename_with(~ to_verbose(.x, ifelse(meas==\"temperature\",\"Temperature\",\"Humidity\")), -Time) %&gt;%\n    clean_names()\n}\n\n# Aggregate to 3-hour steps\ntemp_agg &lt;- t_rh_all$temperature %&gt;%\n  dplyr::mutate(time = lubridate::floor_date(Time, \"3 hours\")) %&gt;%\n  dplyr::group_by(time) %&gt;%\n  dplyr::summarise(across(where(is.numeric), ~ mean(.x, na.rm = TRUE)), .groups = \"drop\")\nnames(temp_agg) &lt;- clean_ids(names(temp_agg))\n\n# long → wide matrix: station rows, time columns\ntemp_matrix &lt;- temp_agg %&gt;%\n  tidyr::pivot_longer(cols = -time, names_to = \"stationid\", values_to = \"value\") %&gt;%\n  tidyr::pivot_wider(names_from = time, values_from = value)\n\n# Join to station geometry and altitude\nstations_pos &lt;- stations_pos %&gt;% dplyr::mutate(stationid = to_verbose(stationid))\nm &lt;- dplyr::left_join(stations_pos, temp_matrix, by = \"stationid\")\n\n# Hygiene\nstations_pos$stationid &lt;- gsub(\"\\\\(℃\\\\)|\\\\(％\\\\)|\\\\(\\\\%\\\\)\", \"\", stations_pos$stationid)\nm$stationid            &lt;- gsub(\"\\\\(℃\\\\)|\\\\(％\\\\)|\\\\(\\\\%\\\\)\", \"\", m$stationid)\nnames(m)               &lt;- fix_names(names(m))\nsaveRDS(m, cleandata_rds)",
    "crumbs": [
      "Block 4: Microclimate Data Retrieval & Spatial Data Interpolation",
      "Ecowitt Air Temperature — End-to-End applied Workflow"
    ]
  },
  {
    "objectID": "block4_5/mc_2025_microclimate_viewer_merged.html#interpolation-preview-per-timestep-ked",
    "href": "block4_5/mc_2025_microclimate_viewer_merged.html#interpolation-preview-per-timestep-ked",
    "title": "Ecowitt Air Temperature — End-to-End applied Workflow",
    "section": "",
    "text": "min_pts &lt;- 5\nvars &lt;- as.list(grep(\"^A\\\\d{8,14}\", names(m), value = TRUE))\n\nkriged_list &lt;- lapply(vars, function(v) {\n  interpolate_kriging(v, m, DEM_render, output_dir = out_dir, label = \"pretty\")\n})\n\nInterpolating: A20230827150000\n\n\nWarning in interpolate_kriging(v, m, DEM_render, output_dir = out_dir, label = \"pretty\"): Variogram\nfailed for A20230827150000\n\n\nInterpolating: A20230827180000\n\n\nWarning in interpolate_kriging(v, m, DEM_render, output_dir = out_dir, label = \"pretty\"): Variogram\nfailed for A20230827180000\n\n\nInterpolating: A20230828150000\n\n\nWarning in interpolate_kriging(v, m, DEM_render, output_dir = out_dir, label = \"pretty\"): Variogram\nfailed for A20230828150000\n\n\nInterpolating: A20230828180000\n\n\nWarning in interpolate_kriging(v, m, DEM_render, output_dir = out_dir, label = \"pretty\"): Variogram\nfailed for A20230828180000\n\n\nInterpolating: A20230828210000\n\n\nWarning in interpolate_kriging(v, m, DEM_render, output_dir = out_dir, label = \"pretty\"): Variogram\nfailed for A20230828210000\n\n\nInterpolating: A20230829\n\n\nWarning in interpolate_kriging(v, m, DEM_render, output_dir = out_dir, label = \"pretty\"): Variogram\nfailed for A20230829\n\n\nInterpolating: A20230829030000\n\n\nWarning in interpolate_kriging(v, m, DEM_render, output_dir = out_dir, label = \"pretty\"): Variogram\nfailed for A20230829030000\n\n\nInterpolating: A20230829060000\n\n\nWarning in interpolate_kriging(v, m, DEM_render, output_dir = out_dir, label = \"pretty\"): Variogram\nfailed for A20230829060000\n\n\nInterpolating: A20230829090000\n\n\n[using universal kriging]\n\n\n✔ Written: /home/creu/edu/gisma-courses/EON2025/block4_5/interpolated/A20230829090000_interpolated.tif\n\n\nInterpolating: A20230829120000\n\n\n[using universal kriging]\n\n\n✔ Written: /home/creu/edu/gisma-courses/EON2025/block4_5/interpolated/A20230829120000_interpolated.tif\n\n\nInterpolating: A20230829150000\n\n\n[using universal kriging]\n\n\n✔ Written: /home/creu/edu/gisma-courses/EON2025/block4_5/interpolated/A20230829150000_interpolated.tif\n\n\nInterpolating: A20230829180000\n\n\n[using universal kriging]\n\n\n✔ Written: /home/creu/edu/gisma-courses/EON2025/block4_5/interpolated/A20230829180000_interpolated.tif\n\n\nInterpolating: A20230829210000\n\n\n[using universal kriging]\n\n\n✔ Written: /home/creu/edu/gisma-courses/EON2025/block4_5/interpolated/A20230829210000_interpolated.tif\n\n\nInterpolating: A20230830\n\n\n[using universal kriging]\n\n\n✔ Written: /home/creu/edu/gisma-courses/EON2025/block4_5/interpolated/A20230830_interpolated.tif\n\n\nInterpolating: A20230830030000\n\n\nWarning in interpolate_kriging(v, m, DEM_render, output_dir = out_dir, label = \"pretty\"): Variogram\nfailed for A20230830030000\n\n\nInterpolating: A20230830060000\n\n\nWarning in interpolate_kriging(v, m, DEM_render, output_dir = out_dir, label = \"pretty\"): Variogram\nfailed for A20230830060000\n\n\nInterpolating: A20230830090000\n\n\n[using universal kriging]\n\n\n✔ Written: /home/creu/edu/gisma-courses/EON2025/block4_5/interpolated/A20230830090000_interpolated.tif\n\n\nInterpolating: A20230830120000\n\n\n[using universal kriging]\n\n\n✔ Written: /home/creu/edu/gisma-courses/EON2025/block4_5/interpolated/A20230830120000_interpolated.tif\n\n\nInterpolating: A20230830150000\n\n\nWarning in interpolate_kriging(v, m, DEM_render, output_dir = out_dir, label = \"pretty\"): Variogram\nfailed for A20230830150000\n\n\nInterpolating: A20230830180000\n\n\nWarning in interpolate_kriging(v, m, DEM_render, output_dir = out_dir, label = \"pretty\"): Variogram\nfailed for A20230830180000\n\n\nInterpolating: A20230830210000\n\n\n[using universal kriging]\n\n\n✔ Written: /home/creu/edu/gisma-courses/EON2025/block4_5/interpolated/A20230830210000_interpolated.tif\n\n\nInterpolating: A20230831\n\n\n[using universal kriging]\n\n\n✔ Written: /home/creu/edu/gisma-courses/EON2025/block4_5/interpolated/A20230831_interpolated.tif\n\n\nInterpolating: A20230831030000\n\n\nWarning in interpolate_kriging(v, m, DEM_render, output_dir = out_dir, label = \"pretty\"): Variogram\nfailed for A20230831030000\n\n\nInterpolating: A20230831060000\n\n\nWarning in interpolate_kriging(v, m, DEM_render, output_dir = out_dir, label = \"pretty\"): Variogram\nfailed for A20230831060000\n\n\nInterpolating: A20230831090000\n\n\n[using universal kriging]\n\n\n✔ Written: /home/creu/edu/gisma-courses/EON2025/block4_5/interpolated/A20230831090000_interpolated.tif\n\n\nInterpolating: A20230831120000\n\n\n[using universal kriging]\n\n\n✔ Written: /home/creu/edu/gisma-courses/EON2025/block4_5/interpolated/A20230831120000_interpolated.tif\n\n\nInterpolating: A20230831150000\n\n\nWarning in interpolate_kriging(v, m, DEM_render, output_dir = out_dir, label = \"pretty\"): Variogram\nfailed for A20230831150000\n\n\nInterpolating: A20230831180000\n\n\nWarning in interpolate_kriging(v, m, DEM_render, output_dir = out_dir, label = \"pretty\"): Variogram\nfailed for A20230831180000\n\n\nInterpolating: A20230831210000\n\n\n[using universal kriging]\n\n\n✔ Written: /home/creu/edu/gisma-courses/EON2025/block4_5/interpolated/A20230831210000_interpolated.tif\n\n\nInterpolating: A20230901\n\n\nWarning in interpolate_kriging(v, m, DEM_render, output_dir = out_dir, label = \"pretty\"): Variogram\nfailed for A20230901\n\nnames(kriged_list) &lt;- vars\n\n\n# Shared color scale across all timestamps\npanel &lt;- timeseries_panel(\n  kriged_list        = kriged_list,\n  plot_area          = plot_area,\n  stations_pos       = stations_pos,\n  cells_target       = 150000,\n  max_cols           = 4,\n  label_pretty_time  = TRUE,\n  out_png            = file.path(fig_dir, \"timeseries_panel_grid.png\"),\n  out_pdf            = file.path(fig_dir, \"timeseries_panel_grid.pdf\"),\n  fill_label         = \"Temperature\"\n)\n\nCoordinate system already present. Adding new coordinate system, which will replace the existing\none.\n\npanel$plot",
    "crumbs": [
      "Block 4: Microclimate Data Retrieval & Spatial Data Interpolation",
      "Ecowitt Air Temperature — End-to-End applied Workflow"
    ]
  },
  {
    "objectID": "block4_5/mc_2025_microclimate_viewer_merged.html#method-comparison-for-densest-timestamp-working-code",
    "href": "block4_5/mc_2025_microclimate_viewer_merged.html#method-comparison-for-densest-timestamp-working-code",
    "title": "Ecowitt Air Temperature — End-to-End applied Workflow",
    "section": "",
    "text": "# Densest timestamp\npick_idx &lt;- pick_densest_index(m, vars)\npick_ts  &lt;- names(m)[pick_idx]\n\n# Extra predictors (kept as in your working code)\nextra_list &lt;- list(\n  slope  = terra::terrain(DEM_scale, v = \"slope\",  unit = \"degrees\"),\n  aspect = terra::terrain(DEM_scale, v = \"aspect\"),\n  tri    = terra::terrain(DEM_scale, v = \"TRI\")\n)\n\n# One timestamp (densest), with extras\nres_one &lt;- run_one(\n  v           = vars[[pick_idx]],\n  m           = m,\n  DEM_render  = DEM_render,\n  DEM_scale   = DEM_scale,\n  method_dir  = method_dir,\n  fig_dir     = fig_dir,\n  report_dir  = report_dir,\n  extra_preds = extra_list,\n  save_figs   = TRUE\n)\n\nWarning in newton(lsp = lsp, X = G$X, y = G$y, Eb = G$Eb, UrS = G$UrS, L = G$L, : Fitting\nterminated with step failure - check results carefully\n\n\nWarning in log(b$scale.est): NaNs produced\nWarning in log(b$scale.est): NaNs produced\n\n\nWarning in newton(lsp = lsp, X = G$X, y = G$y, Eb = G$Eb, UrS = G$UrS, L = G$L, : Fitting\nterminated with step failure - check results carefully\n\n\nWarning in log(b$scale.est): NaNs produced\n\n\nWarning in newton(lsp = lsp, X = G$X, y = G$y, Eb = G$Eb, UrS = G$UrS, L = G$L, : Fitting\nterminated with step failure - check results carefully\nWarning in newton(lsp = lsp, X = G$X, y = G$y, Eb = G$Eb, UrS = G$UrS, L = G$L, : Fitting\nterminated with step failure - check results carefully\nWarning in newton(lsp = lsp, X = G$X, y = G$y, Eb = G$Eb, UrS = G$UrS, L = G$L, : Fitting\nterminated with step failure - check results carefully\n\n\nWarning in log(b$scale.est): NaNs produced\n\n\nWarning in newton(lsp = lsp, X = G$X, y = G$y, Eb = G$Eb, UrS = G$UrS, L = G$L, : Fitting\nterminated with step failure - check results carefully\n\n\nWarning in log(b$scale.est): NaNs produced\n\n\nWarning in newton(lsp = lsp, X = G$X, y = G$y, Eb = G$Eb, UrS = G$UrS, L = G$L, : Fitting\nterminated with step failure - check results carefully\n\n\nWarning in log(b$scale.est): NaNs produced\n\n\nWarning in newton(lsp = lsp, X = G$X, y = G$y, Eb = G$Eb, UrS = G$UrS, L = G$L, : Fitting\nterminated with step failure - check results carefully\nWarning in newton(lsp = lsp, X = G$X, y = G$y, Eb = G$Eb, UrS = G$UrS, L = G$L, : Fitting\nterminated with step failure - check results carefully\nWarning in newton(lsp = lsp, X = G$X, y = G$y, Eb = G$Eb, UrS = G$UrS, L = G$L, : Fitting\nterminated with step failure - check results carefully\nWarning in newton(lsp = lsp, X = G$X, y = G$y, Eb = G$Eb, UrS = G$UrS, L = G$L, : Fitting\nterminated with step failure - check results carefully\n\n\nWarning in log(b$scale.est): NaNs produced\n\n\nWarning in newton(lsp = lsp, X = G$X, y = G$y, Eb = G$Eb, UrS = G$UrS, L = G$L, : Fitting\nterminated with step failure - check results carefully\nWarning in newton(lsp = lsp, X = G$X, y = G$y, Eb = G$Eb, UrS = G$UrS, L = G$L, : Fitting\nterminated with step failure - check results carefully\n\n\n[inverse distance weighted interpolation]\n[using ordinary kriging]\n[using universal kriging]\n[inverse distance weighted interpolation]\n[using ordinary kriging]\n[using universal kriging]\n\n\nR*: tuner failed; falling back to 56.8593 m.\n\n\nWarning in x@pntr$rastDistance(target, exclude, keepNA, tolower(unit), TRUE, : GDAL Message 1:\nPixels not square, distances will be inaccurate.\n\n\n[using ordinary kriging]\n[inverse distance weighted interpolation]\n[using universal kriging]\n[using ordinary kriging]\n[inverse distance weighted interpolation]\n[using universal kriging]\n[using ordinary kriging]\n[inverse distance weighted interpolation]\n[using universal kriging]\n[using ordinary kriging]\n[inverse distance weighted interpolation]\n[using universal kriging]\n[using ordinary kriging]\n[inverse distance weighted interpolation]\n[using universal kriging]\n[using ordinary kriging]\n[inverse distance weighted interpolation]\n[using universal kriging]\n\n\nR*: tuner failed; falling back to 56.8593 m.\n\n\nWarning in x@pntr$rastDistance(target, exclude, keepNA, tolower(unit), TRUE, : GDAL Message 1:\nPixels not square, distances will be inaccurate.\n\n\n[using ordinary kriging]\n[inverse distance weighted interpolation]\n[using universal kriging]\n[using ordinary kriging]\n[inverse distance weighted interpolation]\n[using universal kriging]\n[using ordinary kriging]\n[inverse distance weighted interpolation]\n[using universal kriging]\n[using ordinary kriging]\n[inverse distance weighted interpolation]\n[using universal kriging]\n[using ordinary kriging]\n[inverse distance weighted interpolation]\n[using universal kriging]\n[using ordinary kriging]\n[inverse distance weighted interpolation]\n[using universal kriging]\n\n\n[using ordinary kriging]\n[using ordinary kriging]\n[using ordinary kriging]\n[using ordinary kriging]\n[using ordinary kriging]\n[using ordinary kriging]\n[using ordinary kriging]\n[using ordinary kriging]\n[using ordinary kriging]\n[using ordinary kriging]\n[using ordinary kriging]\n[using ordinary kriging]\n[using ordinary kriging]\n[using ordinary kriging]\n[using ordinary kriging]\n[using ordinary kriging]",
    "crumbs": [
      "Block 4: Microclimate Data Retrieval & Spatial Data Interpolation",
      "Ecowitt Air Temperature — End-to-End applied Workflow"
    ]
  },
  {
    "objectID": "block4_5/mc_2025_microclimate_viewer_merged.html#optional-compute-all-time-steps",
    "href": "block4_5/mc_2025_microclimate_viewer_merged.html#optional-compute-all-time-steps",
    "title": "Ecowitt Air Temperature — End-to-End applied Workflow",
    "section": "",
    "text": "compute_all &lt;- FALSE\n\nif (isTRUE(compute_all)) {\n  req &lt;- c(\"m\",\"DEM_render\",\"DEM_scale\",\"method_dir\",\"fig_dir\",\"report_dir\")\n  miss &lt;- req[!vapply(req, exists, logical(1), inherits = TRUE)]\n  if (length(miss)) stop(\"Fehlende Objekte im Environment: \", paste(miss, collapse = \", \"))\n\n  .best_from_bench &lt;- function(bench_obj) {\n    if (is.null(bench_obj) || !is.data.frame(bench_obj$table) || nrow(bench_obj$table) &lt; 1)\n      return(NULL)\n    b &lt;- bench_obj$table\n    b &lt;- b[is.finite(b$RMSE), , drop = FALSE]\n    if (!nrow(b)) return(NULL)\n    b &lt;- b[order(b$RMSE), , drop = FALSE]\n    b[1, c(\"method\",\"RMSE\"), drop = FALSE]\n  }\n\n  message(sprintf(\"Starte compute_all für %d Zeitschritte …\", length(vars)))\n\n  res_all &lt;- setNames(lapply(vars, function(vv) {\n    message(\"→ run_one: \", pretty_time(vv))\n    tryCatch(\n      run_one(\n        v           = vv,\n        m           = m,\n        DEM_render  = DEM_render,\n        DEM_scale   = DEM_scale,\n        method_dir  = method_dir,\n        fig_dir     = fig_dir,\n        report_dir  = report_dir,\n        extra_preds = extra_list,\n        save_figs   = TRUE,\n        save_tables = TRUE\n      ),\n      error = function(e) {\n        warning(\"run_one fehlgeschlagen für \", vv, \": \", conditionMessage(e))\n        NULL\n      }\n    )\n  }), vars)\n\n  saveRDS(res_all, file.path(report_dir, \"all_results.RDS\"))\n\n  summ &lt;- do.call(rbind, lapply(names(res_all), function(k) {\n    r &lt;- res_all[[k]]\n    if (is.null(r)) {\n      return(data.frame(\n        ts_key      = k, \n        stamp       = pretty_time(k),\n        R_star      = NA_real_,\n        best_source = NA_character_,  # \"no_extras\"/\"with_extras\"\n        best_method = NA_character_,\n        best_RMSE   = NA_real_\n      ))\n    }\n    rstar &lt;- suppressWarnings(as.numeric(r$tune$R_star))\n    if (!is.finite(rstar)) rstar &lt;- NA_real_\n\n    b0 &lt;- .best_from_bench(r$bench)\n    bE &lt;- .best_from_bench(r$bench_ex)\n\n    score0 &lt;- if (!is.null(b0) && isTRUE(is.finite(b0$RMSE))) b0$RMSE else Inf\n    scoreE &lt;- if (!is.null(bE) && isTRUE(is.finite(bE$RMSE))) bE$RMSE else Inf\n\n    if (is.infinite(score0) && is.infinite(scoreE)) {\n      src &lt;- NA_character_; bm &lt;- NA_character_; br &lt;- NA_real_\n    } else if (score0 &lt;= scoreE) {\n      src &lt;- \"no_extras\"; bm &lt;- b0$method; br &lt;- score0\n    } else {\n      src &lt;- \"with_extras\"; bm &lt;- bE$method; br &lt;- scoreE\n    }\n\n    data.frame(\n      ts_key      = k,\n      stamp       = pretty_time(k),\n      R_star      = rstar,\n      best_source = src,\n      best_method = bm,\n      best_RMSE   = br\n    )\n  }))\n\n  utils::write.csv(summ, file.path(report_dir, \"summary_Rstar_bestmethod.csv\"), row.names = FALSE)\n  message(\"✔ Fertig: summary_Rstar_bestmethod.csv geschrieben.\")\n}",
    "crumbs": [
      "Block 4: Microclimate Data Retrieval & Spatial Data Interpolation",
      "Ecowitt Air Temperature — End-to-End applied Workflow"
    ]
  },
  {
    "objectID": "block4_5/mc_2025_microclimate_viewer_merged.html#save-csvs-for-the-one-timestamp",
    "href": "block4_5/mc_2025_microclimate_viewer_merged.html#save-csvs-for-the-one-timestamp",
    "title": "Ecowitt Air Temperature — End-to-End applied Workflow",
    "section": "",
    "text": "ts_label &lt;- pretty_time(pick_ts)\nbench_base_csv &lt;- file.path(report_dir, sprintf(\"benchmark_%s.csv\",        slug(ts_label)))\nbench_ex_csv   &lt;- file.path(report_dir, sprintf(\"benchmark_extras_%s.csv\", slug(ts_label)))\neb_base_csv    &lt;- file.path(report_dir, sprintf(\"error_budget_%s.csv\",     slug(ts_label)))\neb_ex_csv      &lt;- file.path(report_dir, sprintf(\"error_budget_extras_%s.csv\", slug(ts_label)))\n\nif (is.list(res_one$bench)    && is.data.frame(res_one$bench$table))    write.csv(res_one$bench$table,    bench_base_csv, row.names = FALSE)\nif (is.list(res_one$bench_ex) && is.data.frame(res_one$bench_ex$table)) write.csv(res_one$bench_ex$table, bench_ex_csv,   row.names = FALSE)\nif (is.data.frame(res_one$errtab))    write.csv(res_one$errtab,    eb_base_csv, row.names = FALSE)\nif (is.data.frame(res_one$errtab_ex)) write.csv(res_one$errtab_ex, eb_ex_csv,   row.names = FALSE)",
    "crumbs": [
      "Block 4: Microclimate Data Retrieval & Spatial Data Interpolation",
      "Ecowitt Air Temperature — End-to-End applied Workflow"
    ]
  },
  {
    "objectID": "block4_5/mc_2025_microclimate_viewer_merged.html#console-summary",
    "href": "block4_5/mc_2025_microclimate_viewer_merged.html#console-summary",
    "title": "Ecowitt Air Temperature — End-to-End applied Workflow",
    "section": "",
    "text": "n_stations &lt;- nrow(stations_pos)\nn_pts_ts   &lt;- sum(is.finite(m[[pick_ts]]))\nLs         &lt;- get_Ls(res_one$wf$L)\nLs_e       &lt;- if (!is.null(res_one$wf_ex)) get_Ls(res_one$wf_ex$L) else NULL\nRstar_base &lt;- suppressWarnings(as.numeric(res_one$tune$R_star))\nRstar_ex   &lt;- suppressWarnings(as.numeric(res_one$tune_ex$R_star))\n\ncat(sprintf(\"Chosen R (micro/local): %s / %s m\\n\",\n            ifelse(is.finite(res_one$wf$R['micro']), round(res_one$wf$R['micro']), \"NA\"),\n            ifelse(is.finite(res_one$wf$R['local']), round(res_one$wf$R['local']), \"NA\")\n))\n\nChosen R (micro/local): 19 / 57 m",
    "crumbs": [
      "Block 4: Microclimate Data Retrieval & Spatial Data Interpolation",
      "Ecowitt Air Temperature — End-to-End applied Workflow"
    ]
  },
  {
    "objectID": "block4_5/mc_2025_microclimate_viewer_merged.html#shiny-viewer-optional-uses-existing-files",
    "href": "block4_5/mc_2025_microclimate_viewer_merged.html#shiny-viewer-optional-uses-existing-files",
    "title": "Ecowitt Air Temperature — End-to-End applied Workflow",
    "section": "",
    "text": "The viewer launches a Shiny app; to not block Quarto rendering, the launch is wrapped in if (interactive()).\n\n\n# Minimal viewer injection: use your working run_mc_viewer() with robust file matching\nraster_path &lt;- function(method, ts) {\n  stopifnot(length(method) == 1, length(ts) == 1)\n  m &lt;- tolower(method)\n  .ts_tokens &lt;- function(ts_key) {\n    raw &lt;- tolower(as.character(ts_key))\n    pty &lt;- tolower(pretty_time(ts_key))\n    slug_pt &lt;- gsub(\"[^0-9A-Za-z_-]+\",\"-\", pty)\n    d14 &lt;- sub(\"^a\", \"\", raw)\n    ymd  &lt;- if (nchar(d14) &gt;= 8) substr(d14,1,8) else NA_character_\n    hhmm &lt;- if (nchar(d14) &gt;= 12) substr(d14,9,12) else NA_character_\n    comp1 &lt;- if (!is.na(ymd) && !is.na(hhmm))\n      paste0(substr(ymd,1,4),\"-\",substr(ymd,5,6),\"-\",substr(ymd,7,8),\"-\",\n             substr(hhmm,1,2),\"-\",substr(hhmm,3,4)) else NA_character_\n    comp2 &lt;- gsub(\"-\", \"\", comp1)\n    ymd_dash &lt;- if (!is.na(ymd)) paste0(substr(ymd,1,4),\"-\",substr(ymd,5,6),\"-\",substr(ymd,7,8)) else NA_character_\n    unique(na.omit(c(raw, slug_pt, comp1, comp2, ymd_dash, ymd)))\n  }\n  toks &lt;- .ts_tokens(ts)\n  tok_rx &lt;- gsub(\"[-_]\", \"[-_]\", toks)\n\n  all_files &lt;- list.files(method_dir, pattern = \"\\\\.tif$\", full.names = TRUE, ignore.case = TRUE)\n  if (!length(all_files)) return(NA_character_)\n  b &lt;- tolower(basename(all_files))\n\n  keep_pref &lt;- grepl(paste0(\"^\", m, \"_\"), b)\n  files_m &lt;- all_files[keep_pref]; b_m &lt;- b[keep_pref]\n  if (length(files_m)) {\n    score &lt;- vapply(seq_along(b_m), function(i) {\n      max(c(0, vapply(tok_rx, function(rx) if (grepl(rx, b_m[i], perl = TRUE)) nchar(rx) else 0L, integer(1))))\n    }, numeric(1))\n\n    if (any(score &gt; 0)) {\n      best &lt;- files_m[score == max(score)]\n      bbest &lt;- tolower(basename(best))\n      idxR &lt;- grep(\"_rstar\\\\.tif$\", bbest)\n      if (length(idxR)) return(best[idxR[1]])\n      idxL &lt;- grep(\"_l95\\\\.tif$\", bbest)\n      if (length(idxL)) return(best[idxL[1]])\n      return(best[1])\n    }\n  }\n\n  if (toupper(method) %in% c(\"KED\",\"PREVIEW\")) {\n    out_dir_local &lt;- dirname(method_dir)\n    prev &lt;- list.files(out_dir_local, pattern = \"\\\\.tif$\", full.names = TRUE, ignore.case = TRUE)\n    if (length(prev)) {\n      bp &lt;- tolower(basename(prev))\n      rx_prev &lt;- paste0(\"^(\", paste0(tok_rx, collapse = \"|\"), \")_interpolated(_wgs84)?\\\\.tif$\")\n      hit &lt;- grepl(rx_prev, bp, perl = TRUE)\n      if (any(hit)) return(prev[which(hit)[1]])\n    }\n  }\n  NA_character_\n}\n\n\n# Only launch interactively\nif (interactive()) {\n  explanations &lt;- build_explanations(fig_dir = fig_dir, pick_ts = vars[[pick_idx]])\n  run_mc_viewer(\n    vars         = vars,\n    method_dir   = method_dir,\n    fig_dir      = fig_dir,\n    stations_pos = stations_pos,\n    plot_area    = plot_area,\n    wf           = res_one$wf,\n    wf_ex        = res_one$wf_ex,\n    tune         = res_one$tune,\n    tune_ex      = res_one$tune_ex,\n    bench        = res_one$bench,\n    bench_ex     = res_one$bench_ex,\n    tab_err      = res_one$errtab,\n    tab_err_ex   = res_one$errtab_ex,\n    explanations = explanations\n  )\n}",
    "crumbs": [
      "Block 4: Microclimate Data Retrieval & Spatial Data Interpolation",
      "Ecowitt Air Temperature — End-to-End applied Workflow"
    ]
  },
  {
    "objectID": "block4_5/mc_2025_1.html",
    "href": "block4_5/mc_2025_1.html",
    "title": "PipeModel Idealized valley microclimate sandbox",
    "section": "",
    "text": "The PipeModel is a deliberately idealized yet physically plausible valley scenario. It distills terrain to the essentials (parabolic cross-valley profile) and optional features (left-side hill, right-side pond or hollow), so that dominant microclimate drivers become visible and testable:\nYou can sample synthetic stations, train interpolators (IDW, Kriging variants, RF, GAM), and assess them with spatial LBO-CV.",
    "crumbs": [
      "Block 4: Microclimate Data Retrieval & Spatial Data Interpolation",
      "PipeModel Idealized valley microclimate sandbox"
    ]
  },
  {
    "objectID": "block4_5/mc_2025_1.html#what-the-pipemodel-is",
    "href": "block4_5/mc_2025_1.html#what-the-pipemodel-is",
    "title": "PipeModel Idealized valley microclimate sandbox",
    "section": "What the pipemodel is",
    "text": "What the pipemodel is\nA didactic, reproducible pipeline for micro-scale spatial prediction with process-aware features and scale tuning. It simulates or ingests a domain (“scenario”), learns from station points, validates with leave-block-out CV, infers a characteristic scale (R*) from data, re-trains at that scale, and produces tuned maps, diagnostics, and optional exports.",
    "crumbs": [
      "Block 4: Microclimate Data Retrieval & Spatial Data Interpolation",
      "PipeModel Idealized valley microclimate sandbox"
    ]
  },
  {
    "objectID": "block4_5/mc_2025_1.html#architecture",
    "href": "block4_5/mc_2025_1.html#architecture",
    "title": "PipeModel Idealized valley microclimate sandbox",
    "section": "Architecture",
    "text": "Architecture\n\nMain (orchestrator): main_ultra.R is intentionally thin: it wires pieces together, runs the stages in order, shows live previews, and—optionally—saves outputs at the end. No heavy lifting.\n\n\n\nCode\n```{r}\n#| eval: false\n\n  # =====================================================================\n  # main_ultra.R — minimal: run + (optional) save-at-end\n  #\n  # Purpose:\n  #   1) Source packages + your function library + scenario registry\n  #   2) Build scenario + station sets from a chosen scenario \"make()\" factory\n  #   3) Live preview: domain/land-cover/terrain + 2x2 overview + scenario preview\n  #   4) Baseline: leave-block-out CV (LBO-CV) and prediction maps (T14, T05)\n  #   5) Scale inference: empirical variogram -&gt; (L50,L95) -&gt; U-curve -&gt; R*\n  #   6) Tuned CV & maps at R*\n  #   7) OPTIONAL: save all plots/tables/rasters at the end\n  #\n  # Design notes:\n  #   - This script stays \"thin\": all heavy lifting lives in fun_pipemodel.R\n  #     and the scenario files. This keeps the main pipe reproducible and\n  #     testable.\n  #   - Keep side effects (saving files) to the very end; set `export &lt;- FALSE`\n  #     if you just want to run and eyeball plots interactively.\n  # =====================================================================\n  \n  message(\"=== pipe main (ultra) ===\")\n  \n  # Toggle: set to FALSE to only run/plot without saving anything\n  export &lt;- TRUE\n  \n  # ---------------------------------------------------------------------\n  # 0) Setup & packages (centralized in your packages.R)\n  #    - Loads CRAN pkgs (terra, sf, ggplot2, mgcv, gstat, suncalc, ...)\n  #    - Sets knitr options (if used in a notebook context)\n  #    - We also disable spherical geometry in sf to keep planar ops robust\n  #      for projected domains (UTM in our scenarios).\n  # ---------------------------------------------------------------------\n  source(here::here(\"block4_5/src/packages.R\"))\n  sf::sf_use_s2(FALSE)\n  set.seed(42)  # one seed here (scenarios may set more where needed)\n  \n  # ---------------------------------------------------------------------\n  # 1) Functions + Scenario registry\n  #    - fun_pipemodel.R: your full function library (NO side effects)\n  #    - registry.R: maps scenario names to files; exposes source_scenario()\n  #      which returns a `make()` function to build the object.\n  # ---------------------------------------------------------------------\n  source(here::here(\"block4_5/src/fun_pipemodel.R\"))\n  source(here::here(\"block4_5/src/fun_learn_predict_core.R\"))\n  source(here::here(\"block4_5/scenarios/registry.R\"))\n  \n  # ---------------------------------------------------------------------\n  # 2) Pick a scenario\n  #    - Choose via env var SCEN (e.g., export SCEN=scen_scaled_demo)\n  #    - Defaults to \"lake_bump_dense\" which is a realistic didactic scene\n  #      (valley, lake, bump hill, dense micro-relief, LC = forest/water/bare/baseline meadow).\n  # ---------------------------------------------------------------------\n  #scen_name &lt;- Sys.getenv(\"SCEN\", \"lake_bump_dense\")\n  scen_name &lt;- Sys.getenv(\"SCEN\", \"scen_scaled_demo\")\n  make_fun  &lt;- source_scenario(scen_name)  # returns a function make(overrides=list(), do_cv=FALSE)\n  \n  # ---------------------------------------------------------------------\n  # 3) Build the object (domain + scenario + stations + params)\n  #    - `obj` is a list with a stable contract used downstream:\n  #        scen: list of rasters (E, R14, R05, I14, I05, lc, sun, ...)\n  #        stn_sf_14 / stn_sf_05: station sf at 14/05 UTC (features + temp)\n  #        block_size: integer (meters) used by LBO-CV\n  #        params$models: character vector of model names to run\n  # ---------------------------------------------------------------------\n  obj  &lt;- make_fun()\n  scen &lt;- obj$scen\n  st14 &lt;- obj$stn_sf_14\n  st05 &lt;- obj$stn_sf_05\n  bs   &lt;- obj$block_size\n  mods &lt;- obj$params$models\n  \n  # --- Safety checks that catch common wiring issues early ----------------------\n  stopifnot(inherits(st14, \"sf\"), inherits(st05, \"sf\"))\n  stopifnot(all(c(\"E\",\"R14\",\"R05\") %in% names(scen)))\n  \n  # Sun geometry must be present for the R*-tuning (cosine-of-incidence features)\n  if (is.null(scen$sun) || is.null(scen$sun$T14) || is.null(scen$sun$T05)) {\n    stop(\"Scenario did not populate scen$sun$T14 / scen$sun$T05 (alt/az). \",\n         \"Fix in the scenario builder (build_scenario) before tuning.\")\n  }\n  if (any(is.null(c(scen$sun$T14$alt, scen$sun$T14$az,\n                    scen$sun$T05$alt, scen$sun$T05$az)))) {\n    stop(\"Sun angles (alt/az) are NULL. Scenario must supply numeric alt/az for T14/T05.\")\n  }\n  \n  # ---------------------------------------------------------------------\n  # 4) Live preview: plots during the run (no side effects)\n  #    Why plot first?\n  #      - Instant sanity checks: station placement, LC, illumination maps\n  #      - Early visual cues if something is off (e.g., CRS mismatch)\n  # ---------------------------------------------------------------------\n  print(plot_landcover_terrain(scen, stations = st14, layout = \"vertical\"))\n  print(plot_block_overview_2x2_en(scen, pts_sf = st14))\n  # preview_scenario() may show truth fields, histograms, thumbnails, etc.\n  print(preview_scenario(obj))  # accepts obj or obj$scen in the robust version\n  \n    # ---------------------------------------------------------------------\n  # 5) Baseline LBO-CV & prediction maps\n  #    - For each time slice (T14 day / T05 pre-dawn):\n  #      1) Run leave-block-out CV on station data\n  #      2) Produce truth vs predicted raster maps (model ensemble)\n  #    - Diagnostics printed/printed:\n  #      * Metrics table (RMSE/MAE/R2 per model)\n  #      * Blocks plot (spatial CV blocks)\n  #      * Pred-vs-obs scatter plot\n  #      * Truth and prediction maps\n  # ---------------------------------------------------------------------\n  run14 &lt;- run_for_time(st14, scen$R14, \"T14\", scen_local = scen, block_m = bs, models = mods)\n  run05 &lt;- run_for_time(st05, scen$R05, \"T05\", scen_local = scen, block_m = bs, models = mods)\n  \n  cat(\"\\n== Metrics T14 ==\\n\"); print(run14$res$metrics)\n  cat(\"\\n== Metrics T05 ==\\n\"); print(run05$res$metrics)\n  \n  print(run14$res$blocks_plot); print(run14$res$diag_plot)\n  print(run05$res$blocks_plot); print(run05$res$diag_plot)\n  print(run14$maps$p_truth);    print(run14$maps$p_pred)\n  print(run05$maps$p_truth);    print(run05$maps$p_pred)\n  \n  # =====================================================================\n  # 6) SCALE → R* tuning → tuned CV + maps\n  #\n  # Pipeline rationale:\n  #   (a) Variogram reveals correlation ranges in the point field.\n  #   (b) U-curve scans DEM-smoothing radii ~ [L50, L95] to find data-driven R*.\n  #       Each radius implies a different \"macro-signal\" (E*, slope*, cosi*).\n  #       We refit CV at each R and pick the RMSE-minimizer (R*).\n  #   (c) With R* in hand, we derive feature rasters at that scale: E*, slp*, cosi*.\n  #   (d) Re-extract station features at R* to ensure training/prediction consistency.\n  #   (e) Run LBO-CV again (tuned) using E* as the reference raster for blocks/domain.\n  #   (f) Predict tuned maps by injecting the feature rasters.\n  #   (g) Build compact multi-model panels with residual diagnostics.\n  #\n  # Performance tips if tuning feels slow:\n  #   - Reduce n_grid in the U-curve (e.g., 5 instead of 9).\n  #     n_grid sets how many candidate smoothing radii are tested in the U-curve search for R*\n  #   - Trim `mods` to a smaller set while teaching the concept.\n  #   - Increase block size slightly (fewer blocks → fewer CV folds).\n  # =====================================================================\n  \n  # --- (a) Variogram → L50/L95 -------------------------------------------------\n  Ls14 &lt;- compute_Ls_from_points(st14, value_col = \"temp\")\n  Ls05 &lt;- compute_Ls_from_points(st05, value_col = \"temp\")\n  \n  p_vg14 &lt;- plot_variogram_with_scales(Ls14$vg, Ls14$L50, Ls14$L95, Ls14$sill,\n                                       \"T14 — empirical variogram\")\n  p_vg05 &lt;- plot_variogram_with_scales(Ls05$vg, Ls05$L50, Ls05$L95, Ls05$sill,\n                                       \"T05 — empirical variogram\")\n  print(p_vg14); print(p_vg05)\n  \n  # --- (b) U-curve → R* --------------------------------------------------------\n  # We pass *explicit* sun angles so tune_Rstar_ucurve() can build cosi@R\n  # consistently with the scenario's solar geometry.\n  tune14 &lt;- tune_Rstar_ucurve(\n    stn_sf = st14,\n    E      = scen$E,\n    alt    = scen$sun$T14$alt,\n    az     = scen$sun$T14$az,\n    L50    = Ls14$L50,\n    L95    = Ls14$L95,\n    block_fallback = bs,\n    n_grid = 6\n  )\n  \n  tune05 &lt;- tune_Rstar_ucurve(\n    stn_sf = st05,\n    E      = scen$E,\n    alt    = scen$sun$T05$alt,\n    az     = scen$sun$T05$az,\n    L50    = Ls05$L50,\n    L95    = Ls05$L95,\n    block_fallback = bs,\n    n_grid = 6\n  )\n  \n  # Plot the U-curves and report chosen R* (rounded for readability).\n  p_uc14 &lt;- plot_ucurve(tune14$grid, tune14$R_star, \"T14 — U-curve\")\n  p_uc05 &lt;- plot_ucurve(tune05$grid, tune05$R_star, \"T05 — U-curve\")\n  print(p_uc14); print(p_uc05)\n  \n  # IMPORTANT: use %.0f (not %d) because R* is numeric (may be non-integer).\n  message(sprintf(\"Chosen R* — T14: %.0f m | blocks ≈ %d m\", tune14$R_star, tune14$block_m))\n  message(sprintf(\"Chosen R* — T05: %.0f m | blocks ≈ %d m\", tune05$R_star, tune05$block_m))\n  \n  # --- (c) Feature rasters @R* -------------------------------------------------\n  # Smooth DEM at R* and derive slope/incident-cosine given the scenario sun angles.\n  fr14 &lt;- smooth_dem_and_derive(\n    scen$E, scen$sun$T14$alt, scen$sun$T14$az, radius_m = tune14$R_star\n  )\n  fr05 &lt;- smooth_dem_and_derive(\n    scen$E, scen$sun$T05$alt, scen$sun$T05$az, radius_m = tune05$R_star\n  )\n  \n  # --- (d) Station features @R* ------------------------------------------------\n  # Re-extract E*, slope*, cosi* (plus consistent LC factors) at station points.\n  # This keeps training features aligned with the tuned raster features.\n  st14_R &lt;- add_drifts_at_R(\n    st14, scen$E, scen$sun$T14$alt, scen$sun$T14$az, tune14$R_star,\n    lc = scen$lc, lc_levels = scen$lc_levels,\n    na_action = \"fill\"   # or \"drop\" if you prefer to omit affected stations\n  )\n  st05_R &lt;- add_drifts_at_R(\n    st05, scen$E, scen$sun$T05$alt, scen$sun$T05$az, tune05$R_star,\n    lc = scen$lc, lc_levels = scen$lc_levels,\n    na_action = \"fill\"   # or \"drop\" if you prefer to omit affected stations\n  )\n  \n  # --- (e) LBO-CV @R* ----------------------------------------------------------\n  # Use the tuned smoothed DEM (E*) as the reference for CV blocks and domain\n  # geometry so the CV respects the working resolution/scale of the model.\n  bench14 &lt;- run_lbo_cv(st14_R, E = scen$E, block_size = bs, models = mods)\n  bench05 &lt;- run_lbo_cv(st05_R, E = scen$E, block_size = bs, models = mods)\n  print(bench14$metrics); print(bench05$metrics)\n  \n  # --- (f) Tuned maps ----------------------------------------------------------\n  # Inject the tuned feature rasters so model predictions operate at R* scale.\n  maps14_tuned &lt;- predict_maps(\n    stn_sf = st14_R, truth_raster = scen$R14, which_time = \"T14\",\n    scen = scen, models = mods, lc_levels = scen$lc_levels,\n    feature_rasters = list(E = fr14$Es, slp = fr14$slp, cosi = fr14$cosi)\n  )\n  maps05_tuned &lt;- predict_maps(\n    stn_sf = st05_R, truth_raster = scen$R05, which_time = \"T05\",\n    scen = scen, models = mods, lc_levels = scen$lc_levels,\n    feature_rasters = list(E = fr05$Es, slp = fr05$slp, cosi = fr05$cosi)\n  )\n  \n  # --- (g) Panels: truth | predictions | residual diagnostics ------------------\n  panel_T14 &lt;- build_panels_truth_preds_errors_paged(\n    maps = maps14_tuned, truth_raster = scen$R14, cv_tbl = bench14$cv,\n    which_time = \"T14\", models_per_page = 7, scatter_next_to_truth = TRUE\n  )\n  panel_T05 &lt;- build_panels_truth_preds_errors_paged(\n    maps = maps05_tuned, truth_raster = scen$R05, cv_tbl = bench05$cv,\n    which_time = \"T05\", models_per_page = 7, scatter_next_to_truth = TRUE\n  )\n  print(panel_T14[[1]]); print(panel_T05[[1]])\n  \n  \n  # Sensor noise (°C) – from specs or co-location\n  sigma_inst &lt;- 0.5\n  \n  # α from residual variogram (microscale share) – T14 and T05\n  alpha14 &lt;- nugget_fraction_from_cv(bench14$cv, model = \"RF\", crs_ref = st14)\n  alpha05 &lt;- nugget_fraction_from_cv(bench05$cv, model = \"RF\", crs_ref = st05)\n  \n  # Fallbacks if the fit fails\n  if (!is.finite(alpha14)) alpha14 &lt;- 0.6\n  if (!is.finite(alpha05)) alpha05 &lt;- 0.6\n  \n  # Fehlerbudgets berechnen (Base = runXX$res, Tuned = benchXX)\n  eb14_base  &lt;- simple_error_budget(run14$res, sigma_inst, alpha14) |&gt;\n    dplyr::mutate(Time = \"T14\", Mode = \"Base\")\n  eb05_base  &lt;- simple_error_budget(run05$res, sigma_inst, alpha05) |&gt;\n    dplyr::mutate(Time = \"T05\", Mode = \"Base\")\n  eb14_tuned &lt;- simple_error_budget(bench14,   sigma_inst, alpha14) |&gt;\n    dplyr::mutate(Time = \"T14\", Mode = \"Tuned\")\n  eb05_tuned &lt;- simple_error_budget(bench05,   sigma_inst, alpha05) |&gt;\n    dplyr::mutate(Time = \"T05\", Mode = \"Tuned\")\n  \n  eb_all &lt;- dplyr::bind_rows(eb14_base, eb05_base, eb14_tuned, eb05_tuned) |&gt;\n    dplyr::relocate(Time, Mode)\n  \n  print(eb_all)\n  \n  # einfache Stacked-Bar-Plot-Funktion\n  plot_error_budget &lt;- function(df) {\n    d &lt;- df |&gt;\n      dplyr::filter(Component %in% c(\"Instrument var\",\"Microscale var\",\"Mesoscale var\"))\n    ggplot2::ggplot(d,\n                    ggplot2::aes(x = interaction(Time, Mode, sep = \" \"), y = Value, fill = Component)\n    ) +\n      ggplot2::geom_col(position = \"stack\") +\n      ggplot2::theme_minimal() +\n      ggplot2::labs(x = NULL, y = \"Variance (°C²)\", title = \"Error budget by time & mode\")\n  }\n  p_eb &lt;- plot_error_budget(eb_all)\n  print(p_eb)\n  # =====================================================================\n  # 7) Optional: save everything at the end (plots + tables + rasters)\n  #    - Change `export &lt;- FALSE` at the top to only run/plot interactively\n  #    - We wrap saves in try() so a single failed save does not abort the run.\n  # =====================================================================\n  if (export) {\n    # ---------- Ausgabe-Verzeichnis: results_&lt;scen-name&gt; ----------\n    out_root &lt;- here::here(\"block4_5\")\n    out_dir  &lt;- file.path(out_root, sprintf(\"results_%s\", scen_name))\n    fig_dir  &lt;- file.path(out_dir, \"fig\")\n    tab_dir  &lt;- file.path(out_dir, \"tab\")\n    ras_dir  &lt;- file.path(out_dir, \"ras\")\n    # ohne Rückfrage, rekursiv, ohne Warnungen\n    dir.create(fig_dir, recursive = TRUE, showWarnings = FALSE)\n    dir.create(tab_dir, recursive = TRUE, showWarnings = FALSE)\n    dir.create(ras_dir, recursive = TRUE, showWarnings = FALSE)\n    \n  \n    \n    # ---------- Baseline: Previews & CV-Plots ----------------------\n    save_plot_min(plot_landcover_terrain(scen, stations = st14, layout = \"vertical\"),\n                  fn_fig(\"landcover_terrain\"))\n    save_plot_min(plot_block_overview_2x2_en(scen, pts_sf = st14), fn_fig(\"overview_2x2\"))\n    save_plot_min(run14$res$blocks_plot, fn_fig(\"T14_blocks\"))\n    save_plot_min(run14$res$diag_plot,   fn_fig(\"T14_diag\"))\n    save_plot_min(run05$res$blocks_plot, fn_fig(\"T05_blocks\"))\n    save_plot_min(run05$res$diag_plot,   fn_fig(\"T05_diag\"))\n    save_plot_min(run14$maps$p_truth,    fn_fig(\"T14_truth\"))\n    save_plot_min(run14$maps$p_pred,     fn_fig(\"T14_pred\"))\n    save_plot_min(run05$maps$p_truth,    fn_fig(\"T05_truth\"))\n    save_plot_min(run05$maps$p_pred,     fn_fig(\"T05_pred\"))\n    \n    # --- Tuned Panels ---\n    save_plot_min(panel_T14[[1]], fn_fig(\"T14_panel_tuned\"))\n    save_plot_min(panel_T05[[1]], fn_fig(\"T05_panel_tuned\"))\n    \n    # --- Raster ---\n    save_raster_min(scen$E,   fn_ras(\"E_dem\"))\n    save_raster_min(scen$R14, fn_ras(\"R14_truth\"))\n    save_raster_min(scen$R05, fn_ras(\"R05_truth\"))\n    if (\"lc\" %in% names(scen)) save_raster_min(scen$lc, fn_ras(\"landcover\"))\n    # ---------- Scale inference + tuned panels ----------------------\n    safe_save_plot(p_vg14, fn_fig(\"T14_variogram\"))\n    safe_save_plot(p_vg05, fn_fig(\"T05_variogram\"))\n    safe_save_plot(p_uc14, fn_fig(\"T14_ucurve\"))\n    safe_save_plot(p_uc05, fn_fig(\"T05_ucurve\"))\n    safe_save_plot(panel_T14[[1]], fn_fig(\"T14_panel_tuned\"))\n    safe_save_plot(panel_T05[[1]], fn_fig(\"T05_panel_tuned\"))\n    \n    save_table_readable(bench14$metrics, \"metrics_T14_tuned\", \"Tuned metrics — T14\")\n    save_table_readable(bench05$metrics, \"metrics_T05_tuned\", \"Tuned metrics — T05\")\n    save_table_readable(tune14$grid,     \"Ucurve_T14\",       \"U-curve grid — T14\")\n    save_table_readable(tune05$grid,     \"Ucurve_T05\",       \"U-curve grid — T05\")\n    save_table_readable(data.frame(L50 = Ls14$L50, L95 = Ls14$L95, R_star = tune14$R_star),\n                        \"scales_T14\", \"Scales — T14 (L50/L95/R*)\")\n    save_table_readable(data.frame(L50 = Ls05$L50, L95 = Ls05$L95, R_star = tune05$R_star),\n                        \"scales_T05\", \"Scales — T05 (L50/L95/R*)\")\n    save_table_readable(run14$res$metrics, file.path(tab_dir, sprintf(\"metrics_T14_%s\", scen_name)))\n    save_table_readable(run05$res$metrics, file.path(tab_dir, sprintf(\"metrics_T05_%s\", scen_name)))\n    save_table_readable(bench14$metrics,   file.path(tab_dir, sprintf(\"metrics_T14_tuned_%s\", scen_name)))\n    save_table_readable(bench05$metrics,   file.path(tab_dir, sprintf(\"metrics_T05_tuned_%s\", scen_name)))\n    save_table_readable(eb_all,            file.path(tab_dir, sprintf(\"error_budget_%s\", scen_name)))\n    #\n    # ---------- Raster mit Szenario-Präfix --------------------------\n    try(terra::writeRaster(scen$E,   fn_ras(\"E_dem\")),     silent = TRUE)\n    try(terra::writeRaster(scen$R14, fn_ras(\"R14_truth\")), silent = TRUE)\n    try(terra::writeRaster(scen$R05, fn_ras(\"R05_truth\")), silent = TRUE)\n    if (\"lc\" %in% names(scen))\n      try(terra::writeRaster(scen$lc, fn_ras(\"landcover\")), silent = TRUE)\n    \n    # ---------- Sessioninfo -----------------------------------------\n    try(saveRDS(sessionInfo(), file.path(out_dir, sprintf(\"%s_sessionInfo.rds\", scen_name))),\n        silent = TRUE)\n    \n    message(\"✔ Exports written to: \", normalizePath(out_dir, winslash = \"/\"))\n  }\n```\n\n\n\nHelpers / Core library:\n\npackages.R: centralized package loading and global run options (e.g., sf_use_s2(FALSE), seeds).\n\n\n\n\nCode\n```{r}\n#| eval: false\n# --- Paketliste an EINER Stelle pflegen ---------------------------------\n.req_pkgs &lt;- list(\n  core      = c(\"terra\",\"sf\",\"suncalc\",\"gstat\"),\n  modeling  = c(\"randomForest\",\"mgcv\"),\n  wrangling = c(\"dplyr\",\"tibble\",\"tidyr\"),\n  viz       = c(\"ggplot2\",\"scales\",\"patchwork\",\"RColorBrewer\"),\n  report    = c(\"knitr\",\"kableExtra\",\"here\",\"zoo\", \"gt\", \"openxlsx\", \"writexl\")\n\n)\n\nensure_packages &lt;- function(pkgs = unlist(.req_pkgs)) {\n  inst &lt;- rownames(installed.packages())\n  missing &lt;- setdiff(pkgs, inst)\n  if (length(missing)) install.packages(missing, dependencies = TRUE)\n  invisible(lapply(pkgs, require, character.only = TRUE))\n}\n\nafter_load &lt;- function() {\n  if (requireNamespace(\"sf\", quietly = TRUE)) sf::sf_use_s2(FALSE)  # wie bisher\n}\n\n# Aufruf:\nensure_packages()\nafter_load()\n\n# ---- Pfade ------------------------------------------------------------\nbase_dir &lt;- tryCatch(here::here(), error = function(e) getwd())\nsrc_dir  &lt;- file.path(base_dir, \"block4_5\", \"src\")\nout_dir  &lt;- file.path(base_dir, \"block4_5\", \"exports\")\nfig_dir  &lt;- file.path(out_dir, \"figs\")\ntab_dir  &lt;- file.path(out_dir, \"tables\")\nras_dir  &lt;- file.path(out_dir, \"rasters\")\ndat_dir  &lt;- file.path(out_dir, \"data\")\n\ndir.create(out_dir, showWarnings = FALSE, recursive = TRUE)\nfor (d in c(fig_dir, tab_dir, ras_dir, dat_dir)) dir.create(d, showWarnings = FALSE, recursive = TRUE)\n```\n\n\n-   `fun_pipemodel.R`: domain modeling utilities (plots, feature\n    derivation, variogram utilities, U-curve tuning, panels, saving\n    helpers).\n\n\nCode\n```{r}\n#| eval: false\n\n## ======================================================================\n## pipemodel_functions.R  —  nur Funktionen, keine Seiteneffekte\n## ======================================================================\n# ========================= I/O helpers (tables & plots) ======================\n\n\n# ---- Export-Helper (einfügen NACH out_dir/fig_dir/tab_dir/ras_dir) ----\nfn_fig &lt;- function(stem, ext = \"png\") file.path(fig_dir, sprintf(\"%s.%s\", stem, ext))\nfn_ras &lt;- function(stem, ext = \"tif\") file.path(ras_dir, sprintf(\"%s.%s\", stem, ext))\n\nsave_plot_min &lt;- function(p, file, width = 9, height = 6, dpi = 150, bg = \"white\") {\n  # Speichert ggplot ODER \"druckbare\" Plot-Objekte\n  dir.create(dirname(file), recursive = TRUE, showWarnings = FALSE)\n  if (inherits(p, \"ggplot\")) {\n    ggplot2::ggsave(filename = file, plot = p, width = width, height = height, dpi = dpi, bg = bg)\n  } else {\n    grDevices::png(filename = file, width = width, height = height, units = \"in\", res = dpi, bg = bg)\n    print(p)\n    grDevices::dev.off()\n  }\n  invisible(normalizePath(file, winslash = \"/\"))\n}\n\nsafe_save_plot &lt;- function(p, file, width = 9, height = 6, dpi = 150, bg = \"white\") {\n  try(save_plot_min(p, file, width, height, dpi, bg), silent = TRUE)\n}\n\nsave_raster_min &lt;- function(r, file, overwrite = TRUE) {\n  dir.create(dirname(file), recursive = TRUE, showWarnings = FALSE)\n  terra::writeRaster(r, file, overwrite = overwrite)\n  invisible(normalizePath(file, winslash = \"/\"))\n}\n\n\n# Save a table in CSV (+ optional HTML via gt, XLSX via openxlsx/writexl)\n# Robust to tibbles, list cols (ignored), and mistaken positional args.\n# Save a table as CSV (always), HTML (if gt is installed), and XLSX\n# file_stem: full path without extension, e.g. fn_tab(\"metrics_T14_base\")\nsave_table_readable &lt;- function(df, file_stem,\n                                title = NULL,\n                                digits = 3,\n                                make_dirs = TRUE,\n                                verbose = FALSE) {\n  if (!inherits(df, \"data.frame\")) df &lt;- as.data.frame(df)\n  \n  # Drop list-cols so write.csv/openxlsx don't choke\n  is_listcol &lt;- vapply(df, is.list, logical(1))\n  if (any(is_listcol)) df &lt;- df[ , !is_listcol, drop = FALSE]\n  \n  if (isTRUE(make_dirs)) dir.create(dirname(file_stem), showWarnings = FALSE, recursive = TRUE)\n  \n  # Round numeric columns safely\n  numcols &lt;- vapply(df, is.numeric, TRUE)\n  if (any(numcols)) {\n    for (nm in names(df)[numcols]) df[[nm]] &lt;- round(df[[nm]], digits)\n  }\n  \n  paths &lt;- list()\n  \n  ## CSV\n  csv_path &lt;- paste0(file_stem, \".csv\")\n  utils::write.csv(df, csv_path, row.names = FALSE)\n  paths$csv &lt;- csv_path\n  \n  ## HTML via gt (optional)\n  if (requireNamespace(\"gt\", quietly = TRUE)) {\n    gt_tbl &lt;- gt::gt(df)\n    if (!is.null(title)) gt_tbl &lt;- gt::tab_header(gt_tbl, title = title)\n    gt::gtsave(gt_tbl, paste0(file_stem, \".html\"))\n    paths$html &lt;- paste0(file_stem, \".html\")\n  } else if (verbose) {\n    message(\"[save_table_readable] Package 'gt' not installed → skipping HTML.\")\n  }\n  \n  ## XLSX via openxlsx (preferred) or writexl (fallback)\n  xlsx_path &lt;- paste0(file_stem, \".xlsx\")\n  if (requireNamespace(\"openxlsx\", quietly = TRUE)) {\n    wb &lt;- openxlsx::createWorkbook()\n    openxlsx::addWorksheet(wb, \"table\")\n    \n    # Optional title in A1, style it a bit\n    start_row &lt;- 1L\n    if (!is.null(title)) {\n      openxlsx::writeData(wb, \"table\", title, startRow = start_row, startCol = 1)\n      # bold, bigger font for title\n      st &lt;- openxlsx::createStyle(textDecoration = \"bold\", fontSize = 14)\n      openxlsx::addStyle(wb, \"table\", st, rows = start_row, cols = 1, gridExpand = TRUE, stack = TRUE)\n      start_row &lt;- start_row + 2L  # blank row after title\n    }\n    \n    openxlsx::writeData(wb, \"table\", df, startRow = start_row, startCol = 1)\n    openxlsx::saveWorkbook(wb, xlsx_path, overwrite = TRUE)\n    paths$xlsx &lt;- xlsx_path\n  } else if (requireNamespace(\"writexl\", quietly = TRUE)) {\n    writexl::write_xlsx(df, xlsx_path)\n    paths$xlsx &lt;- xlsx_path\n  } else if (verbose) {\n    message(\"[save_table_readable] Neither 'openxlsx' nor 'writexl' installed → skipping XLSX.\")\n  }\n  \n  invisible(paths)\n}\n\n\n\n#' Save a ggplot/patchwork safely (no-op if not a plot)\n#'\n#' @param p A ggplot/patchwork object.\n#' @param file Output path (with extension, e.g. \\code{.png}).\n#' @param width,height Figure size in inches.\n#' @param dpi Resolution in dots per inch.\n#' @param bg Background color (default \\code{\"white\"}).\n#' @keywords io export plot\nsave_plot_safe &lt;- function(p, file, width = 9, height = 6, dpi = 300, bg = \"white\") {\n  if (inherits(p, c(\"gg\", \"ggplot\", \"patchwork\"))) {\n    dir.create(dirname(file), showWarnings = FALSE, recursive = TRUE)\n    try(ggplot2::ggsave(file, p, width = width, height = height, dpi = dpi, bg = bg),\n        silent = TRUE)\n  }\n}\n# =============================================================================\n\norder_models_by_median_rmse &lt;- function(cv_tbl) {\n  bm &lt;- block_metrics_long(cv_tbl)\n  bm |&gt;\n    dplyr::filter(Metric == \"RMSE\") |&gt;\n    dplyr::group_by(model) |&gt;\n    dplyr::summarise(med = stats::median(Value, na.rm = TRUE), .groups = \"drop\") |&gt;\n    dplyr::arrange(med) |&gt;\n    dplyr::pull(model)\n}\n\n# Block-wise metrics (RMSE, MAE)\nblock_metrics_long &lt;- function(cv_tbl) {\n  stopifnot(all(c(\"model\",\"block_id\",\"obs\",\"pred\") %in% names(cv_tbl)))\n  cv_tbl |&gt;\n    dplyr::group_by(model, block_id) |&gt;\n    dplyr::summarise(\n      RMSE = sqrt(mean((obs - pred)^2, na.rm = TRUE)),\n      MAE  = mean(abs(obs - pred), na.rm = TRUE),\n      .groups = \"drop\"\n    ) |&gt;\n    tidyr::pivot_longer(c(RMSE, MAE), names_to = \"Metric\", values_to = \"Value\")\n}\nmake_block_metric_box &lt;- function(cv_tbl, which_time = \"T14\", tail_cap = 0.995) {\n  bm &lt;- block_metrics_long(cv_tbl) |&gt;\n    dplyr::filter(is.finite(Value))\n  if (!is.null(tail_cap)) {\n    ymax &lt;- stats::quantile(bm$Value, tail_cap, na.rm = TRUE)\n  }\n  lev &lt;- order_models_by_median_rmse(cv_tbl)\n  bm$model &lt;- factor(bm$model, levels = lev)\n  \n  ggplot2::ggplot(bm, ggplot2::aes(model, Value)) +\n    ggplot2::geom_boxplot(outlier.alpha = 0.35, width = 0.7) +\n    ggplot2::stat_summary(fun = mean, geom = \"point\", shape = 23, size = 3,\n                          fill = \"white\", colour = \"black\", stroke = 0.5) +\n    ggplot2::coord_cartesian(ylim = c(0, ymax)) +\n    ggplot2::theme_minimal() +\n    ggplot2::labs(title = sprintf(\"%s — Block-wise errors (LBO-CV)\", which_time),\n                  subtitle = \"Box = IQR · line = median · ◆ = mean\",\n                  x = \"Model\", y = \"Error\") +\n    ggplot2::facet_wrap(~ Metric, scales = \"free_y\")\n}\n\nmake_abs_error_box &lt;- function(cv_tbl, which_time = \"T14\", tail_cap = 0.995) {\n  df &lt;- cv_tbl |&gt;\n    dplyr::mutate(abs_err = abs(pred - obs)) |&gt;\n    dplyr::filter(is.finite(abs_err))\n  ymax &lt;- if (!is.null(tail_cap)) stats::quantile(df$abs_err, tail_cap, na.rm = TRUE) else max(df$abs_err, na.rm = TRUE)\n  lev &lt;- df |&gt;\n    dplyr::group_by(model) |&gt;\n    dplyr::summarise(med = stats::median(abs_err, na.rm = TRUE), .groups = \"drop\") |&gt;\n    dplyr::arrange(med) |&gt;\n    dplyr::pull(model)\n  df$model &lt;- factor(df$model, levels = lev)\n  \n  ggplot2::ggplot(df, ggplot2::aes(model, abs_err)) +\n    ggplot2::geom_boxplot(outlier.alpha = 0.3, width = 0.7) +\n    ggplot2::stat_summary(fun = mean, geom = \"point\", shape = 23, size = 3,\n                          fill = \"white\", colour = \"black\", stroke = 0.5) +\n    ggplot2::coord_cartesian(ylim = c(0, ymax)) +\n    ggplot2::theme_minimal() +\n    ggplot2::labs(title = sprintf(\"%s — Absolute errors per station (LBO-CV)\", which_time),\n                  subtitle = \"Box = IQR · line = median · ◆ = mean\",\n                  x = \"Model\", y = \"|pred − obs|\")\n}\n\n\nmake_obs_pred_scatter &lt;- function(cv_tbl, which_time = \"T14\") {\n  lab &lt;- .make_labeller(cv_tbl)\n  ggplot(cv_tbl, aes(obs, pred)) +\n    geom_abline(slope = 1, intercept = 0, linetype = \"dashed\") +\n    geom_point(alpha = 0.7, shape = 16) +\n    coord_equal() + theme_minimal() +\n    labs(title = sprintf(\"%s — Observed vs Predicted (LBO-CV)\", which_time), x = \"Observed\", y = \"Predicted\") +\n    facet_wrap(~ model, ncol = 3, labeller = ggplot2::as_labeller(lab))\n}\n\nmake_residual_density &lt;- function(cv_tbl, which_time = \"T14\") {\n  cv_tbl |&gt; dplyr::mutate(resid = pred - obs) |&gt; ggplot2::ggplot(ggplot2::aes(resid, fill = model)) +\n    ggplot2::geom_density(alpha = 0.4) + ggplot2::theme_minimal() +\n    ggplot2::labs(title = sprintf(\"%s — Residual density\", which_time), x = \"Residual (°C)\", y = \"Density\")\n}\n\n# Prediction maps & error panels ---------------------------------------\n.make_labeller &lt;- function(cv_tbl) {\n  m &lt;- cv_tbl |&gt;\n    dplyr::group_by(model) |&gt;\n    dplyr::summarise(RMSE = sqrt(mean((obs - pred)^2, na.rm = TRUE)), MAE  = mean(abs(obs - pred), na.rm = TRUE), .groups = \"drop\")\n  setNames(sprintf(\"%s  (RMSE=%.2f · MAE=%.2f)\", m$model, m$RMSE, m$MAE), m$model)\n}\n.plot_raster_gg &lt;- function(r, title = \"\", palette = temp_palette, q = c(0.02,0.98), lims = NULL) {\n  stopifnot(terra::nlyr(r) == 1)\n  df &lt;- as.data.frame(r, xy = TRUE, na.rm = FALSE)\n  nm &lt;- names(df)[3]\n  if (is.null(lims)) {\n    vv &lt;- terra::values(r, na.rm = TRUE)\n    lims &lt;- stats::quantile(vv, probs = q, na.rm = TRUE, names = FALSE)\n  }\n  ggplot2::ggplot(df, ggplot2::aes(.data$x, .data$y, fill = .data[[nm]])) +\n    ggplot2::geom_raster() +\n    ggplot2::coord_equal() +\n    ggplot2::scale_fill_gradientn(colours = palette, limits = lims, oob = scales::squish) +\n    ggplot2::labs(title = title, x = NULL, y = NULL, fill = \"°C\") +\n    ggplot2::theme_minimal(base_size = 11) +\n    ggplot2::theme(legend.position = \"right\",\n                   plot.title = ggplot2::element_text(face = \"bold\"))\n}\n\n.get_preds_from_maps &lt;- function(maps) {\n  # 1) SpatRaster direkt\n  if (inherits(maps, \"SpatRaster\")) {\n    ul &lt;- terra::unstack(maps)\n    names(ul) &lt;- names(maps)\n    return(ul)\n  }\n  # 2) Liste mit typischen Feldern\n  if (is.list(maps)) {\n    if (!is.null(maps$preds))          return(maps$preds)\n    if (!is.null(maps$pred_rasters))   return(maps$pred_rasters)\n    if (!is.null(maps$pred_stack) && inherits(maps$pred_stack, \"SpatRaster\")) {\n      ul &lt;- terra::unstack(maps$pred_stack); names(ul) &lt;- names(maps$pred_stack); return(ul)\n    }\n    if (!is.null(maps$stack) && inherits(maps$stack, \"SpatRaster\")) {\n      ul &lt;- terra::unstack(maps$stack); names(ul) &lt;- names(maps$stack); return(ul)\n    }\n    if (!is.null(maps$maps) && inherits(maps$maps, \"SpatRaster\")) {\n      ul &lt;- terra::unstack(maps$maps); names(ul) &lt;- names(maps$maps); return(ul)\n    }\n    # 3) Liste, die bereits einzelne SpatRaster oder ggplots enthält\n    cand &lt;- maps[ vapply(maps, function(x) inherits(x, \"SpatRaster\") || inherits(x, \"ggplot\"), logical(1)) ]\n    if (length(cand) &gt; 0) return(cand)\n  }\n  stop(\"build_panels_with_errors(): In 'maps' keine Vorhersagen gefunden.\")\n}\n# --- Kartenplot mit optionalen Achsenticks/labels ----------------------\n.plot_map_axes &lt;- function(r, title, cols, lims, q = c(0.02,0.98),\n                           base_size = 14, tick_n = 5,\n                           show_axis_labels = FALSE, show_axis_ticks = TRUE) {\n  stopifnot(terra::nlyr(r) == 1)\n  df &lt;- as.data.frame(r, xy = TRUE, na.rm = FALSE)\n  nm &lt;- names(df)[3]\n  \n  if (is.null(lims) || !all(is.finite(lims)) || lims[1] &gt;= lims[2]) {\n    vv &lt;- terra::values(r, na.rm = TRUE)\n    lims &lt;- stats::quantile(vv, probs = q, na.rm = TRUE, names = FALSE)\n    if (!all(is.finite(lims)) || lims[1] == lims[2]) lims &lt;- range(vv, na.rm = TRUE) + c(-1e-6, 1e-6)\n  }\n  if (is.function(cols)) cols &lt;- cols(256)\n  if (!is.atomic(cols) || length(cols) &lt; 2) cols &lt;- grDevices::hcl.colors(256, \"YlOrRd\")\n  \n  ggplot2::ggplot(df, ggplot2::aes(x, y, fill = .data[[nm]])) +\n    ggplot2::geom_raster() +\n    ggplot2::coord_equal(expand = FALSE) +\n    ggplot2::scale_x_continuous(expand = c(0,0), breaks = scales::breaks_pretty(n = tick_n)) +\n    ggplot2::scale_y_continuous(expand = c(0,0), breaks = scales::breaks_pretty(n = tick_n)) +\n    ggplot2::scale_fill_gradientn(colours = cols, limits = lims, oob = scales::squish) +\n    ggplot2::labs(title = title, x = NULL, y = NULL, fill = \"°C\") +\n    ggplot2::theme_minimal(base_size = base_size) +\n    ggplot2::theme(\n      legend.position = \"right\",\n      plot.title = ggplot2::element_text(face = \"bold\"),\n      axis.title   = ggplot2::element_blank(),\n      axis.text    = if (show_axis_labels) ggplot2::element_text(size = base_size - 3) else ggplot2::element_blank(),\n      axis.ticks   = if (show_axis_ticks)  ggplot2::element_line(linewidth = 0.25) else ggplot2::element_blank(),\n      panel.border = ggplot2::element_rect(fill = NA, colour = \"grey40\", linewidth = .4)\n    )\n}\nbuild_panels_truth_preds_errors_paged &lt;- function(\n    maps, truth_raster, cv_tbl, which_time,\n    models_per_page = 4,\n    model_order      = NULL,\n    temp_pal         = temp_palette,     # Vektor ODER Funktion -&gt; wird zu Vektor\n    stretch_q        = c(0.02, 0.98),\n    errors_height    = 1.2,\n    scatter_next_to_truth = TRUE,        # Scatter rechts von Truth?\n    top_widths       = c(1.1, 0.9),      # Breitenverhältnis Truth | Scatter\n    show_second_legend = FALSE           # zweite °C-Legende bei den Preds unterdrücken\n) {\n  stopifnot(length(stretch_q) == 2)\n  if (is.function(temp_pal)) temp_pal &lt;- temp_pal(256)\n  stopifnot(is.atomic(temp_pal), length(temp_pal) &gt;= 2)\n  \n  # Vorhersagen einsammeln / Reihenfolge\n  preds_raw  &lt;- .get_preds_from_maps(maps)\n  pred_names &lt;- names(preds_raw) %||% paste0(\"model_\", seq_along(preds_raw))\n  if (!is.null(model_order)) {\n    keep &lt;- intersect(model_order, pred_names)\n    if (!length(keep)) stop(\"model_order enthält keine gültigen Modellnamen.\")\n    preds_raw  &lt;- preds_raw[keep]\n    pred_names &lt;- keep\n  }\n  \n  # Gemeinsame Farbskala\n  all_vals &lt;- c(terra::values(truth_raster, na.rm = TRUE))\n  for (p in preds_raw) if (inherits(p, \"SpatRaster\")) all_vals &lt;- c(all_vals, terra::values(p, na.rm = TRUE))\n  lims &lt;- stats::quantile(all_vals, probs = stretch_q, na.rm = TRUE, names = FALSE)\n  if (all(is.finite(lims)) && lims[1] == lims[2]) {\n    eps &lt;- .Machine$double.eps * max(1, abs(lims[1])); lims &lt;- lims + c(-eps, eps)\n  }\n  \n  # Helfer: Raster -&gt; ggplot\n  make_tile &lt;- function(obj, title_txt, show_legend = TRUE) {\n    if (inherits(obj, \"SpatRaster\")) {\n      g &lt;- .plot_raster_gg(obj, title = title_txt, palette = temp_pal, q = stretch_q, lims = lims)\n      if (!show_legend) g &lt;- g + ggplot2::theme(legend.position = \"none\")\n      g\n    } else if (inherits(obj, \"ggplot\")) {\n      obj + ggplot2::labs(title = title_txt)\n    } else stop(\"Nicht unterstützter Prediction-Typ: \", class(obj)[1])\n  }\n  \n  # Truth (+ optional Scatter daneben)\n  p_truth   &lt;- .plot_raster_gg(truth_raster, title = paste0(which_time, \" — truth\"),\n                               palette = temp_pal, q = stretch_q, lims = lims)\n  p_scatter &lt;- make_obs_pred_scatter(cv_tbl, which_time = which_time)\n  \n  # Prediction-Kacheln bauen (nur erste mit °C-Legende falls gewünscht)\n  pred_tiles &lt;- lapply(seq_along(preds_raw), function(i) {\n    show_leg &lt;- if (isTRUE(show_second_legend)) TRUE else (i == 1L)\n    make_tile(preds_raw[[i]], pred_names[i], show_legend = show_leg)\n  })\n  \n  # Paginierung\n  n &lt;- length(pred_tiles)\n  idx_split &lt;- split(seq_len(n), ceiling(seq_len(n) / models_per_page))\n  \n  pages &lt;- lapply(idx_split, function(idx) {\n    preds_row &lt;- patchwork::wrap_plots(pred_tiles[idx], nrow = 1, ncol = length(idx))\n    \n    top_row &lt;- if (isTRUE(scatter_next_to_truth)) {\n      (p_truth | (p_scatter + ggplot2::theme(legend.position = \"none\"))) +\n        patchwork::plot_layout(widths = top_widths)\n    } else {\n      p_truth\n    }\n    \n    # Errors unten: wenn Scatter schon oben, unten nur Dichte\n    p_box_rmse &lt;- make_block_metric_box(cv_tbl, which_time = which_time, tail_cap = 0.995)\n    p_box_ae   &lt;- make_abs_error_box  (cv_tbl, which_time = which_time, tail_cap = 0.995)\n    p_dens     &lt;- make_residual_density(cv_tbl, which_time = which_time)\n    p_errors   &lt;- (p_box_rmse | p_box_ae) / p_dens\n    \n    (top_row / preds_row / p_errors) +\n      patchwork::plot_layout(heights = c(1, 1, errors_height), guides = \"collect\") &\n      ggplot2::theme(legend.position = \"right\")\n  })\n  \n  pages\n}\n.pm_verbose &lt;- function(v = NULL) {\n  if (!is.null(v)) return(isTRUE(v))\n  isTRUE(getOption(\"pipemodel.verbose\", FALSE))\n}\npm_say &lt;- function(fmt, ..., v = NULL) {\n  if (.pm_verbose(v)) message(sprintf(fmt, ...))\n}\n.k_for_xy &lt;- function(n, n_xy) max(3, min(60, n_xy - 1L, floor(n * 0.8)))\n.kcap_unique &lt;- function(x, kmax) {\n  ux &lt;- unique(x[is.finite(x)])\n  nu &lt;- length(ux)\n  if (nu &lt;= 3) return(0L)                # treat as constant/near-constant\n  max(4L, min(kmax, nu - 1L))\n}\n\n`%||%` &lt;- function(a, b) if (!is.null(a)) a else b\n# --- Sun helper (self-contained in the lib) --------------------------\nsun_pos_utc &lt;- function(y, m, d, h, lat, lon) {\n  t  &lt;- as.POSIXct(sprintf(\"%04d-%02d-%02d %02d:00:00\", y, m, d, h), tz = \"UTC\")\n  sp &lt;- suncalc::getSunlightPosition(date = t, lat = lat, lon = lon)\n  list(\n    alt = sp$altitude,\n    az  = (sp$azimuth + pi) %% (2*pi)  # convert to [0, 2π) from north\n  )\n}\n\n# Sun helper: pull sun14/sun05 from scen; else compute; else fallback\n.get_sun &lt;- function(scen, which = c(\"T14\",\"T05\")) {\n  which &lt;- match.arg(which)\n  key   &lt;- if (which == \"T14\") \"sun14\" else \"sun05\"\n  \n  # 1) stored in scen?\n  s &lt;- scen[[key]]\n  if (is.list(s) && is.finite(s$alt) && is.finite(s$az)) return(s)\n  \n  # 2) compute from lat/lon/sun_date if available\n  if (all(c(\"lat\",\"lon\",\"sun_date\") %in% names(scen))) {\n    hour &lt;- if (which == \"T14\") 14L else 5L\n    return(sun_pos_utc(scen$sun_date, hour, scen$lat, scen$lon))\n  }\n  \n  # 3) hard fallback\n  list(alt = if (which == \"T14\") 0.75 else 0.10, az = 0.0)\n}\n\n# -------------------------- Defaults -----------------------------------\nlc_levels_default &lt;- c(\"forest\",\"water\",\"bare soil\",\"meadows\")\nlc_levels &lt;- getOption(\"pipemodel.lc_levels\", lc_levels_default)\n\nlc_colors_default &lt;- c(\n  \"forest\"   = \"#2E8B57\",\n  \"water\"    = \"#5DADE2\",\n  \"bare soil\"= \"#C49A6C\",\n  \"meadows\"  = \"#7FBF7B\"\n)\ntemp_palette &lt;- grDevices::colorRampPalette(c(\"#0000FF\",\"#FF0000\"))\nstretch_q    &lt;- c(0.02, 0.98)\n# Normalize any CRS input to a non-empty character string\nnorm_crs &lt;- function(crs, fallback = \"EPSG:32632\") {\n  # allow sf::crs, numeric EPSG, character EPSG/WKT\n  if (inherits(crs, \"crs\")) {\n    out &lt;- sf::st_crs(crs)$wkt\n  } else if (is.numeric(crs) && length(crs) == 1 && is.finite(crs)) {\n    out &lt;- sprintf(\"EPSG:%d\", as.integer(crs))\n  } else if (is.character(crs) && length(crs) &gt;= 1) {\n    out &lt;- crs[1]\n  } else {\n    out &lt;- NA_character_\n  }\n  if (!length(out) || is.na(out) || identical(out, \"\")) out &lt;- fallback\n  out\n}\n\n# -------------------------- Domain/Template -----------------------------\nmake_domain &lt;- function(center_E, center_N, len_x, len_y, res, crs = \"EPSG:32632\") {\n  crs &lt;- norm_crs(crs)\n  ext &lt;- terra::ext(center_E - len_x/2, center_E + len_x/2,\n                    center_N - len_y/2, center_N + len_y/2)\n  Rtemplate &lt;- terra::rast(ext, resolution = res, crs = crs)\n  list(\n    xmin = terra::xmin(ext), xmax = terra::xmax(ext),\n    ymin = terra::ymin(ext), ymax = terra::ymax(ext),\n    x0 = center_E, y0 = center_N,\n    res = as.numeric(res), crs = crs,\n    Rtemplate = Rtemplate\n  )\n}\n\n\ncompute_block_size &lt;- function(len_x, len_y, n_st,\n                               target_st_per_block = 3,\n                               min_blocks_axis = 3,\n                               round_to = 50) {\n  area &lt;- len_x * len_y\n  B_target &lt;- max(min_blocks_axis^2, round(n_st / target_st_per_block))\n  bs &lt;- sqrt(area / B_target)\n  bs &lt;- min(bs, len_x / min_blocks_axis, len_y / min_blocks_axis)\n  bs &lt;- max(round_to, round(bs / round_to) * round_to)\n  as.integer(bs)\n}\n\n# -------------------------- Sonne/Geometrie -----------------------------\n\n# Cosine of incidence on a slope/aspect for a given sun (radians)\ncosi_fun &lt;- function(alt, az, slp_r, asp_r) {\n  zen &lt;- (pi/2 - alt)\n  ci  &lt;- cos(slp_r)*cos(zen) + sin(slp_r)*sin(zen)*cos(az - asp_r)\n  terra::ifel(ci &lt; 0, 0, ci)\n}\n\n# Sun position at a given UTC date + hour (numeric hour), return radians\nsun_pos_utc &lt;- function(sun_date, hour_utc, lat, lon) {\n  stopifnot(inherits(sun_date, \"Date\"), length(hour_utc) == 1)\n  t  &lt;- as.POSIXct(sprintf(\"%s %02d:00:00\", format(sun_date, \"%Y-%m-%d\"), as.integer(hour_utc)), tz = \"UTC\")\n  sp &lt;- suncalc::getSunlightPosition(date = t, lat = lat, lon = lon)\n  list(\n    alt = as.numeric(sp$altitude),                   # radians\n    az  = as.numeric((sp$azimuth + pi) %% (2*pi))    # convert to [0..2π) from north\n  )\n}\n\n\n# -------------------------- Rauschen ------------------------------------\nmake_noise_pair &lt;- function(template, sd14 = 0.3, sd05 = 0.3,\n                            seed14 = 1001, seed05 = 1002) {\n  set.seed(seed14)\n  n14 &lt;- terra::setValues(terra::rast(template), rnorm(terra::ncell(template), 0, sd14))\n  set.seed(seed05)\n  n05 &lt;- terra::setValues(terra::rast(template), rnorm(terra::ncell(template), 0, sd05))\n  list(noise14 = n14, noise05 = n05)\n}\n\n# -------------------------- Topographie ---------------------------------\nbuild_topography &lt;- function(domain,\n                             lake_mode = c(\"none\",\"water\",\"hollow\"),\n                             hill_mode = c(\"none\",\"bump\"),\n                             lake_diam_m  = 50,  lake_depth_m = 10, smooth_edges = FALSE,\n                             hill_diam_m  = 80,  hill_height_m = 50, hill_smooth  = FALSE) {\n  lake_mode &lt;- match.arg(lake_mode); hill_mode &lt;- match.arg(hill_mode)\n  Rtemplate &lt;- domain$Rtemplate\n  x0 &lt;- domain$x0; y0 &lt;- domain$y0\n  xmin &lt;- domain$xmin; xmax &lt;- domain$xmax\n  len_x &lt;- xmax - xmin; y_hc &lt;- y0\n  x_hc &lt;- xmin + len_x/3; x_lc &lt;- xmin + 2*len_x/3; y_lc &lt;- y0\n  \n  XY &lt;- as.data.frame(terra::xyFromCell(Rtemplate, 1:terra::ncell(Rtemplate)))\n  names(XY) &lt;- c(\"x\",\"y\")\n  dy &lt;- XY$y - y0\n  a  &lt;- 100 / (( (domain$ymax - domain$ymin)/2 )^2)\n  elev &lt;- 500 + a * dy^2\n  \n  # See/Grube\n  rl &lt;- sqrt((XY$x - x_lc)^2 + (XY$y - y_lc)^2); lr &lt;- lake_diam_m/2\n  if (lake_mode %in% c(\"water\",\"hollow\")) {\n    w_l &lt;- if (smooth_edges) pmax(0, 1 - (rl/lr)^2) else as.numeric(rl &lt;= lr)\n    elev &lt;- elev - lake_depth_m * w_l\n  } else w_l &lt;- 0\n  \n  # Haupt-Hügel\n  if (hill_mode == \"bump\") {\n    rh &lt;- sqrt((XY$x - x_hc)^2 + (XY$y - y_hc)^2); hr &lt;- max(1e-6, hill_diam_m/2)\n    w_h &lt;- if (hill_smooth) exp(-(rh/hr)^2) else as.numeric(rh &lt;= hr)\n    elev &lt;- elev + hill_height_m * w_h\n  } else w_h &lt;- 0\n  \n  E &lt;- Rtemplate; terra::values(E) &lt;- elev; names(E) &lt;- \"elev\"\n  lakeR &lt;- Rtemplate; terra::values(lakeR) &lt;- if (lake_mode==\"water\") as.numeric(w_l&gt;0) else 0; names(lakeR) &lt;- \"lake\"\n  hillW &lt;- Rtemplate; terra::values(hillW) &lt;- w_h; names(hillW) &lt;- \"hillW\"\n  \n  slp  &lt;- terra::terrain(E, v=\"slope\",  unit=\"radians\")\n  asp  &lt;- terra::terrain(E, v=\"aspect\", unit=\"radians\")\n  \n  list(E = E, lake = lakeR, hillW = hillW,\n       slp = terra::ifel(is.na(slp), 0, slp),\n       asp = terra::ifel(is.na(asp), 0, asp))\n}\n# --- Sun helpers (UTC) -------------------------------------------------\nsun_pos_utc &lt;- function(date, hour, lat, lon) {\n  t  &lt;- as.POSIXct(sprintf(\"%s %02d:00:00\", as.Date(date), hour), tz = \"UTC\")\n  sp &lt;- suncalc::getSunlightPosition(date = t, lat = lat, lon = lon)\n  # Azimut: 0 = Nord, positiv im Uhrzeigersinn\n  list(alt = sp$altitude, az = (sp$azimuth + pi) %% (2*pi))\n}\n\n# -------------------------- Physikfelder --------------------------------\nbuild_physics_fields &lt;- function(topography, landcover,\n                                 noise14, noise05,\n                                 alpha_I_by_lc = c(\"forest\" = 3.5, \"water\" = 1.5, \"bare soil\" = 6.0, \"meadows\" = 4.0),\n                                 shade_fac_by_lc = c(\"forest\" = 0.60, \"water\" = 1.00, \"bare soil\" = 1.00, \"meadows\" = 0.95),\n                                 dawn_bias_by_lc = c(\"forest\" = 0.30, \"water\" = 1.20, \"bare soil\" = -0.50, \"meadows\" = 0.05),\n                                 pool_fac_by_lc  = c(\"forest\" = 0.70, \"water\" = 0.80, \"bare soil\" = 1.10, \"meadows\" = 1.05),\n                                 pool_block_gain = 0.4,\n                                 sun14 = list(alt = 0.75, az = 0.0),\n                                 sun05 = list(alt = 0.10, az = 0.0))\n                                 {\n  E    &lt;- topography$E\n  slp0 &lt;- topography$slp\n  asp0 &lt;- topography$asp\n  hillW&lt;- topography$hillW\n  \n  # Sonnen-Inzidenz\n  I14 &lt;- cosi_fun(sun14$alt, sun14$az, slp0, asp0)\n  I05 &lt;- cosi_fun(sun05$alt, sun05$az, slp0, asp0)\n  \n  lc &lt;- if (inherits(landcover, \"SpatRaster\")) landcover else landcover$lc\n  stopifnot(inherits(lc, \"SpatRaster\"))\n  \n  v &lt;- as.integer(terra::values(lc))\n  v[!is.finite(v)] &lt;- 1L\n  v &lt;- pmax(1L, pmin(v, length(lc_levels_default)))\n  lc_char &lt;- factor(lc_levels_default[v], levels = lc_levels_default)\n  \n  to_r &lt;- function(x) terra::setValues(terra::rast(E), x)\n  alpha_I &lt;- to_r(as.numeric(alpha_I_by_lc[lc_char]))\n  shade_f &lt;- to_r(as.numeric(shade_fac_by_lc[lc_char]))\n  dawn_b  &lt;- to_r(as.numeric(dawn_bias_by_lc[lc_char]))\n  pool_f  &lt;- to_r(as.numeric(pool_fac_by_lc[lc_char]))\n  \n  I14_eff &lt;- I14 * shade_f\n  \n  E_mean &lt;- terra::global(E, \"mean\", na.rm = TRUE)[1,1]\n  Y &lt;- terra::init(E, \"y\")\n  dist2ax &lt;- abs(Y - (terra::ymax(E)+terra::ymin(E))/2)\n  w_pool &lt;- 70\n  pool_base &lt;- 4.0 * exp(- (dist2ax / w_pool)^2)\n  pool_mod  &lt;- pool_base * (1 - pool_block_gain * hillW) * pool_f\n  \n  T0_14 &lt;- 26.0; lapse_14 &lt;- -0.0065\n  R14 &lt;- T0_14 + lapse_14 * (E - E_mean) + alpha_I * I14_eff + noise14; names(R14) &lt;- \"T14\"\n  \n  T0_05 &lt;- 8.5; inv_05 &lt;- 0.003; eta_slope &lt;- 0.6\n  R05 &lt;- T0_05 + inv_05 * (E - E_mean) + eta_slope * slp0 - pool_mod + dawn_b + noise05; names(R05) &lt;- \"T05\"\n  \n  list(R14 = R14, R05 = R05, I14 = I14, I05 = I05)\n}\n\n# --- Sun + cos(i) helpers (safe to keep once in your lib) --------------------\ncosi_fun &lt;- function(alt, az, slp_r, asp_r) {\n  zen &lt;- (pi/2 - alt)\n  ci  &lt;- cos(slp_r)*cos(zen) + sin(slp_r)*sin(zen)*cos(az - asp_r)\n  terra::ifel(ci &lt; 0, 0, ci)\n}\n\nsun_pos_utc &lt;- function(sun_date, hour_utc, lat, lon) {\n  stopifnot(inherits(sun_date, \"Date\"))\n  t  &lt;- as.POSIXct(sprintf(\"%s %02d:00:00\",\n                           format(sun_date, \"%Y-%m-%d\"),\n                           as.integer(hour_utc)), tz = \"UTC\")\n  sp &lt;- suncalc::getSunlightPosition(date = t, lat = lat, lon = lon)\n  list(\n    alt = as.numeric(sp$altitude),                  # radians\n    az  = as.numeric((sp$azimuth + pi) %% (2*pi))   # [0..2π) from north\n  )\n}\n\nbuild_scenario &lt;- function(\n    domain,\n    lake_mode = c(\"none\",\"water\",\"hollow\"),\n    hill_mode = c(\"none\",\"bump\"),\n    # main hill / lake geometry (meters)\n    lake_diam_m  = 50,  lake_depth_m = 10, smooth_edges = FALSE,\n    hill_diam_m  = 80,  hill_height_m = 50, hill_smooth  = FALSE,\n    # micro-relief (meters)\n    random_hills        = 0,\n    micro_hill_diam_m   = 30,\n    micro_hill_height_m = 50,\n    micro_hill_smooth   = TRUE,\n    micro_seed          = NULL,\n    # sun / geo\n    lat = 51.8, lon = 10.6, sun_date = as.Date(\"2024-06-21\"),\n    # optional noise\n    noise14 = NULL, noise05 = NULL\n) {\n  lake_mode &lt;- match.arg(lake_mode)\n  hill_mode &lt;- match.arg(hill_mode)\n  \n  # --- 0) Template & CRS guard (must be meters) -----------------------\n  ext &lt;- terra::ext(domain$xmin, domain$xmax, domain$ymin, domain$ymax)\n  Rtemplate &lt;- terra::rast(ext, resolution = domain$res, crs = domain$crs)\n  \n  crs_sf &lt;- sf::st_crs(terra::crs(Rtemplate, proj=TRUE))\n  if (isTRUE(sf::st_is_longlat(crs_sf))) {\n    stop(\n      \"build_scenario(): Domain CRS is geographic (degrees). \",\n      \"All geometry is in meters. Use a projected CRS (e.g. UTM / EPSG:32632).\"\n    )\n  }\n  \n  xmin &lt;- terra::xmin(ext); xmax &lt;- terra::xmax(ext)\n  ymin &lt;- terra::ymin(ext); ymax &lt;- terra::ymax(ext)\n  len_x &lt;- xmax - xmin;     len_y &lt;- ymax - ymin\n  x0 &lt;- (xmin + xmax)/2;    y0 &lt;- (ymin + ymax)/2\n  \n  # coordinate rasters\n  X &lt;- terra::init(Rtemplate, \"x\")\n  Y &lt;- terra::init(Rtemplate, \"y\")\n  \n  # quick sanity for scale\n  px &lt;- mean(terra::res(Rtemplate))\n  lr_px &lt;- (lake_diam_m/2) / px\n  hr_px &lt;- (hill_diam_m/2) / px\n  message(sprintf(\"[build_scenario] pixel=%.2f m; lake r=%.1f px; hill r=%.1f px\", px, lr_px, hr_px))\n  \n  # --- 1) Base valley --------------------------------------------------\n  a  &lt;- 100 / ((len_y/2)^2)\n  E  &lt;- 500 + a * (Y - y0)^2\n  names(E) &lt;- \"elev\"\n  \n  # --- 2) Lake (mirror of hill, negative) ------------------------------\n  x_lc &lt;- xmin + 2*len_x/3;  y_lc &lt;- y0\n  lr   &lt;- max(1e-6, lake_diam_m/2)\n  rl   &lt;- sqrt((X - x_lc)^2 + (Y - y_lc)^2)\n  \n  w_l &lt;- if (isTRUE(smooth_edges)) {\n    exp(-(rl/lr)^2)            # Gaussian \"bump\"\n  } else {\n    terra::ifel(rl &lt;= lr, 1, 0) # hard disc\n  }\n  \n  if (lake_mode %in% c(\"water\",\"hollow\")) {\n    E &lt;- E - as.numeric(lake_depth_m) * w_l\n  }\n  lakeR &lt;- if (identical(lake_mode, \"water\")) terra::ifel(w_l &gt; 1e-6, 1L, 0L)\n  else terra::setValues(terra::rast(Rtemplate), 0L)\n  names(lakeR) &lt;- \"lake\"\n  \n  # --- 3) Main hill ----------------------------------------------------\n  x_hc &lt;- xmin + len_x/3;  y_hc &lt;- y0\n  hr   &lt;- max(1e-6, hill_diam_m/2)\n  rh   &lt;- sqrt((X - x_hc)^2 + (Y - y_hc)^2)\n  \n  w_h_main &lt;- if (hill_mode == \"bump\") {\n    if (isTRUE(hill_smooth)) exp(-(rh/hr)^2) else terra::ifel(rh &lt;= hr, 1, 0)\n  } else {\n    0 * X\n  }\n  E &lt;- E + as.numeric(hill_height_m) * w_h_main\n  \n  # --- 4) Micro hills (additive, clamped to 1) ------------------------\n  w_h_micro &lt;- 0 * X\n  if (random_hills &gt; 0) {\n    if (!is.null(micro_seed)) set.seed(micro_seed)\n    margin &lt;- micro_hill_diam_m/2 + 5\n    hrm &lt;- max(1e-6, micro_hill_diam_m/2)\n    for (i in seq_len(random_hills)) {\n      cx &lt;- runif(1, xmin + margin, xmax - margin)\n      cy &lt;- runif(1, ymin + margin, ymax - margin)\n      r  &lt;- sqrt((X - cx)^2 + (Y - cy)^2)\n      wi &lt;- if (isTRUE(micro_hill_smooth)) exp(-(r/hrm)^2) else terra::ifel(r &lt;= hrm, 1, 0)\n      sum_i &lt;- w_h_micro + wi\n      w_h_micro &lt;- terra::ifel(sum_i &gt; 1, 1, sum_i)  # clamp without pmin()\n    }\n    E &lt;- E + as.numeric(micro_hill_height_m) * w_h_micro\n  }\n  \n  hillW &lt;- w_h_main + w_h_micro\n  hillW &lt;- terra::ifel(hillW &gt; 1, 1, hillW); names(hillW) &lt;- \"hillW\"\n  \n  # --- 5) Derivatives --------------------------------------------------\n  slp &lt;- terra::terrain(E, v = \"slope\",  unit = \"radians\")\n  asp &lt;- terra::terrain(E, v = \"aspect\", unit = \"radians\")\n  \n  # --- 6) Sun & cos(i) -------------------------------------------------\n  sun14 &lt;- sun_pos_utc(sun_date, 14L, lat, lon)\n  sun05 &lt;- sun_pos_utc(sun_date,  5L, lat, lon)\n  I14   &lt;- cosi_fun(sun14$alt, sun14$az, slp, asp); names(I14) &lt;- \"I14\"\n  I05   &lt;- cosi_fun(sun05$alt, sun05$az, slp, asp); names(I05) &lt;- \"I05\"\n  \n  # --- 7) Land cover (1 forest, 2 water, 3 bare, 4 meadows) -----------\n  lc &lt;- terra::setValues(terra::rast(Rtemplate), 4L)  # meadows\n  lc &lt;- terra::ifel(lakeR &gt; 0, 2L, lc)                # water overrides\n  forest_mask &lt;- terra::ifel((hillW &gt; 0.2) | ((slp &gt; 0.15) & (Y &gt; y0)), 1, 0)\n  lc &lt;- terra::ifel((forest_mask == 1) & (lakeR &lt;= 0), 1L, lc)\n  v_slp   &lt;- terra::values(slp)\n  thr_slp &lt;- stats::quantile(v_slp[is.finite(v_slp)], 0.90, na.rm = TRUE)\n  bare_mask &lt;- terra::ifel((slp &gt;= thr_slp) & (lakeR &lt;= 0) & (forest_mask == 0), 1, 0)\n  lc &lt;- terra::ifel(bare_mask == 1, 3L, lc)\n  lc &lt;- terra::clamp(lc, 1L, 4L); names(lc) &lt;- \"lc\"\n  \n  lc_levels &lt;- c(\"forest\",\"water\",\"bare soil\",\"meadows\")\n  lc_colors &lt;- c(\"forest\"=\"#2E8B57\",\"water\"=\"#5DADE2\",\"bare soil\"=\"#C49A6C\",\"meadows\"=\"#7FBF7B\")\n  \n  # --- 8) Noise --------------------------------------------------------\n  if (is.null(noise14)) {\n    set.seed(1001)\n    noise14 &lt;- terra::setValues(terra::rast(E), rnorm(terra::ncell(E), 0, 0.3))\n  }\n  if (is.null(noise05)) {\n    set.seed(1002)\n    noise05 &lt;- terra::setValues(terra::rast(E), rnorm(terra::ncell(E), 0, 0.3))\n  }\n  \n  # --- 9) Physics fields ----------------------------------------------\n  topo &lt;- list(E = E, slp = slp, asp = asp, hillW = hillW)\n  phys &lt;- build_physics_fields(\n    topography = topo, landcover = lc,\n    noise14 = noise14, noise05 = noise05,\n    sun14 = sun14, sun05 = sun05\n  )\n  R14 &lt;- phys$R14; R05 &lt;- phys$R05\n  \n  # --- 10) Return ------------------------------------------------------\n  list(\n    E = E, slp = slp, asp = asp,\n    I14 = I14, I05 = I05,\n    R14 = R14, R05 = R05,\n    lake = lakeR, hillW = hillW,\n    lc = lc, lc_levels = lc_levels, lc_colors = lc_colors,\n    sun = list(T14 = sun14, T05 = sun05)\n  )\n}\n\n\n\n\n# -------------------------- Stationen/Features -------------------------\nmake_stations &lt;- function(domain, n_st = 60,\n                          station_mode = c(\"random\",\"ns_transect\",\"ew_transect\"),\n                          transect_margin_m = 10, ns_offset_m = 0, ew_offset_m = 0,\n                          crs = sf::st_crs(domain$Rtemplate)) {\n  station_mode &lt;- match.arg(station_mode)\n  with(domain, {\n    if (station_mode == \"random\") {\n      pts &lt;- tibble::tibble(\n        id = 1:n_st,\n        x  = runif(n_st, xmin + transect_margin_m, xmax - transect_margin_m),\n        y  = runif(n_st, ymin + transect_margin_m, ymax - transect_margin_m)\n      )\n    } else if (station_mode == \"ns_transect\") {\n      x_const &lt;- min(max(x0 + ns_offset_m, xmin + transect_margin_m), xmax - transect_margin_m)\n      y_seq   &lt;- seq(ymin + transect_margin_m, ymax - transect_margin_m, length.out = n_st)\n      pts &lt;- tibble::tibble(id = 1:n_st, x = x_const, y = y_seq)\n    } else {\n      y_const &lt;- min(max(y0 + ew_offset_m, ymin + transect_margin_m), ymax - transect_margin_m)\n      x_seq   &lt;- seq(xmin + transect_margin_m, xmax - transect_margin_m, length.out = n_st)\n      pts &lt;- tibble::tibble(id = 1:n_st, x = x_seq, y = y_const)\n    }\n    sf::st_as_sf(pts, coords = c(\"x\",\"y\"), crs = crs, remove = FALSE)\n  })\n}\n\nstations_from_scenario &lt;- function(scen, pts_sf) {\n  vpts &lt;- terra::vect(pts_sf)\n  df &lt;- tibble::as_tibble(pts_sf) %&gt;%\n    dplyr::mutate(\n      z_surf = as.numeric(terra::extract(scen$E,   vpts, ID = FALSE)[,1]),\n      slp    = as.numeric(terra::extract(scen$slp, vpts, ID = FALSE)[,1]),\n      I14    = as.numeric(terra::extract(scen$I14, vpts, ID = FALSE)[,1]),\n      I05    = as.numeric(terra::extract(scen$I05, vpts, ID = FALSE)[,1]),\n      lc     = as.integer(terra::extract(scen$lc,  vpts, ID = FALSE)[,1]),\n      T14    = as.numeric(terra::extract(scen$R14, vpts, ID = FALSE)[,1]),\n      T05    = as.numeric(terra::extract(scen$R05, vpts, ID = FALSE)[,1])\n    )\n  lc_levels &lt;- scen$lc_levels\n  pts14 &lt;- df[stats::complete.cases(df[, c(\"x\",\"y\",\"z_surf\",\"slp\",\"I14\",\"lc\",\"T14\")]), ]\n  pts05 &lt;- df[stats::complete.cases(df[, c(\"x\",\"y\",\"z_surf\",\"slp\",\"I05\",\"lc\",\"T05\")]), ]\n  stn_sf_14 &lt;- pts14 %&gt;%\n    dplyr::transmute(id, x, y,\n                     z_surf = as.numeric(z_surf), slp = as.numeric(slp), cosi = as.numeric(I14),\n                     lc = factor(lc_levels[pmax(1, pmin(lc, length(lc_levels)))], levels = lc_levels),\n                     temp = as.numeric(T14)) %&gt;%\n    sf::st_as_sf(coords = c(\"x\",\"y\"), crs = sf::st_crs(pts_sf), remove = FALSE)\n  stn_sf_05 &lt;- pts05 %&gt;%\n    dplyr::transmute(id, x, y,\n                     z_surf = as.numeric(z_surf), slp = as.numeric(slp), cosi = as.numeric(I05),\n                     lc = factor(lc_levels[pmax(1, pmin(lc, length(lc_levels)))], levels = lc_levels),\n                     temp = as.numeric(T05)) %&gt;%\n    sf::st_as_sf(coords = c(\"x\",\"y\"), crs = sf::st_crs(pts_sf), remove = FALSE)\n  list(T14 = stn_sf_14, T05 = stn_sf_05)\n}\n\n# -------------------------- Plots: Übersicht ---------------------------\n\n# -------------------------- Preview: Domain ----------------------------\n# Zeigt Extent, optional ein Grid, und annotiert Kern-Parameter.\npreview_domain &lt;- function(domain, grid = TRUE, grid_step = NULL, annotate = TRUE) {\n  stopifnot(is.list(domain), !is.null(domain$Rtemplate))\n  crs &lt;- sf::st_crs(domain$Rtemplate)\n  bb  &lt;- sf::st_as_sfc(sf::st_bbox(c(\n    xmin = domain$xmin, ymin = domain$ymin,\n    xmax = domain$xmax, ymax = domain$ymax\n  ), crs = crs))\n\n  p &lt;- ggplot2::ggplot() +\n    ggplot2::geom_sf(data = bb, fill = NA, color = \"black\", linewidth = 0.7) +\n    ggplot2::coord_sf(crs = crs, datum = NA, expand = FALSE) +\n    ggplot2::theme_minimal() +\n    ggplot2::labs(title = \"Domain preview\", x = \"Easting (m)\", y = \"Northing (m)\")\n\n  if (isTRUE(grid)) {\n    len_x &lt;- domain$xmax - domain$xmin\n    len_y &lt;- domain$ymax - domain$ymin\n    step  &lt;- if (is.null(grid_step)) max(100, round(min(len_x, len_y)/6, -2)) else grid_step\n    gr    &lt;- sf::st_make_grid(bb, cellsize = c(step, step), what = \"polygons\")\n    gr_ln &lt;- sf::st_as_sf(sf::st_boundary(gr))\n    p &lt;- p + ggplot2::geom_sf(data = gr_ln, color = \"grey70\", linewidth = 0.2)\n  }\n\n  if (isTRUE(annotate)) {\n    r   &lt;- terra::res(domain$Rtemplate)\n    txt &lt;- sprintf(\n      \"Center: (%.0f, %.0f)\\nSize: %.0f × %.0f m\\nRes: %.0f × %.0f m\\nCRS: %s\",\n      domain$x0, domain$y0,\n      domain$xmax - domain$xmin, domain$ymax - domain$ymin,\n      r[1], r[2], as.character(terra::crs(domain$Rtemplate))\n    )\n    p &lt;- p + ggplot2::annotate(\n      \"label\",\n      x = domain$xmin + 0.02 * (domain$xmax - domain$xmin),\n      y = domain$ymin + 0.06 * (domain$ymax - domain$ymin),\n      label = txt, hjust = 0, vjust = 0, size = 3\n    )\n  }\n\n  print(p)\n  invisible(p)\n}\n\n# -------------------------- Preview: Szenario-Raster -------------------\n# Visualisiert die vorhandenen Raster im Szenario (ohne Modelle/CV).\npreview_scenario &lt;- function(x,\n                             which = c(\"lc\",\"E\",\"slp\",\"R14\",\"R05\"),\n                             stations = NULL,\n                             show_contours = TRUE,\n                             layout = c(\"grid\",\"vertical\")) {\n  layout &lt;- match.arg(layout)\n  \n  # ---- Szenario + Stationen erkennen ---------------------------------\n  scen &lt;- if (is.list(x) && !is.null(x$scen)) x$scen else x\n  if (is.null(stations) && is.list(x)) {\n    stations &lt;- x$pts_sf %||% x$stn_sf_14 %||% x$stn_sf_05\n  }\n  \n  # ---- Verfügbare Ebenen sammeln -------------------------------------\n  layers &lt;- list(\n    lc  = scen$lc,\n    E   = scen$E,\n    slp = scen$slp,\n    R14 = scen$R14,\n    R05 = scen$R05\n  )\n  keep &lt;- names(layers) %in% which & vapply(layers, function(r) inherits(r,\"SpatRaster\"), TRUE)\n  layers &lt;- layers[keep]\n  if (!length(layers)) stop(\"preview_scenario(): Keine der angefragten Ebenen im Szenario vorhanden.\")\n  \n  # ---- optionale Konturen vorbereiten --------------------------------\n  add_contours &lt;- function(p) p\n  if (isTRUE(show_contours)) {\n    adders &lt;- list()\n    \n    has_lake &lt;- !is.null(scen$lake) && inherits(scen$lake, \"SpatRaster\")\n    if (has_lake) {\n      lake_df &lt;- as.data.frame(scen$lake, xy = TRUE); names(lake_df) &lt;- c(\"x\",\"y\",\"lake\")\n      adders[[length(adders)+1]] &lt;- ggplot2::geom_contour(\n        data = lake_df,\n        mapping = ggplot2::aes(x = x, y = y, z = lake),\n        breaks = 0.5, colour = \"black\", linewidth = 0.35,\n        inherit.aes = FALSE\n      )\n    }\n    \n    has_hill &lt;- !is.null(scen$hillW) && inherits(scen$hillW, \"SpatRaster\")\n    if (has_hill) {\n      hill_df &lt;- as.data.frame(scen$hillW, xy = TRUE); names(hill_df) &lt;- c(\"x\",\"y\",\"hillW\")\n      adders[[length(adders)+1]] &lt;- ggplot2::geom_contour(\n        data = hill_df,\n        mapping = ggplot2::aes(x = x, y = y, z = hillW),\n        breaks = 0.5, colour = \"black\", linetype = \"22\", linewidth = 0.3,\n        inherit.aes = FALSE\n      )\n    }\n    \n    add_contours &lt;- function(p) {\n      if (length(adders)) for (a in adders) p &lt;- p + a\n      p\n    }\n  }\n  \n  # ---- Einzelplots bauen ----------------------------------------------\n  make_plot &lt;- function(name, r) {\n    df &lt;- as.data.frame(r, xy = TRUE); names(df) &lt;- c(\"x\",\"y\",\"val\")\n    \n    if (name == \"lc\") {\n      levs &lt;- scen$lc_levels %||% c(\"1\",\"2\",\"3\",\"4\")\n      pal  &lt;- scen$lc_colors %||% setNames(scales::hue_pal()(length(levs)), levs)\n      df$val &lt;- factor(df$val, levels = seq_along(levs), labels = levs)\n      p &lt;- ggplot2::ggplot(df, ggplot2::aes(x, y, fill = val)) +\n        ggplot2::geom_raster() +\n        ggplot2::scale_fill_manual(\n          values = pal[levels(df$val)],  # &lt;-- sicher auf Levels abbilden\n          na.value = \"grey90\", name = \"Landuse\"\n        ) +\n        ggplot2::coord_equal() + ggplot2::theme_minimal() +\n        ggplot2::labs(title = \"Landuse\", x = \"Easting\", y = \"Northing\")\n    } else {\n      p &lt;- ggplot2::ggplot(df, ggplot2::aes(x, y, fill = val)) +\n        ggplot2::geom_raster() +\n        ggplot2::coord_equal() + ggplot2::theme_minimal() +\n        ggplot2::labs(\n          title = switch(name,\n                         E   = \"Elevation (m)\",\n                         slp = \"Slope (rad)\",\n                         R14 = \"T 14 UTC\",\n                         R05 = \"T 05 UTC\",\n                         name),\n          x = \"Easting\", y = \"Northing\"\n        )\n      if (name %in% c(\"E\",\"slp\")) {\n        p &lt;- p + ggplot2::scale_fill_viridis_c()\n      } else {\n        p &lt;- p + ggplot2::scale_fill_gradientn(colors = temp_palette(256), name = \"Temp\")\n      }\n    }\n    \n    # Konturen (nur falls vorhanden)\n    p &lt;- add_contours(p)\n    \n    # Stationen optional\n    if (!is.null(stations) && inherits(stations, \"sf\")) {\n      # stilles CRS-Align (Fehlermeldungen unterdrücken)\n      suppressWarnings({\n        stations_plot &lt;- try(sf::st_transform(stations, sf::st_crs(scen$lc %||% scen$E)), silent = TRUE)\n        if (inherits(stations_plot, \"try-error\")) stations_plot &lt;- stations\n        p &lt;- p + ggplot2::geom_sf(\n          data = stations_plot, colour = \"black\", fill = \"white\",\n          shape = 21, size = 1.6, stroke = 0.25, inherit.aes = FALSE\n        )\n      })\n    }\n    p\n  }\n  \n  plots &lt;- Map(make_plot, names(layers), layers)\n  \n  # ---- kombinieren ----------------------------------------------------\n  if (length(plots) == 1) {\n    p_out &lt;- plots[[1]]\n  } else if (layout == \"vertical\") {\n    p_out &lt;- patchwork::wrap_plots(plots, ncol = 1)\n  } else {\n    p_out &lt;- patchwork::wrap_plots(plots, ncol = min(3, length(plots)))\n  }\n  \n  print(p_out)\n  invisible(p_out)\n}\n\n\n\n\nplot_landcover_terrain &lt;- function(scen, stations = NULL, show_contours = TRUE,\n                                   layout = c(\"grid\",\"vertical\")) {\n  layout &lt;- match.arg(layout)\n  lc_df  &lt;- as.data.frame(scen$lc,  xy = TRUE); names(lc_df)  &lt;- c(\"x\",\"y\",\"lc\")\n  E_df   &lt;- as.data.frame(scen$E,   xy = TRUE); names(E_df)   &lt;- c(\"x\",\"y\",\"elev\")\n  slp_df &lt;- as.data.frame(scen$slp, xy = TRUE); names(slp_df) &lt;- c(\"x\",\"y\",\"slp\")\n  lc_df$lc &lt;- factor(lc_df$lc, levels = seq_along(scen$lc_levels), labels = scen$lc_levels)\n  \n  p_lc &lt;- ggplot2::ggplot() +\n    ggplot2::geom_raster(data = lc_df, ggplot2::aes(x, y, fill = lc)) +\n    ggplot2::scale_fill_manual(values = scen$lc_colors, na.value = \"grey90\", name = \"Landuse\") +\n    ggplot2::coord_equal() + ggplot2::theme_minimal() +\n    ggplot2::labs(title = \"Landuse\", x = \"Easting\", y = \"Northing\")\n  \n  p_elev &lt;- ggplot2::ggplot() +\n    ggplot2::geom_raster(data = E_df, ggplot2::aes(x, y, fill = elev)) +\n    ggplot2::scale_fill_viridis_c(name = \"Altitude [m]\") +\n    ggplot2::coord_equal() + ggplot2::theme_minimal() +\n    ggplot2::labs(title = \"Altitude\", x = \"Easting\", y = \"Northing\")\n  \n  p_slp &lt;- ggplot2::ggplot() +\n    ggplot2::geom_raster(data = slp_df, ggplot2::aes(x, y, fill = slp)) +\n    ggplot2::scale_fill_viridis_c(name = \"Slope [rad]\") +\n    ggplot2::coord_equal() + ggplot2::theme_minimal() +\n    ggplot2::labs(title = \"Slope\", x = \"Easting\", y = \"Northing\")\n  \n  if (isTRUE(show_contours)) {\n    lake_df &lt;- as.data.frame(scen$lake, xy = TRUE); names(lake_df) &lt;- c(\"x\",\"y\",\"lake\")\n    hill_df &lt;- as.data.frame(scen$hillW, xy = TRUE); names(hill_df) &lt;- c(\"x\",\"y\",\"hillW\")\n    p_lc  &lt;- p_lc  +\n      ggplot2::geom_contour(data = lake_df, ggplot2::aes(x, y, z = lake),\n                            breaks = 0.5, colour = \"black\", linewidth = 0.35) +\n      ggplot2::geom_contour(data = hill_df, ggplot2::aes(x, y, z = hillW),\n                            breaks = 0.5, colour = \"black\", linetype = \"22\", linewidth = 0.3)\n    p_slp &lt;- p_slp +\n      ggplot2::geom_contour(data = lake_df, ggplot2::aes(x, y, z = lake),\n                            breaks = 0.5, colour = \"black\", linewidth = 0.35) +\n      ggplot2::geom_contour(data = hill_df, ggplot2::aes(x, y, z = hillW),\n                            breaks = 0.5, colour = \"black\", linetype = \"22\", linewidth = 0.3)\n  }\n  if (!is.null(stations)) {\n    add_st &lt;- list(ggplot2::geom_sf(data = stations, colour = \"black\", fill = \"white\",\n                                    shape = 21, size = 1.6, stroke = 0.25, inherit.aes = FALSE))\n    p_lc   &lt;- p_lc   + add_st\n    p_elev &lt;- p_elev + add_st\n    p_slp  &lt;- p_slp  + add_st\n  }\n  if (layout == \"vertical\") {\n    (p_lc / p_elev / p_slp) + patchwork::plot_layout(guides = \"keep\")\n  } else {\n    (p_lc | (p_elev | p_slp)) + patchwork::plot_layout(guides = \"keep\")\n  }\n}\n\nplot_block_overview_2x2_en &lt;- function(scen, pts_sf = NULL) {\n  Rstack &lt;- c(scen$E, scen$slp, scen$I14, scen$I05)\n  df &lt;- terra::as.data.frame(Rstack, xy = TRUE, na.rm = FALSE)\n  names(df) &lt;- c(\"x\",\"y\",\"elev\",\"slope\",\"I14\",\"I05\")\n  theme_base &lt;- ggplot2::theme_minimal(base_size = 11)\n  pal_terrain &lt;- grDevices::hcl.colors(256, \"Terrain\")\n  pal_slope   &lt;- grDevices::hcl.colors(256, \"Viridis\")\n  pal_hot     &lt;- grDevices::hcl.colors(256, \"YlOrRd\")\n  pal_cool    &lt;- grDevices::hcl.colors(256, \"PuBuGn\")\n  p_elev &lt;- ggplot2::ggplot(df, ggplot2::aes(x, y, fill = elev)) +\n    ggplot2::geom_raster() + ggplot2::coord_equal() +\n    ggplot2::scale_fill_gradientn(colours = pal_terrain, name = \"m\") +\n    ggplot2::labs(title = \"Terrain (Elevation)\") + theme_base\n  p_slope &lt;- ggplot2::ggplot(df, ggplot2::aes(x, y, fill = slope)) +\n    ggplot2::geom_raster() + ggplot2::coord_equal() +\n    ggplot2::scale_fill_gradientn(colours = pal_slope, name = \"rad\") +\n    ggplot2::labs(title = \"Slope (radians)\") + theme_base\n  p_I14 &lt;- ggplot2::ggplot(df, ggplot2::aes(x, y, fill = I14)) +\n    ggplot2::geom_raster() + ggplot2::coord_equal() +\n    ggplot2::scale_fill_gradientn(colours = pal_hot, name = \"\") +\n    ggplot2::labs(title = \"Insolation 14 UTC (cos i)\") + theme_base\n  p_I05 &lt;- ggplot2::ggplot(df, ggplot2::aes(x, y, fill = I05)) +\n    ggplot2::geom_raster() + ggplot2::coord_equal() +\n    ggplot2::scale_fill_gradientn(colours = pal_cool, name = \"\") +\n    ggplot2::labs(title = \"Insolation 05 UTC (cos i)\") + theme_base\n  if (!is.null(pts_sf)) {\n    pts_df &lt;- sf::st_drop_geometry(pts_sf)\n    add_pts &lt;- function(p)\n      p + ggplot2::geom_point(data = pts_df, ggplot2::aes(x = x, y = y),\n                              inherit.aes = FALSE, size = 0.7, alpha = 0.7,\n                              colour = \"black\")\n    p_elev  &lt;- add_pts(p_elev); p_slope &lt;- add_pts(p_slope)\n    p_I14   &lt;- add_pts(p_I14);  p_I05   &lt;- add_pts(p_I05)\n  }\n  (p_elev | (p_slope)) / (p_I14 | p_I05) + patchwork::plot_layout(guides = \"collect\")\n}\n\n# -------------------------- Geostat/Models -----------------------------\n# helpers\n.align_factor_to_model &lt;- function(x, lev_model) {\n  xs &lt;- as.character(x)\n  if (!length(lev_model)) return(factor(rep(NA_character_, length(xs))))\n  y &lt;- factor(xs, levels = lev_model)\n  if (anyNA(y)) { xs[is.na(y)] &lt;- lev_model[1]; y &lt;- factor(xs, levels = lev_model) }\n  y\n}\n.default_vgm &lt;- function(values, model = \"Exp\", range = 100) {\n  psill &lt;- stats::var(values, na.rm = TRUE); nug &lt;- 0.1 * psill\n  gstat::vgm(psill = psill, model = model, range = range, nugget = nug)\n}\nsafe_r2 &lt;- function(obs, pred) {\n  idx &lt;- is.finite(obs) & is.finite(pred)\n  if (sum(idx) &lt; 2) return(NA_real_)\n  x &lt;- obs[idx]; y &lt;- pred[idx]\n  sx &lt;- stats::sd(x); sy &lt;- stats::sd(y)\n  if (!is.finite(sx) || !is.finite(sy) || sx == 0 || sy == 0) return(NA_real_)\n  stats::cor(x, y)^2\n}\nsafe_gam_formula &lt;- function(d, include_lc = FALSE) {\n  stopifnot(all(c(\"temp\",\"x\",\"y\") %in% names(d)))\n  d &lt;- d[stats::complete.cases(d[, c(\"temp\",\"x\",\"y\")]), , drop = FALSE]\n  n    &lt;- nrow(d)\n  n_xy &lt;- dplyr::n_distinct(paste0(round(d$x,3), \"_\", round(d$y,3)))\n  k_xy &lt;- max(3, min(60, n_xy - 1L, floor(n * 0.8)))\n  base &lt;- if (n_xy &gt;= 4) sprintf(\"temp ~ s(x,y,bs='tp',k=%d)\", k_xy) else \"temp ~ x + y\"\n  add &lt;- character(0)\n  kcap &lt;- function(x, kmax) {\n    ux &lt;- unique(x[is.finite(x)]); nu &lt;- length(ux)\n    if (nu &lt;= 3) return(0L); max(4L, min(kmax, nu - 1L))\n  }\n  if (\"z_surf\" %in% names(d) && dplyr::n_distinct(d$z_surf) &gt; 3) add &lt;- c(add, sprintf(\"s(z_surf,bs='tp',k=%d)\", kcap(d$z_surf, 20)))\n  if (\"slp\"    %in% names(d) && dplyr::n_distinct(d$slp)    &gt; 3) add &lt;- c(add, sprintf(\"s(slp,bs='tp',k=%d)\",    kcap(d$slp, 12)))\n  if (\"cosi\"   %in% names(d) && dplyr::n_distinct(d$cosi)   &gt; 3) add &lt;- c(add, sprintf(\"s(cosi,bs='tp',k=%d)\",   kcap(d$cosi, 12)))\n  if (include_lc && \"lc\" %in% names(d)) { d$lc &lt;- droplevels(factor(d$lc)); if (nlevels(d$lc) &gt;= 2) add &lt;- c(add, \"lc\") }\n  stats::as.formula(paste(base, paste(add, collapse = \" + \"), sep = if (length(add)) \" + \" else \"\"))\n}\n# learners\n# NOTE:\n# The following learner functions have been moved to a dedicated file\n# (e.g., learners_geostat_core.R):\n#   - pred_Voronoi\n#   - pred_IDW\n#   - pred_OK\n#   - pred_KED\n#   - pred_RF\n#   - pred_GAM\n#\n# Source that file alongside your helpers BEFORE any code that calls them.\n# -------------------------- Block-CV -----------------------------------\nmake_blocks_and_assign &lt;- function(pts_sf, E, block_size = 100) {\n  bb &lt;- sf::st_as_sfc(sf::st_bbox(c(xmin = terra::xmin(E), ymin = terra::ymin(E),\n                                    xmax = terra::xmax(E), ymax = terra::ymax(E)),\n                                  crs = sf::st_crs(pts_sf)))\n  gr &lt;- sf::st_make_grid(bb, cellsize = c(block_size, block_size), what = \"polygons\")\n  blocks &lt;- sf::st_sf(block_id = seq_along(gr), geometry = gr)\n  pts_blk &lt;- sf::st_join(pts_sf, blocks, join = sf::st_intersects, left = TRUE)\n  if (any(is.na(pts_blk$block_id))) {\n    nearest &lt;- sf::st_nearest_feature(pts_blk[is.na(pts_blk$block_id), ], blocks)\n    pts_blk$block_id[is.na(pts_blk$block_id)] &lt;- blocks$block_id[nearest]\n  }\n  list(blocks = blocks, pts = pts_blk)\n}\nplot_blocks_grid &lt;- function(blocks, pts_blk, title = \"Blocks & stations\") {\n  crs_plot &lt;- sf::st_crs(pts_blk)\n  bb       &lt;- sf::st_bbox(blocks)\n  n_blocks &lt;- dplyr::n_distinct(pts_blk$block_id)\n  cols     &lt;- scales::hue_pal()(max(1, n_blocks))\n  ggplot2::ggplot() +\n    ggplot2::geom_sf(data = blocks, fill = NA, color = \"grey50\", linewidth = 0.25) +\n    ggplot2::geom_sf(data = pts_blk, ggplot2::aes(color = factor(block_id)), size = 2, alpha = 0.95) +\n    ggplot2::scale_color_manual(values = cols, name = \"Block\") +\n    ggplot2::coord_sf(crs  = crs_plot, datum = NA,\n                      xlim = c(bb[\"xmin\"], bb[\"xmax\"]),\n                      ylim = c(bb[\"ymin\"], bb[\"ymax\"]), expand = FALSE) +\n    ggplot2::theme_minimal() + ggplot2::labs(title = title, x = \"Easting (m)\", y = \"Northing (m)\")\n}\nrun_lbo_cv &lt;- function(stn_sf, E, block_size = 100, models = c(\"Voronoi\",\"IDW\",\"OK\",\"KED\",\"RF\",\"GAM\")) {\n  if (!all(c(\"x\",\"y\") %in% names(stn_sf))) { xy &lt;- sf::st_coordinates(stn_sf); stn_sf$x &lt;- xy[,1]; stn_sf$y &lt;- xy[,2] }\n  blk &lt;- make_blocks_and_assign(stn_sf, E, block_size = block_size)\n  blocks_sf &lt;- blk$blocks; stn_blk &lt;- blk$pts\n  for (nm in c(\"temp\",\"z_surf\",\"slp\",\"cosi\",\"lc\",\"x\",\"y\")) if (!(nm %in% names(stn_blk))) stn_blk[[nm]] &lt;- stn_sf[[nm]][match(stn_blk$id, stn_sf$id)]\n  \n  block_ids &lt;- sort(unique(stn_blk$block_id))\n  out_list &lt;- vector(\"list\", length(block_ids))\n  for (k in seq_along(block_ids)) {\n    b &lt;- block_ids[k]\n    test_idx  &lt;- which(stn_blk$block_id == b)\n    train_idx &lt;- which(stn_blk$block_id != b)\n    train_sf &lt;- stn_blk[train_idx, ]; test_sf &lt;- stn_blk[test_idx, ]\n    pred_tbl &lt;- lapply(models, function(m) {\n      p &lt;- switch(m,\n                  \"Voronoi\" = pred_Voronoi(train_sf, test_sf),\n                  \"IDW\"     = pred_IDW    (train_sf, test_sf),\n                  \"OK\"      = pred_OK     (train_sf, test_sf),\n                  \"KED\"     = pred_KED    (train_sf, test_sf, E = E),\n                  \"RF\"      = pred_RF     (train_sf, test_sf),\n                  \"GAM\"     = pred_GAM    (train_sf, test_sf),\n                  stop(\"Unknown model: \", m)\n      )\n      tibble::tibble(model = m, id = test_sf$id, obs = test_sf$temp, pred = p, block_id = b)\n    })\n    out_list[[k]] &lt;- dplyr::bind_rows(pred_tbl)\n  }\n  \n  cv_tbl &lt;- dplyr::bind_rows(out_list)\n  metrics &lt;- cv_tbl %&gt;%\n    dplyr::group_by(model) %&gt;%\n    dplyr::summarise(\n      n    = dplyr::n(),\n      n_ok = sum(is.finite(obs) & is.finite(pred)),\n      MAE  = {i &lt;- is.finite(obs) & is.finite(pred); if (any(i)) mean(abs(pred[i]-obs[i])) else NA_real_},\n      RMSE = {i &lt;- is.finite(obs) & is.finite(pred); if (any(i)) sqrt(mean((pred[i]-obs[i])^2)) else NA_real_},\n      Bias = {i &lt;- is.finite(obs) & is.finite(pred); if (any(i)) mean(pred[i]-obs[i]) else NA_real_},\n      R2   = safe_r2(obs, pred),\n      .groups = \"drop\"\n    ) |&gt;\n    dplyr::arrange(RMSE)\n  \n  diag_plot &lt;- ggplot2::ggplot(cv_tbl, ggplot2::aes(obs, pred)) +\n    ggplot2::geom_abline(slope=1, intercept=0, linetype=\"dashed\") +\n    ggplot2::geom_point(alpha=0.7) +\n    ggplot2::coord_equal() + ggplot2::theme_minimal() +\n    ggplot2::labs(title = sprintf(\"LBO-CV (block = %dm) — Observed vs Predicted\", block_size), x = \"Observed\", y = \"Predicted\") +\n    ggplot2::facet_wrap(~ model)\n  \n  blocks_plot &lt;- plot_blocks_grid(blocks_sf, stn_blk, title = sprintf(\"Blocks (%.0f m) & stations\", block_size))\n  list(cv = cv_tbl, metrics = metrics, diag_plot = diag_plot, blocks_plot = blocks_plot)\n}\n\n# -------------------------- „run_for_time“ Wrapper ---------------------\nrun_for_time &lt;- function(stn_sf, truth_r, label,\n                         scen_local,\n                         block_m,\n                         models = c(\"Voronoi\",\"IDW\",\"OK\",\"KED\",\"RF\",\"GAM\"),\n                         layout = c(\"horizontal\",\"vertical\")) {\n  layout &lt;- match.arg(layout)\n  res   &lt;- run_lbo_cv(stn_sf, scen_local$E, block_size = block_m, models = models)\n  maps  &lt;- predict_maps(stn_sf, truth_r, which_time = label,\n                        scen = scen_local, models = models,\n                        lc_levels = scen_local$lc_levels)\n  list(res = res, maps = maps)\n}\n\n# -------------------------- Skalen & Tuning ----------------------------\nplot_variogram_with_scales &lt;- function(vg, L50, L95, sill, title = \"Empirical variogram\") {\n  df &lt;- as.data.frame(vg)\n  ggplot2::ggplot(df, ggplot2::aes(dist, gamma)) +\n    ggplot2::geom_point(size = 1.4) +\n    ggplot2::geom_line(alpha = 0.5) +\n    ggplot2::geom_hline(yintercept = sill, linetype = \"dotted\", linewidth = 0.4) +\n    ggplot2::geom_vline(xintercept = L50, colour = \"#2b8cbe\", linetype = \"dashed\") +\n    ggplot2::geom_vline(xintercept = L95, colour = \"#de2d26\", linetype = \"dashed\") +\n    ggplot2::theme_minimal() +\n    ggplot2::labs(title = title, x = \"Distance (m)\", y = \"Semivariance\")\n}\n.mean_kernel_for_R &lt;- function(r, R_m) {\n  px &lt;- mean(terra::res(r))\n  half &lt;- max(1L, ceiling(R_m / px))\n  k &lt;- 2L * half + 1L\n  W &lt;- matrix(1, nrow = k, ncol = k)\n  W / sum(W)\n}\nsmooth_mean_R &lt;- function(r, R_m) {\n  W &lt;- .mean_kernel_for_R(r, R_m)\n  terra::focal(r, w = W, fun = \"mean\", na.policy = \"omit\", pad = TRUE, normalize = FALSE)\n}\ngaussian_focal &lt;- function(r, radius_m, sigma_m = NULL) {\n  resx &lt;- terra::res(r)[1]\n  if (is.null(sigma_m)) sigma_m &lt;- radius_m / 2\n  rad_px   &lt;- max(1L, round(radius_m / resx))\n  sigma_px &lt;- max(0.5, sigma_m / resx)\n  xs &lt;- -rad_px:rad_px\n  k1 &lt;- exp(-0.5 * (xs / sigma_px)^2); k1 &lt;- k1 / sum(k1)\n  K  &lt;- outer(k1, k1); K / sum(K)\n}\nsmooth_dem_and_derive &lt;- function(E, alt, az, radius_m) {\n  resx &lt;- terra::res(E)[1]\n  pad_cells &lt;- ceiling(radius_m / resx) + 2L\n  \n  E_pad &lt;- terra::extend(E, pad_cells)\n  \n  K &lt;- gaussian_focal(E_pad, radius_m)\n  Es_pad  &lt;- terra::focal(E_pad, w = K, fun = mean, na.policy = \"omit\", pad = TRUE)\n  \n  slp_pad &lt;- terra::terrain(Es_pad, v = \"slope\",  unit = \"radians\")\n  asp_pad &lt;- terra::terrain(Es_pad, v = \"aspect\", unit = \"radians\")\n  \n  ci_pad  &lt;- cosi_fun(alt, az, slp_pad, asp_pad)\n  \n  list(\n    Es   = terra::crop(Es_pad,  E),\n    slp  = terra::crop(slp_pad, E),\n    cosi = terra::crop(ci_pad,  E)\n  )\n}\n\n .extract_to_pts &lt;- function(r, pts_sf) {\n  out &lt;- try(terra::extract(r, terra::vect(pts_sf), ID = FALSE)[,1], silent = TRUE)\n  if (inherits(out, \"try-error\") || length(out) == 0L) rep(NA_real_, nrow(pts_sf)) else out\n}\n cv_gam_with_R &lt;- function(stn_sf, E, alt = NULL, az = NULL, R, block_size_m = NULL, verbose = TRUE,...) \n   {\n   t0 &lt;- proc.time()\n   # robust check whether to compute cos(i)\n   use_cosi &lt;- isTRUE(!is.null(alt) && !is.null(az) &&\n                        is.finite(alt) && is.finite(az)) \n   \n   # --- 0) Block size guard\n   bs &lt;- suppressWarnings(as.numeric(block_size_m)[1])\n   if (!is.finite(bs) || bs &lt;= 0) {\n     bs &lt;- suppressWarnings(as.numeric(get0(\"block_size\", ifnotfound = NA_real_)))\n   }\n   if (!is.finite(bs) || bs &lt;= 0)\n     stop(\"cv_gam_with_R(): no valid block size (block_size_m or global block_size).\")\n   \n   # --- 1) DEM smoothing + derived fields\n   zR   &lt;- smooth_mean_R(E, R)\n   slpR &lt;- terra::terrain(zR, v = \"slope\",  unit = \"radians\")\n   aspR &lt;- terra::terrain(zR, v = \"aspect\", unit = \"radians\")\n   \n   # Only compute cos(i) if BOTH angles are clean, finite scalars\n   use_cosi &lt;- isTRUE(is.numeric(alt) && is.numeric(az) &&\n                        length(alt) == 1L && length(az) == 1L &&\n                        is.finite(alt) && is.finite(az))\n   if (use_cosi) {\n     zen  &lt;- (pi/2 - alt)\n     ci   &lt;- cos(slpR)*cos(zen) + sin(slpR)*sin(zen)*cos(az - aspR)\n     cosiR &lt;- terra::ifel(ci &lt; 0, 0, ci)\n   } else {\n     cosiR &lt;- NULL\n   }\n   \n   # --- 2) Extract to stations (fill missing with medians)\n   if (!all(c(\"x\",\"y\") %in% names(stn_sf))) {\n     xy &lt;- sf::st_coordinates(stn_sf); stn_sf$x &lt;- xy[,1]; stn_sf$y &lt;- xy[,2]\n   }\n   fill_med &lt;- function(v) {\n     m &lt;- stats::median(v[is.finite(v)], na.rm = TRUE)\n     v[!is.finite(v)] &lt;- m\n     v\n   }\n   stn_sf$z_surf_R &lt;- fill_med(.extract_to_pts(zR,   stn_sf))\n   stn_sf$slp_R    &lt;- fill_med(.extract_to_pts(slpR, stn_sf))\n   if (use_cosi) {\n     stn_sf$cosi_R &lt;- fill_med(.extract_to_pts(cosiR, stn_sf))\n   } else {\n     stn_sf$cosi_R &lt;- NA_real_\n   }\n   \n   # --- 3) Build blocks\n   bb_poly &lt;- sf::st_as_sfc(sf::st_bbox(stn_sf), crs = sf::st_crs(stn_sf))\n   blocks  &lt;- sf::st_make_grid(bb_poly, cellsize = c(bs, bs), what = \"polygons\")\n   blocks  &lt;- sf::st_sf(block_id = seq_along(blocks), geometry = blocks)\n   \n   stn_blk &lt;- sf::st_join(stn_sf, blocks, join = sf::st_intersects, left = TRUE)\n   if (anyNA(stn_blk$block_id)) {\n     i &lt;- is.na(stn_blk$block_id)\n     stn_blk$block_id[i] &lt;- blocks$block_id[sf::st_nearest_feature(stn_blk[i,], blocks)]\n   }\n   if (!all(c(\"x\",\"y\") %in% names(stn_blk))) {\n     xy &lt;- sf::st_coordinates(stn_blk); stn_blk$x &lt;- xy[,1]; stn_blk$y &lt;- xy[,2]\n   }\n   \n   # --- 4) LBO-CV\n   bids  &lt;- sort(unique(stn_blk$block_id))\n   pm_say(\"[cv_gam_with_R] R=%.0f m | block=%.0f m | stations=%d | blocks=%d | cos(i)=%s\",\n          R, block_size_m, nrow(stn_sf), length(bids),\n          if (use_cosi) \"yes\" else \"no\", v = verbose)\n   preds &lt;- vector(\"list\", length(bids)); j &lt;- 0L\n   \n   for (b in bids) {\n     te &lt;- stn_blk[stn_blk$block_id == b, ]\n     tr &lt;- stn_blk[stn_blk$block_id != b, ]\n     pm_say(\"  - block %d: train=%d, test=%d\", b, nrow(tr), nrow(te), v = verbose)\n     \n     \n     dtr  &lt;- sf::st_drop_geometry(tr)\n     # include cosi_R only if it’s present with finite values\n     need &lt;- c(\"temp\",\"x\",\"y\",\"z_surf_R\",\"slp_R\")\n     inc_cosi &lt;- (\"cosi_R\" %in% names(dtr)) && any(is.finite(dtr$cosi_R))\n     if (inc_cosi) need &lt;- c(need, \"cosi_R\")\n     \n     dtr  &lt;- dtr[stats::complete.cases(dtr[, need, drop = FALSE]), need, drop = FALSE]\n     if (nrow(dtr) &lt; 10) next\n     \n     # dynamic k guards\n     n_xy &lt;- dplyr::n_distinct(paste0(round(dtr$x,3), \"_\", round(dtr$y,3)))\n     k_xy &lt;- .k_for_xy(nrow(dtr), n_xy)\n     k_z  &lt;- .kcap_unique(dtr$z_surf_R, 20)\n     k_sl &lt;- .kcap_unique(dtr$slp_R,    12)\n     if (inc_cosi) k_ci &lt;- .kcap_unique(dtr$cosi_R, 12)\n     \n     # assemble formula with only informative terms\n     terms &lt;- c()\n     terms &lt;- c(terms, if (n_xy &gt;= 4) sprintf(\"s(x,y,bs='tp',k=%d)\", k_xy) else \"x + y\")\n     terms &lt;- c(terms, if (k_z  &gt;= 4) sprintf(\"s(z_surf_R,bs='tp',k=%d)\", k_z)  else \"z_surf_R\")\n     if (length(unique(dtr$slp_R[is.finite(dtr$slp_R)])) &gt; 1)\n       terms &lt;- c(terms, if (k_sl &gt;= 4) sprintf(\"s(slp_R,bs='tp',k=%d)\", k_sl) else \"slp_R\")\n     if (inc_cosi && any(is.finite(dtr$cosi_R)) &&\n         length(unique(dtr$cosi_R[is.finite(dtr$cosi_R)])) &gt; 1)\n       terms &lt;- c(terms, if (k_ci &gt;= 4) sprintf(\"s(cosi_R,bs='tp',k=%d)\", k_ci) else \"cosi_R\")\n     \n     form &lt;- stats::as.formula(paste(\"temp ~\", paste(terms, collapse = \" + \")))\n     gm &lt;- mgcv::gam(form, data = dtr, method = \"REML\", select = TRUE)\n     \n     dte &lt;- sf::st_drop_geometry(te)\n     # restrict to variables actually in the model\n     vars_needed &lt;- setdiff(all.vars(form), \"temp\")\n     dte &lt;- dte[, vars_needed, drop = FALSE]\n     ph  &lt;- try(stats::predict(gm, newdata = dte, type = \"response\"), silent = TRUE)\n     if (inherits(ph, \"try-error\")) ph &lt;- rep(NA_real_, nrow(dte))\n     \n     j &lt;- j + 1L\n     preds[[j]] &lt;- tibble::tibble(id = te$id, obs = te$temp, pred = as.numeric(ph), block_id = b)\n   }\n   \n   preds &lt;- preds[seq_len(j)]\n   if (!length(preds)) {\n     return(list(cv = tibble::tibble(id = integer(), obs = numeric(), pred = numeric(), block_id = integer()),\n                 RMSE = NA_real_))\n   }\n   out  &lt;- dplyr::bind_rows(preds)\n   rmse &lt;- sqrt(mean((out$pred - out$obs)^2, na.rm = TRUE))\n   list(cv = out, RMSE = rmse)\n }\n \n \n \n\n tune_Rstar_ucurve &lt;- function(stn_sf, E, alt = NULL, az = NULL,\n                               L50, L95, block_fallback = 120,\n                               n_grid = 6, extra = c(0.8, 1.2),\n                               scen = NULL, which_time = c(\"T14\",\"T05\")) {\n   \n   which_time &lt;- match.arg(which_time)\n   \n   # fallback L50/L95 if broken\n   e &lt;- terra::ext(E)\n   dom_diag &lt;- sqrt((terra::xmax(e)-terra::xmin(e))^2 + (terra::ymax(e)-terra::ymin(e))^2)\n   if (!is.finite(L50) || !is.finite(L95) || L95 &lt;= L50) {\n     L50 &lt;- dom_diag/10; L95 &lt;- dom_diag/4\n   }\n   block_m &lt;- max(block_fallback, round(L50))\n   \n   # sun from scen if not given\n   if ((is.null(alt) || is.null(az)) && !is.null(scen)) {\n     s &lt;- .get_sun(scen, which_time)\n     alt &lt;- s$alt; az &lt;- s$az\n   }\n   \n   R_min &lt;- max(10, round(L50*extra[1])); R_max &lt;- round(L95*extra[2])\n   R_grid &lt;- unique(round(seq(R_min, R_max, length.out = n_grid)))\n   \n   rows &lt;- lapply(R_grid, function(R) {\n     z &lt;- cv_gam_with_R(stn_sf, E, alt = alt, az = az, R = R,\n                        block_size_m = block_m, scen = NULL, which_time = which_time)\n     data.frame(R = R, RMSE = z$RMSE)\n   })\n   df &lt;- do.call(rbind, rows)\n   R_star &lt;- df$R[which.min(df$RMSE)]\n   list(grid = df, R_star = as.numeric(R_star), block_m = block_m)\n }\n \n \n\nplot_ucurve &lt;- function(df, R_star, title = \"U-curve: tune R\") {\n  ggplot2::ggplot(df, ggplot2::aes(R, RMSE)) +\n    ggplot2::geom_line() + ggplot2::geom_point() +\n    ggplot2::geom_vline(xintercept = R_star, linetype = \"dashed\", colour = \"#de2d26\") +\n    ggplot2::theme_minimal() + ggplot2::labs(title = title, x = \"Drift radius R (m)\", y = \"RMSE (block-CV)\")\n}\n\n# Drop-in replacement\nadd_drifts_at_R &lt;- function(stn_sf, E, alt, az, R,\n                            lc = NULL, lc_levels = NULL,\n                            na_action = c(\"error\",\"fill\",\"drop\")) {\n  na_action &lt;- match.arg(na_action)\n  \n  # 0) Align CRS (key cause of NA extractions)\n  crs_r &lt;- sf::st_crs(E)\n  if (!isTRUE(sf::st_crs(stn_sf) == crs_r)) {\n    stn_sf &lt;- sf::st_transform(stn_sf, crs_r)\n  }\n  \n  # 1) Build @R* features (Es, slope, cosi) — your existing function\n  fr &lt;- smooth_dem_and_derive(E, alt, az, radius_m = R)\n  \n  # 2) Extract to points\n  v &lt;- terra::vect(stn_sf)\n  stn_sf$E_R    &lt;- as.numeric(terra::extract(fr$Es,   v, ID = FALSE)[, 1])\n  stn_sf$slp_R  &lt;- as.numeric(terra::extract(fr$slp,  v, ID = FALSE)[, 1])\n  stn_sf$cosi_R &lt;- as.numeric(terra::extract(fr$cosi, v, ID = FALSE)[, 1])\n  \n  # Optional LC (factor) — unchanged logic\n  if (!is.null(lc)) {\n    if (is.null(lc_levels)) lc_levels &lt;- lc_levels_default\n    lc_idx &lt;- as.integer(terra::extract(lc, v, ID = FALSE)[, 1])\n    lc_idx[!is.finite(lc_idx)] &lt;- 1L\n    lc_idx &lt;- pmax(1L, pmin(lc_idx, length(lc_levels)))\n    stn_sf$lc &lt;- factor(lc_levels[lc_idx], levels = lc_levels)\n  }\n  \n  # 3) Handle NA per policy\n  d &lt;- sf::st_drop_geometry(stn_sf)\n  miss &lt;- !stats::complete.cases(d[, c(\"E_R\",\"slp_R\",\"cosi_R\"), drop = FALSE])\n  \n  if (any(miss)) {\n    if (na_action == \"error\") {\n      stop(\"Station features at R* contain NA. Increase padding in smooth_dem_and_derive(), \",\n           \"reduce R*, or call add_drifts_at_R(..., na_action='fill'/'drop').\")\n    }\n    if (na_action == \"fill\") {\n      fill_med &lt;- function(x) { m &lt;- stats::median(x[is.finite(x)], na.rm = TRUE); x[!is.finite(x)] &lt;- m; x }\n      stn_sf$E_R    &lt;- fill_med(stn_sf$E_R)\n      stn_sf$slp_R  &lt;- fill_med(stn_sf$slp_R)\n      stn_sf$cosi_R &lt;- fill_med(stn_sf$cosi_R)\n    }\n    if (na_action == \"drop\") {\n      stn_sf &lt;- stn_sf[!miss, ]\n    }\n  }\n  \n  stn_sf\n}\n\ncompute_Ls_from_points &lt;- function(stn_sf, value_col = \"temp\",\n                                   maxdist = NULL, nlag = 18, smooth_k = 3) {\n  stopifnot(inherits(stn_sf, \"sf\"), value_col %in% names(stn_sf))\n  pts &lt;- stn_sf[is.finite(stn_sf[[value_col]]), ]\n  if (is.null(maxdist)) {\n    bb &lt;- sf::st_bbox(pts)\n    dom_diag &lt;- sqrt((bb[\"xmax\"]-bb[\"xmin\"])^2 + (bb[\"ymax\"]-bb[\"ymin\"])^2)\n    maxdist &lt;- dom_diag / 2\n  }\n  form &lt;- stats::as.formula(sprintf(\"%s ~ 1\", value_col))\n  vg  &lt;- gstat::variogram(form, data = pts, cutoff = maxdist, width = maxdist/nlag)\n  if (nrow(vg) &gt;= smooth_k) {\n    vg$gamma &lt;- stats::filter(vg$gamma, rep(1/smooth_k, smooth_k), sides = 2)\n    vg$gamma[!is.finite(vg$gamma)] &lt;- zoo::na.approx(vg$gamma, na.rm = FALSE)\n    vg$gamma &lt;- zoo::na.locf(zoo::na.locf(vg$gamma, fromLast = TRUE))\n  }\n  sill &lt;- max(vg$gamma, na.rm = TRUE)\n  if (!is.finite(sill) || sill &lt;= 0) sill &lt;- stats::median(vg$gamma, na.rm = TRUE)\n  L_at_q &lt;- function(q) {\n    thr &lt;- q * sill\n    i   &lt;- which(vg$gamma &gt;= thr)[1]\n    if (is.na(i)) return(NA_real_)\n    if (i == 1) return(vg$dist[1])\n    d0 &lt;- vg$dist[i-1]; d1 &lt;- vg$dist[i]\n    g0 &lt;- vg$gamma[i-1]; g1 &lt;- vg$gamma[i]\n    if (!is.finite(d0) || !is.finite(d1) || g1 == g0) return(d1)\n    d0 + (thr - g0) * (d1 - d0) / (g1 - g0)\n  }\n  list(vg = vg, sill = sill, L50 = L_at_q(0.5), L95 = L_at_q(0.95), cutoff = maxdist)\n}\n\n# -------------------------- Error-Budget --------------------------------\n\n# -------------------------- Error-Budget --------------------------------\nnugget_fraction_from_cv &lt;- function(cv_sf_or_df, model, crs_ref, x_col=\"x\", y_col=\"y\",\n                                    cutoff = NULL, width = NULL) {\n  stopifnot(!missing(model))\n  df &lt;- dplyr::filter(cv_sf_or_df, .data$model == !!model)\n  \n  # Ensure sf\n  sf &lt;- if (inherits(df, \"sf\")) df else sf::st_as_sf(df, coords = c(x_col, y_col), crs = sf::st_crs(crs_ref))\n  sf &lt;- sf::st_transform(sf, sf::st_crs(crs_ref))\n  sf$resid &lt;- sf$obs - sf$pred\n  \n  xy  &lt;- sf::st_coordinates(sf) %&gt;% as.data.frame()\n  dat &lt;- dplyr::bind_cols(sf::st_drop_geometry(sf), xy)\n  \n  if (is.null(cutoff)) cutoff &lt;- max(dist(xy)) * 0.5\n  if (is.null(width))  width  &lt;- cutoff / 12\n  \n  vg  &lt;- gstat::variogram(resid ~ 1, data = dat, locations = ~ X + Y,\n                          cutoff = cutoff, width = width)\n  fit &lt;- gstat::fit.variogram(vg, gstat::vgm(\"Mat\"))\n  nug &lt;- fit$psill[fit$model == \"Nug\"]; sill &lt;- sum(fit$psill)\n  if (length(nug) && is.finite(sill) && sill &gt; 0) nug / sill else NA_real_\n}\n\nsimple_error_budget &lt;- function(res_cv, sigma_inst = 0.5, alpha = 0.6) {\n  res &lt;- res_cv$cv\n  res &lt;- res[is.finite(res$obs) & is.finite(res$pred), , drop = FALSE]\n  RMSE &lt;- sqrt(mean((res$pred - res$obs)^2))\n  Bias &lt;- mean(res$pred - res$obs)\n  VarE &lt;- stats::var(res$pred - res$obs, na.rm = TRUE)\n  meas &lt;- sigma_inst^2\n  proc &lt;- max(0, VarE - meas)\n  micro &lt;- alpha * proc\n  meso  &lt;- (1 - alpha) * proc\n  tibble::tibble(Component = c(\"RMSE\",\"Bias\",\"Total var\",\"Instrument var\",\"Microscale var\",\"Mesoscale var\"),\n                 Value     = c(RMSE, Bias, VarE, meas, micro, meso))\n}\n## ======================================================================\n## Ende der Bibliothek\n## ======================================================================\n\n## ---------- Mini-Beispiel (nicht Teil der Bibliothek) -----------------\n## domain  &lt;- make_domain()\n## scen    &lt;- build_scenario(domain, lake_mode=\"water\", hill_mode=\"bump\",\n##                           random_hills = 100, micro_seed = 1)\n## pts_sf  &lt;- make_stations(domain, n_st = 60, station_mode = \"random\")\n## stns    &lt;- stations_from_scenario(scen, pts_sf)\n## bs      &lt;- compute_block_size(len_x = domain$xmax-domain$xmin,\n##                               len_y = domain$ymax-domain$ymin, n_st = nrow(pts_sf))\n## out14   &lt;- run_for_time(stns$T14, scen$R14, \"T14\", scen, bs)\n## out05   &lt;- run_for_time(stns$T05, scen$R05, \"T05\", scen, bs)\n## # Plot-Beispiele:\n## # print(plot_landcover_terrain(scen, stations = stns$T14))\n## # print(out14$res$blocks_plot); print(out14$res$diag_plot)\n```\n\n\n-   `fun_learn_predict_core.R`: learning/validation/prediction\n    routines (block CV, learners, map prediction, residual\n    diagnostics).\n\n\nCode\n```{r}\n#| eval: false\n#' Geostatistical Learners & Map Predictor (Core Only)\n#'\n#' @title Learners and Raster Predictor (no helpers inside)\n#' @description\n#' A compact set of **model-specific predictors** used in your teaching/\n#' pipeline code, plus a high-level `predict_maps()` convenience that\n#' evaluates multiple learners on a full grid.  \n#'\n#' This file intentionally contains **no helpers**. It assumes that common\n#' utilities and constants are sourced from your *helpers* module, including:\n#' - `%||%` — null-coalescing helper\n#' - `.default_vgm()` — conservative variogram fallback\n#' - `.align_factor_to_model()` — align factor levels at predict time\n#' - `safe_gam_formula()` — guarded GAM formula constructor\n#' - `lc_levels_default` — global land-cover levels\n#' - `temp_palette`, `stretch_q` — visualization defaults\n#'\n#' @details\n#' **Contract expected by all learners**:\n#' - `train_sf`, `test_sf` are `sf` objects with at least:\n#'   - `temp` (numeric): the response variable to be learned\n#'   - `x`, `y` (numeric): planar coordinates (will be derived from geometry\n#'     if absent)\n#'   - Drift/covariate columns depending on the learner (see each function)\n#' - Each learner returns a numeric vector of predictions aligned with\n#'   `nrow(test_sf)`.\n#'\n#' **Coordinate Reference System**: all learners assume that `x` and `y`\n#' are in a **projected CRS** with meter-like units (e.g., UTM).\n#'\n#' **Error handling**:\n#' - Learners are defensive; if inputs are insufficient (e.g., too few rows,\n#'   missing drift columns), they return `NA_real_` predictions of the correct\n#'   length instead of failing hard (except where a *hard requirement* is unmet\n#'   such as missing KED drifts in training).\n#'\n#' @section Dependencies:\n#' - **Packages**: `sf`, `gstat`, `mgcv`, `randomForest`, `terra`, `ggplot2`,\n#'   `tibble`, `dplyr`, `stats`, `scales`\n#' - **Helpers (sourced elsewhere)**: `%||%`, `.default_vgm`, `.align_factor_to_model`,\n#'   `safe_gam_formula`, `lc_levels_default`, `temp_palette`, `stretch_q`\n#'\n#' @seealso\n#' - Your helpers/utilities module for the functions noted above.\n#' - `gstat::krige`, `gstat::idw`, `gstat::variogram`, `gstat::fit.variogram`\n#' - `mgcv::gam`, `randomForest::randomForest`\n#'\n#' @keywords geostatistics interpolation kriging regression GAM randomForest\n#' @family learners\n\n\n#' Voronoi / Nearest-Station Predictor\n#'\n#' @description\n#' Assigns each prediction point the observed value from the **nearest**\n#' training station (a fast proxy for Voronoi interpolation).\n#'\n#' @param train_sf `sf` with at least `temp` and geometry.\n#' @param test_sf  `sf` with geometry to predict for.\n#'\n#' @return Numeric vector `length(nrow(test_sf))` with nearest-neighbor temps.\n#' @examples\n#' # y_hat &lt;- pred_Voronoi(train_sf, grid_sf)\npred_Voronoi &lt;- function(train_sf, test_sf) {\n  idx &lt;- sf::st_nearest_feature(test_sf, train_sf)\n  as.numeric(train_sf$temp)[idx]\n}\n\n\n#' Inverse Distance Weighting (IDW)\n#'\n#' @description\n#' Classic **IDW** using `gstat::idw`, predicting from training points to\n#' the test geometry.\n#'\n#' @param train_sf `sf` with `temp` and geometry.\n#' @param test_sf  `sf` with geometry.\n#' @param idp      Inverse distance power (default `2`).\n#'\n#' @return Numeric vector of predictions for `test_sf`.\n#' @examples\n#' # y_hat &lt;- pred_IDW(train_sf, grid_sf, idp = 2)\npred_IDW &lt;- function(train_sf, test_sf, idp = 2) {\n  pr &lt;- suppressWarnings(gstat::idw(temp ~ 1, locations = train_sf, newdata = test_sf, idp = idp))\n  as.numeric(pr$var1.pred)\n}\n\n\n#' Ordinary Kriging (OK)\n#'\n#' @description\n#' Univariate **OK** with an automatically fitted **exponential** variogram.\n#' Falls back to `.default_vgm()` if fitting fails (e.g., too few points).\n#'\n#' @param train_sf `sf` with `temp` and geometry.\n#' @param test_sf  `sf` with geometry.\n#'\n#' @return Numeric vector of kriged predictions.\n#' @examples\n#' # y_hat &lt;- pred_OK(train_sf, grid_sf)\npred_OK &lt;- function(train_sf, test_sf) {\n  vg      &lt;- suppressWarnings(gstat::variogram(temp ~ 1, data = train_sf))\n  vgm_fit &lt;- try(suppressWarnings(gstat::fit.variogram(vg, gstat::vgm(\"Exp\"))), silent = TRUE)\n  if (inherits(vgm_fit, \"try-error\")) vgm_fit &lt;- .default_vgm(train_sf$temp)\n  pr &lt;- suppressWarnings(gstat::krige(temp ~ 1, locations = train_sf, newdata = test_sf, model = vgm_fit))\n  as.numeric(pr$var1.pred)\n}\n\n\n#' Kriging with External Drift (KED)\n#'\n#' @description\n#' **KED** with additive drift terms. Requires drifts in *training*, fills\n#' non-finite values in *test* by median of training. If `lc` is present in\n#' both sets, it is included as a categorical drift with aligned levels.\n#'\n#' @details\n#' **Required drift columns** in `train_sf`: `z_surf`, `slp`, `cosi`.  \n#' If any are missing in training, this function errors (by design).\n#'\n#' @param train_sf `sf`; must contain `temp`, `z_surf`, `slp`, `cosi`, geometry,\n#'   and optionally `lc`.\n#' @param test_sf  `sf` with geometry and preferably the same drift columns\n#'   (non-finite values are median-filled).\n#' @param ...      Unused (placeholder for compatibility).\n#'\n#' @return Numeric vector of KED predictions, `length(nrow(test_sf))`.\n#' @examples\n#' # y_hat &lt;- pred_KED(train_sf, grid_sf)\npred_KED &lt;- function(train_sf, test_sf, ...) {\n  need &lt;- c(\"z_surf\",\"slp\",\"cosi\")\n  miss &lt;- setdiff(need, names(train_sf))\n  if (length(miss)) stop(\"pred_KED(): missing drifts in training: \", paste(miss, collapse = \", \"))\n  use_lc &lt;- \"lc\" %in% names(train_sf) && \"lc\" %in% names(test_sf)\n  tr &lt;- train_sf; te &lt;- test_sf\n  if (use_lc) {\n    tr$lc &lt;- droplevels(factor(tr$lc))\n    te$lc &lt;- factor(as.character(te$lc), levels = levels(tr$lc))\n    te$lc[is.na(te$lc)] &lt;- levels(tr$lc)[1]\n  }\n  for (nm in need) {\n    m &lt;- stats::median(tr[[nm]][is.finite(tr[[nm]])], na.rm = TRUE)\n    te[[nm]][!is.finite(te[[nm]])] &lt;- m\n  }\n  keep_tr &lt;- c(\"temp\", need, if (use_lc) \"lc\")\n  dtr &lt;- sf::st_drop_geometry(tr)[, keep_tr, drop = FALSE]\n  ok  &lt;- stats::complete.cases(dtr); tr &lt;- tr[ok, ]\n  if (nrow(tr) &lt; 5) return(rep(NA_real_, nrow(te)))\n  form &lt;- stats::as.formula(paste(\"temp ~\", paste(c(need, if (use_lc) \"lc\"), collapse = \" + \")))\n  vg      &lt;- suppressWarnings(gstat::variogram(form, data = tr))\n  vgm_fit &lt;- try(suppressWarnings(gstat::fit.variogram(vg, gstat::vgm(\"Exp\"))), silent = TRUE)\n  if (inherits(vgm_fit, \"try-error\")) {\n    ps &lt;- stats::var(sf::st_drop_geometry(tr)$temp, na.rm = TRUE)\n    vgm_fit &lt;- gstat::vgm(psill = ps, model = \"Exp\", range = max(vg$dist, na.rm = TRUE)/3, nugget = 0.1*ps)\n  }\n  pr &lt;- suppressWarnings(gstat::krige(form, locations = tr, newdata = te, model = vgm_fit))\n  as.numeric(pr$var1.pred)\n}\n\n\n#' Random Forest Regressor (RF)\n#'\n#' @description\n#' A **RandomForest** on spatial and drift features. If `lc` is absent, a\n#' harmless single-level factor is injected (levels provided by\n#' `lc_levels_default`). At prediction, factor levels are aligned using\n#' `.align_factor_to_model()`.\n#'\n#' @param train_sf `sf` with `temp`, `x`, `y`, `z_surf`, `slp`, `cosi`,\n#'   optionally `lc` (factor), and geometry.\n#' @param test_sf  `sf` with the same predictors (geometry required).\n#'\n#' @return Numeric vector of RF predictions.\n#' @examples\n#' # y_hat &lt;- pred_RF(train_sf, grid_sf)\npred_RF &lt;- function(train_sf, test_sf) {\n  dtr &lt;- sf::st_drop_geometry(train_sf)\n  if (!(\"lc\" %in% names(dtr))) dtr$lc &lt;- factor(lc_levels_default[1], levels = lc_levels_default)\n  dtr$lc &lt;- droplevels(factor(as.character(dtr$lc), levels = lc_levels_default))\n  dtr &lt;- stats::na.omit(dtr)\n  if (nrow(dtr) &lt; 5) return(rep(NA_real_, nrow(test_sf)))\n  rf  &lt;- randomForest::randomForest(temp ~ x + y + z_surf + slp + cosi + lc, data = dtr, na.action = na.omit)\n  dte &lt;- sf::st_drop_geometry(test_sf)\n  if (!(\"lc\" %in% names(dte))) dte$lc &lt;- factor(lc_levels_default[1], levels = lc_levels_default)\n  lev &lt;- levels(dtr$lc)\n  dte$lc &lt;- .align_factor_to_model(dte$lc, lev)\n  good &lt;- stats::complete.cases(dte[, c(\"x\",\"y\",\"z_surf\",\"slp\",\"cosi\",\"lc\")])\n  out  &lt;- rep(NA_real_, nrow(dte)); if (any(good)) out[good] &lt;- stats::predict(rf, dte[good, ])\n  out\n}\n\n\n#' Generalized Additive Model (GAM)\n#'\n#' @description\n#' A **GAM** (thin-plate splines) built with a protective formula from\n#' `safe_gam_formula()` that caps basis sizes and includes `lc` only if\n#' useful. Requires a minimal number of complete rows.\n#'\n#' @param train_sf `sf` with `temp`, `x`, `y`, `z_surf`, `slp`, `cosi`,\n#'   optionally `lc` (factor).\n#' @param test_sf  `sf` with matching predictors.\n#'\n#' @return Numeric vector of GAM predictions; `NA_real_` if the model could\n#'   not be trained.\n#' @examples\n#' # y_hat &lt;- pred_GAM(train_sf, grid_sf)\npred_GAM &lt;- function(train_sf, test_sf) {\n  dtr  &lt;- sf::st_drop_geometry(train_sf)\n  keep &lt;- intersect(c(\"temp\",\"x\",\"y\",\"z_surf\",\"slp\",\"cosi\",\"lc\"), names(dtr))\n  dtr  &lt;- dtr[stats::complete.cases(dtr[, keep, drop = FALSE]), keep, drop = FALSE]\n  if (!nrow(dtr)) return(rep(NA_real_, nrow(test_sf)))\n  if (\"lc\" %in% names(dtr)) dtr$lc &lt;- droplevels(factor(dtr$lc))\n  inc_lc &lt;- \"lc\" %in% names(dtr) && nlevels(dtr$lc) &gt;= 2\n  if (nrow(dtr) &lt; 10) return(rep(NA_real_, nrow(test_sf)))\n  gm &lt;- mgcv::gam(formula = safe_gam_formula(dtr, include_lc = inc_lc), data = dtr, method = \"REML\", select = TRUE)\n  dte &lt;- sf::st_drop_geometry(test_sf)\n  vars &lt;- c(\"x\",\"y\",\"z_surf\",\"slp\",\"cosi\", if (inc_lc) \"lc\"); vars &lt;- intersect(vars, names(dte))\n  if (inc_lc) {\n    lev &lt;- levels(model.frame(gm)$lc)\n    if (!(\"lc\" %in% names(dte))) dte$lc &lt;- lev[1]\n    dte$lc &lt;- .align_factor_to_model(dte$lc, lev)\n  }\n  good &lt;- stats::complete.cases(dte[, vars, drop = FALSE])\n  out  &lt;- rep(NA_real_, nrow(dte)); if (any(good)) out[good] &lt;- stats::predict(gm, dte[good, vars, drop = FALSE], type = \"response\")\n  out\n}\n\n\n#' Predict on a Raster Grid with Multiple Learners + Pretty Plots\n#'\n#' @description\n#' High-level utility that:\n#' 1. Ensures station covariates exist (E, slope, cos(i), optional LC).\n#' 2. Builds a **full-grid** data frame of covariates from rasters.\n#' 3. Runs selected learners (`Voronoi`, `IDW`, `OK`, `KED`, `RF`, `GAM`).\n#' 4. Returns both **prediction rasters** and **ggplot** panels.\n#'\n#' @param stn_sf `sf` training stations; must have `temp` and (if missing)\n#'   this function will derive `x`, `y` and extract missing covariates from\n#'   rasters.\n#' @param truth_raster `SpatRaster` (single-layer) used only for common\n#'   color scaling in plots (and optional “truth” visualization).\n#' @param which_time Character; `\"T14\"` or `\"T05\"` (plot titles only).\n#' @param scen A scenario list containing at least: `E`, `slp`, and either\n#'   `I14` or `I05` (for cos(i)) and optionally `lc` + `lc_levels`.\n#' @param models Character vector of learners to run.\n#' @param lc_levels Optional character vector of LC levels (defaults to\n#'   `scen$lc_levels`).\n#' @param feature_rasters Optional list with named rasters `E`, `slp`, `cosi`\n#'   to **override** the scenario’s baseline (e.g., when using tuned R*).\n#'\n#' @return A list with:\n#' \\describe{\n#'   \\item{pred_df}{Tidy `tibble` of predictions for all models & grid cells}\n#'   \\item{pred_rasters}{`list` of `SpatRaster` predictions, one per model}\n#'   \\item{p_pred}{`ggplot` facet showing all model maps}\n#'   \\item{p_truth}{`ggplot` of the truth raster (for reference)}\n#' }\n#'\n#' @note\n#' Requires helpers/constants: `%||%`, `temp_palette`, `stretch_q`, plus\n#' land-cover level alignment utilities.\n#'\n#' @examples\n#' # out &lt;- predict_maps(stn_sf, scen$R14, which_time = \"T14\", scen = scen)\n#' # print(out$p_truth); print(out$p_pred)\npredict_maps &lt;- function(stn_sf, truth_raster,\n                         which_time = c(\"T14\",\"T05\"),\n                         scen, models = c(\"Voronoi\",\"IDW\",\"OK\",\"KED\",\"RF\",\"GAM\"),\n                         lc_levels = NULL,\n                         feature_rasters = NULL) {\n  which_time &lt;- match.arg(which_time)\n  lc_levels  &lt;- lc_levels %||% scen$lc_levels\n  E      &lt;- feature_rasters$E   %||% scen$E\n  slp_r  &lt;- feature_rasters$slp %||% scen$slp\n  cosi_r &lt;- feature_rasters$cosi %||% if (which_time == \"T14\") scen$I14 else scen$I05\n  has_lc &lt;- (\"lc\" %in% names(scen)) && !is.null(scen$lc)\n  lc_r   &lt;- if (has_lc) scen$lc else NULL\n  \n  train_sf &lt;- stn_sf\n  if (!all(c(\"x\",\"y\") %in% names(train_sf))) {\n    xy &lt;- sf::st_coordinates(train_sf); train_sf$x &lt;- xy[,1]; train_sf$y &lt;- xy[,2]\n  }\n  if (!(\"z_surf\" %in% names(train_sf)))\n    train_sf$z_surf &lt;- as.numeric(terra::extract(E,      sf::st_coordinates(train_sf))[,1])\n  if (!(\"slp\" %in% names(train_sf)))\n    train_sf$slp    &lt;- as.numeric(terra::extract(slp_r,  sf::st_coordinates(train_sf))[,1])\n  if (!(\"cosi\" %in% names(train_sf)))\n    train_sf$cosi   &lt;- as.numeric(terra::extract(cosi_r, sf::st_coordinates(train_sf))[,1])\n  if (has_lc && !(\"lc\" %in% names(train_sf))) {\n    lc_codes &lt;- as.integer(terra::extract(lc_r, sf::st_coordinates(train_sf))[,1])\n    lc_codes[is.na(lc_codes)] &lt;- 1L\n    lc_codes &lt;- pmax(1L, pmin(lc_codes, length(lc_levels)))\n    train_sf$lc &lt;- factor(lc_levels[lc_codes], levels = lc_levels)\n  }\n  \n  xy &lt;- as.data.frame(terra::xyFromCell(E, 1:terra::ncell(E))); names(xy) &lt;- c(\"x\",\"y\")\n  grid_df &lt;- xy\n  grid_df$z_surf &lt;- as.numeric(terra::values(E))\n  grid_df$slp    &lt;- as.numeric(terra::values(slp_r))\n  grid_df$cosi   &lt;- as.numeric(terra::values(cosi_r))\n  if (has_lc) {\n    lc_codes &lt;- as.integer(terra::values(lc_r))\n    lc_codes[!is.finite(lc_codes)] &lt;- 1L\n    lc_codes &lt;- pmax(1L, pmin(lc_codes, length(lc_levels)))\n    grid_df$lc &lt;- factor(lc_levels[lc_codes], levels = lc_levels)\n  }\n  grid_sf &lt;- sf::st_as_sf(grid_df, coords = c(\"x\",\"y\"),\n                          crs = sf::st_crs(train_sf), remove = FALSE)\n  \n  use_lc &lt;- has_lc && (\"lc\" %in% names(train_sf)) && (\"lc\" %in% names(grid_sf))\n  if (use_lc) {\n    lev &lt;- levels(droplevels(factor(train_sf$lc)))\n    train_sf$lc &lt;- factor(as.character(train_sf$lc), levels = lev)\n    grid_sf$lc  &lt;- factor(as.character(grid_sf$lc),  levels = lev)\n    if (anyNA(train_sf$lc) || anyNA(grid_sf$lc)) {\n      use_lc &lt;- FALSE; train_sf$lc &lt;- NULL; grid_sf$lc &lt;- NULL\n    }\n  }\n  \n  pred_list &lt;- list()\n  if (\"Voronoi\" %in% models) pred_list$Voronoi &lt;- pred_Voronoi(train_sf, grid_sf)\n  if (\"IDW\"     %in% models) pred_list$IDW     &lt;- pred_IDW    (train_sf, grid_sf, idp = 2)\n  if (\"OK\"      %in% models) pred_list$OK      &lt;- pred_OK     (train_sf, grid_sf)\n  if (\"KED\"     %in% models) pred_list$KED     &lt;- pred_KED    (train_sf, grid_sf)\n  if (\"RF\"      %in% models) {\n    dtr &lt;- sf::st_drop_geometry(train_sf)\n    rf_vars &lt;- c(\"x\",\"y\",\"z_surf\",\"slp\",\"cosi\", if (use_lc) \"lc\")\n    dtr &lt;- stats::na.omit(dtr[, c(\"temp\", rf_vars), drop = FALSE])\n    pred_list$RF &lt;- if (nrow(dtr) &gt;= 5) {\n      rf &lt;- randomForest::randomForest(stats::as.formula(paste(\"temp ~\", paste(rf_vars, collapse = \" + \"))),\n                                       data = dtr, na.action = na.omit)\n      as.numeric(stats::predict(rf, sf::st_drop_geometry(grid_sf)[, rf_vars, drop = FALSE]))\n    } else rep(NA_real_, nrow(grid_sf))\n  }\n  if (\"GAM\"     %in% models) pred_list$GAM     &lt;- pred_GAM    (train_sf, grid_sf)\n  \n  pred_df &lt;- dplyr::bind_rows(lapply(names(pred_list), function(nm) {\n    tibble::tibble(model = nm, x = grid_df$x, y = grid_df$y, pred = pred_list[[nm]])\n  }))\n  \n  make_r &lt;- function(vals, template = E) { r &lt;- terra::rast(template); terra::values(r) &lt;- as.numeric(vals); r }\n  pred_rasters &lt;- lapply(pred_list, make_r)\n  \n  truth_df &lt;- as.data.frame(truth_raster, xy = TRUE, na.rm = FALSE)\n  names(truth_df) &lt;- c(\"x\",\"y\",\"truth\")\n  lims &lt;- stats::quantile(truth_df$truth, probs = stretch_q, na.rm = TRUE)\n  \n  p_pred &lt;- ggplot2::ggplot(pred_df, ggplot2::aes(x, y, fill = pred)) +\n    ggplot2::geom_raster() +\n    ggplot2::scale_fill_gradientn(colors = temp_palette(256), limits = lims,\n                                  oob = scales::squish, name = \"Temp\") +\n    ggplot2::coord_equal() + ggplot2::theme_minimal() +\n    ggplot2::labs(title = sprintf(\"%s — Predictions by model\", which_time),\n                  x = \"Easting\", y = \"Northing\") +\n    ggplot2::facet_wrap(~ model, ncol = 3)\n  \n  p_truth &lt;- ggplot2::ggplot(truth_df, ggplot2::aes(x, y, fill = truth)) +\n    ggplot2::geom_raster() +\n    ggplot2::scale_fill_gradientn(colors = temp_palette(256), limits = lims,\n                                  oob = scales::squish, name = \"Temp\") +\n    ggplot2::coord_equal() + ggplot2::theme_minimal() +\n    ggplot2::labs(title = sprintf(\"%s — Truth raster\", which_time),\n                  x = \"Easting\", y = \"Northing\")\n  \n  list(pred_df = pred_df, pred_rasters = pred_rasters, p_pred = p_pred, p_truth = p_truth)\n}\n```\n\n\n\nScenarios (data & geometry): scenarios/ + registry.R. Each scenario provides a make() factory returning a standard object contract (see below). registry.R maps a name to its builder; source_scenario(name) returns the make() function. Scenarios control domain rasters, station sets, and recommended block size.\n\n\n\nCode\n```{r}\n#| eval: false\n# =====================================================================\n# block4_5/scenarios/lake_bump_dense.R\n# Benötigt: source(\"block4_5/src/pipemodel_functions.R\") davor\n# Definiert: SCEN_NAME, SCEN_DESC, make(...)\n# =====================================================================\n\nSCEN_NAME &lt;- \"lake_bump_dense\"\nSCEN_DESC &lt;- \"Valley with lake (right), bump hill (left) + dense micro-hills; random stations.\"\n\n# Defaults so wie bisher genutzt (nur Settings/Parameter)\n.defaults &lt;- list(\n  # Domain\n  center_E = 600000, center_N = 5725000,\n  len_x = 600, len_y = 400, res = 10, crs = \"EPSG:32632\",\n  \n  # Topographie-Features\n  lake_mode = \"water\",   # \"none\" | \"water\" | \"hollow\"\n  hill_mode = \"bump\",    # \"none\" | \"bump\"\n  lake_diam_m  = 100, lake_depth_m = 500, smooth_edges = FALSE,\n  hill_diam_m  = 100, hill_height_m = 500, hill_smooth  = FALSE,\n  \n  # micro-relief\n  random_hills        = 50,\n  micro_hill_diam_m   = 30,\n  micro_hill_height_m = 50,\n  micro_hill_smooth   = FALSE,\n  micro_seed          = NULL,\n  \n\n  # Sonne/Geo\n  lat = 51.8, lon = 10.6, sun_date = as.Date(\"2024-06-21\"),\n  \n  # Stationen\n  station_mode = \"random\",  # \"random\" | \"ns_transect\" | \"ew_transect\"\n  n_st = 60,\n  transect_margin_m = 10,\n  ns_offset_m = 0,\n  ew_offset_m = 0,\n  \n  # Modelle + Block-CV\n  models = c(\"Voronoi\",\"IDW\",\"OK\",\"KED\",\"RF\",\"GAM\"),\n  block_size = NA_real_    # wenn NA -&gt; automatisch berechnet\n)\n\n# einfaches Mergen (ohne Seiteneffekte)\n.merge &lt;- function(a, b) { a[names(b)] &lt;- b; a }\n\n# ---------------------------------------------------------------------\n# make(overrides = list(), do_cv = FALSE)\n# baut Domain -&gt; Szenario -&gt; Stationen -&gt; (optional) CV\n# nutzt ausschließlich Funktionen aus pipemodel_functions.R\n# ---------------------------------------------------------------------\nmake &lt;- function(overrides = list(), do_cv = FALSE) {\n  p &lt;- .merge(.defaults, overrides)\n  \n  # 1) Domain\n  domain &lt;- make_domain(\n    center_E = p$center_E, center_N = p$center_N,\n    len_x = p$len_x, len_y = p$len_y, res = p$res, crs = p$crs\n  )\n  \n  # 2) Szenario (Topographie/Physikfelder)\n  scen &lt;- build_scenario(\n    domain       = domain,\n    lake_mode    = p$lake_mode,\n    hill_mode    = p$hill_mode,\n    \n    # &lt;&lt;&lt; diese Zeilen fehlten bisher\n    lake_diam_m  = p$lake_diam_m,\n    lake_depth_m = p$lake_depth_m,\n    smooth_edges = p$smooth_edges,\n    hill_diam_m  = p$hill_diam_m,\n    hill_height_m= p$hill_height_m,\n    hill_smooth  = p$hill_smooth,\n    # &gt;&gt;&gt;\n    \n    # micro-relief\n    random_hills        = p$random_hills,\n    micro_hill_diam_m   = p$micro_hill_diam_m,\n    micro_hill_height_m = p$micro_hill_height_m,\n    micro_hill_smooth   = p$micro_hill_smooth,\n    micro_seed          = p$micro_seed,\n    \n    # Sonne/Geo\n    lat = p$lat, lon = p$lon, sun_date = p$sun_date\n  )\n  \n  # 3) Stationen\n  pts_sf &lt;- make_stations(\n    domain,\n    n_st = p$n_st,\n    station_mode = p$station_mode,\n    transect_margin_m = p$transect_margin_m,\n    ns_offset_m = p$ns_offset_m,\n    ew_offset_m = p$ew_offset_m\n  )\n  \n  # 4) Station-Features/Targets extrahieren\n  stns &lt;- stations_from_scenario(scen, pts_sf)\n  stn_sf_14 &lt;- stns$T14\n  stn_sf_05 &lt;- stns$T05\n  \n  # 5) Blockgröße\n  block_size &lt;- if (is.finite(p$block_size)) {\n    as.numeric(p$block_size)\n  } else {\n    compute_block_size(\n      len_x = domain$xmax - domain$xmin,\n      len_y = domain$ymax - domain$ymin,\n      n_st  = p$n_st\n    )\n  }\n  \n  out &lt;- list(\n    name       = SCEN_NAME,\n    desc       = SCEN_DESC,\n    params     = p,\n    domain     = domain,\n    scen       = scen,\n    pts_sf     = pts_sf,\n    stn_sf_14  = stn_sf_14,\n    stn_sf_05  = stn_sf_05,\n    block_size = block_size\n  )\n  \n  # 6) Optional: Block-CV (nur wenn gewünscht)\n  if (isTRUE(do_cv)) {\n    out$cv &lt;- list(\n      T14 = run_for_time(stn_sf_14, scen$R14, \"T14\",\n                         scen_local = scen, block_m = block_size, models = p$models),\n      T05 = run_for_time(stn_sf_05, scen$R05, \"T05\",\n                         scen_local = scen, block_m = block_size, models = p$models)\n    )\n  }\n  \n  out\n}\n# =====================================================================\n```",
    "crumbs": [
      "Block 4: Microclimate Data Retrieval & Spatial Data Interpolation",
      "PipeModel Idealized valley microclimate sandbox"
    ]
  },
  {
    "objectID": "block4_5/mc_2025_1.html#pipeline-stages",
    "href": "block4_5/mc_2025_1.html#pipeline-stages",
    "title": "PipeModel Idealized valley microclimate sandbox",
    "section": "Pipeline stages",
    "text": "Pipeline stages\n\nSetup model\n\nSetup Source packages.R; disable s2 for robust planar ops in projected UTM domains; set seeds. Source core helpers and the scenario registry.\nScenario selection & build Select scen_name via Sys.getenv(\"SCEN\", \"&lt;default&gt;\"). make_fun &lt;- source_scenario(scen_name) → obj &lt;- make_fun().\nSanity checks Assert E, R14, R05 exist; sun$T14/sun$T05 include numeric alt,az.\nLive preview (no side effects) Quick plots to catch wiring/CRS issues early: land-cover + terrain overview, 2×2 domain panel, a scenario preview.\n\n\n\n\n\n\n\n\n\n\n\nOverview Setup\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nStep\nFunction/Call\nPurpose\nKey args (examples)\nOutput\n\n\n\n\nS1\nsource(here::here(\"block4_5/src/packages.R\"))\nLoad packages & global options\n–\nPackages loaded\n\n\nS2\nsf::sf_use_s2(FALSE)\nRobust planar ops in UTM\n–\ns2 disabled\n\n\nS3\nsource(here::here(\"block4_5/src/fun_pipemodel.R\"))\nPlot/feature/scale utilities\n–\nHelper funcs\n\n\nS4\nsource(here::here(\"block4_5/src/fun_learn_predict_core.R\"))\nLearn/validate/predict core\n–\nCore funcs\n\n\nS5\nsource(here::here(\"block4_5/scenarios/registry.R\"))\nScenario registry\n–\nsource_scenario()\n\n\nS6\nmake_fun &lt;- source_scenario(scen_name)\nSelect scenario\nscen_name\nmake() factory\n\n\nS7\nobj &lt;- make_fun()\nBuild scenario object\noverrides, do_cv (opt.)\nscen, stn_sf_14, stn_sf_05, block_size, params\n\n\n\nScenario object obj (returned by make()):\n\nscen: named list of rasters and metadata\n\nE: DEM (reference geometry)\nR14, R05: truth rasters for ~14 UTC and ~05 UTC\nlc: land-cover raster (optional; with scen$lc_levels)\nsun: list with T14 and T05, each having numeric alt and az (mandatory for R* tuning)\noptional helpers (illumination, indices) as needed by features/plots\n\nstn_sf_14, stn_sf_05: sf point layers with at least temp and covariate fields\nblock_size: integer (meters) used for leave-block-out CV\nparams$models: character vector of learner names to run\n\nTuned feature rasters @ R* (produced later):\n\nEs (smoothed DEM), slp (slope from Es), cosi (cosine of incidence given sun alt,az)\n\n\n\n\n\n\nLearning and Predicting\n\nBaseline learning & maps For each time slice (T14, T05):\n\nrun_for_time(st, R, \"Txx\", scen_local=scen, block_m=block_size, models=mods)\nProduces: CV metrics (MAE/RMSE/R²), blocks plot (spatial CV folds), diag plot (pred vs obs), truth & prediction maps.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWhat do we see?\nThe pipeline is behaving sensibly: block CV exposes how much of the error comes from sensors versus spatial structure. At daytime (T14), most error is already “accounted for” by instrumentation; what remains splits between fine-scale heterogeneity and broader structure. At pre-dawn (T05), fields are smoother at the choosen station spacing: after peeling off instrument noise, almost all leftover error is mesoscale—i.e., large-scale processes your mean/residual model doesn’t yet capture.\n\n\nDaytime (T14)\n\nWhat the errors mean: Sensors do a lot of the damage; once you discount that, the remainder is balanced: some is truly microscale (representativeness—canopy edges, roughness, small topographic breaks), and some is mesoscale (gentle gradients / structure not fully in the drift).\nWhat you likely see on maps: Predictions look coherent, but local pockets near environmental transitions (edge effects, slope/aspect shifts) show residual texture. Scatter plots are tight with mild spread; block-wise boxplots show similar medians across your better models.\nModel behavior: Simple geometric baselines (Voronoi/IDW) are adequate as references but can over- or under-smooth. KED/GAM/RF generally capture the daytime drift better; their residuals still show a bit of spatial correlation, which is consistent with the micro+meso split.\n\nImplication: Improvements now come from targeted features (multi-scale terrain/roughness/canopy, radiation states) and, if residuals are directional, anisotropic/cost-aware residual modeling. More raw smoothing won’t help much.\n\n\nPre-dawn (T05)\n\nWhat the errors mean: After instrument noise, the leftover error is overwhelmingly mesoscale. That points to organised nocturnal processes—cold-air drainage, pooling, gentle advection—operating at scales larger than your microstructure and not fully encoded by the current drift.\nWhat you likely see on maps: Smooth, broad residual swaths aligned with terrain and flow paths rather than speckled local noise. Scatter remains fair, but block-wise errors can vary between blocks that sit on/away from drainage lines.\nModel behavior: Purely geometric methods under-represent the organised night-time structure; RF/GAM help if fed flow-aware features, but without those drivers their residuals still carry structure. Kriging alone won’t fix it unless the metric (anisotropy, barriers, cost distance) matches the physics.\n\nImplication: Focus on process-scale cues: flow-aligned neighborhoods, barriers across ridgelines, cost/geodesic distances that penalise uphill motion, and drift terms that proxy stratification (e.g., cold-air pathways, sky-view, topographic indices at appropriate scales). If residual correlation persists after that, add residual kriging with an anisotropic/cost metric.\n\n\nPractical takeaways\n\nYou’re not bottlenecked by algorithm choice; you’re bottlenecked by process representation:\n\nT14: refine multi-scale features (terrain filters, canopy/roughness, radiation states) and allow directional residuals where wind/valley orientation matters.\nT05: elevate flow physics in the mean and residual metric; think valley graphs, cost distance, barriers.\n\nSampling/design: If T14 micro noise bothers you, micro-targeted placements (edges, small basins) or denser spacing help. For T05 meso, ensure transects across valley axes and basin outlets so the model “sees” the gradients it must learn.\nQC nudge: If any model shows physically implausible tails (e.g., extreme GAM values at night), tighten smoothness or clip to plausible ranges and revisit feature scaling.\n\nBottom line: baseline runs already tell a coherent story—daytime limits are representativeness + mild missing drift; night-time limits are larger-scale drainage/advection. Aim your next changes at process-aware features and matching the residual metric to the physics rather than chasing extra smoothing.\n\nScale inference & tuning\n\nVariogram → L50/L95: compute_Ls_from_points(st, value_col=\"temp\") returns empirical variogram and correlation scales; plotting highlights L50/L95.\nU-curve → R*: tune_Rstar_ucurve(st, E, alt, az, L50, L95, block_fallback, n_grid) scans smoothing radii around [L50, L95] to find the RMSE-minimizer R* via spatial CV.\nFeatures @ R*: smooth_dem_and_derive(E, alt, az, radius_m=R*) → Es, slp, cosi.\nRe-extract station features @ R*: add_drifts_at_R(st, E, alt, az, R*, lc=..., lc_levels=...) aligns training features with tuned rasters.\nTuned CV: run_lbo_cv(st_R, E=scen$E, block_size=block_size, models=mods) → updated metrics and CV table.\nTuned maps: predict_maps(st_R, truth_raster=Rxx, which_time=\"Txx\", scen, models, lc_levels, feature_rasters=list(E=Es, slp=slp, cosi=cosi)).\nPanels & diagnostics: build_panels_truth_preds_errors_paged(...) shows truth | predictions | residual diagnostics.\nError budgets (optional): simple_error_budget(...) aggregates instrument/micro/meso components; plot_error_budget() stacks them for base vs tuned.\n\n\n\n\n\n\n\n\n\n\n\n\nModeling, Prediction & Tuning\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nStep\nFunction/Call\nPurpose\nKey args (examples)\nOutput\n\n\n\n\n\n\nP1\nplot_landcover_terrain(scen, stations = st14, layout=\"vertical\")\nQuick domain sanity check\nscen, stations\nggplot\n\n\n\n\nP2\nplot_block_overview_2x2_en(scen, pts_sf = st14)\n2×2 overview\nscen, pts_sf\nggplot\n\n\n\n\nP3\npreview_scenario(obj)\nScenario preview\nobj\nPanels/plots\n\n\n\n\nP4\nrun_for_time(st14, scen$R14, \"T14\", scen_local = scen, block_m = bs, models = mods)\nBaseline LBO-CV + maps (T14)\nstations, truth raster, blocks, models\nres={metrics, blocks_plot, diag_plot}, maps={p_truth, p_pred}\n\n\n\n\nP5\nrun_for_time(st05, scen$R05, \"T05\", …)\nBaseline (T05)\nas above\nas above\n\n\n\n\nP6\nLs14 &lt;- compute_Ls_from_points(st14, value_col=\"temp\")\nVariogram & scales (T14)\nstations, value col\n{vg, L50, L95, sill}\n\n\n\n\nP7\nplot_variogram_with_scales(Ls14$vg, Ls14$L50, Ls14$L95, Ls14$sill, \"…\")\nVariogram plot\nvariogram + scales\nggplot\n\n\n\n\nP8\ntune_Rstar_ucurve(st14, scen$E, alt=scen$sun$T14$alt, az=scen$sun$T14$az, L50=Ls14$L50, L95=Ls14$L95, block_fallback=bs, n_grid=6)\nU-curve → R* (T14)\nstations, DEM, sun, L50/L95\n{grid, R_star, block_m}\n\n\n\n\nP9\nfr14 &lt;- smooth_dem_and_derive(scen$E, scen$sun$T14$alt, scen$sun$T14$az, radius_m=tune14$R_star)\nFeatures @ R*\nDEM, sun, R*\n{Es, slp, cosi}\n\n\n\n\nP10\nst14_R &lt;- add_drifts_at_R(st14, scen$E, scen$sun$T14$alt, scen$sun$T14$az, tune14$R_star, lc=scen$lc, lc_levels=scen$lc_levels)\nStation features @ R*\nstations, DEM, sun, R*, LC\nsf with drifts\n\n\n\n\nP11\nbench14 &lt;- run_lbo_cv(st14_R, E=scen$E, block_size=bs, models=mods)\nTuned LBO-CV (T14)\ntuned stations, DEM, blocks, models\n{metrics, cv, …}\n\n\n\n\nP12\nmaps14_tuned &lt;- predict_maps(st14_R, truth_raster=scen$R14, which_time=\"T14\", scen=scen, models=mods, lc_levels=scen$lc_levels, feature_rasters=list(E=fr14$Es, slp=fr14$slp, cosi=fr14$cosi))\nMaps @ R* (T14)\ntuned stations + features\nPred rasters/plots\n\n\n\n\nP13\npanel_T14 &lt;- build_panels_truth_preds_errors_paged(maps14_tuned, scen$R14, bench14$cv, \"T14\", models_per_page=7, scatter_next_to_truth=TRUE)\nTruth\npreds\nresiduals panel\nmaps, truth, CV\nlist of ggplot\n\n\nP14\nsimple_error_budget(run14$res, sigma_inst=0.5, alpha=0.6)\nError budget (baseline/tuned)\nCV results, params\ntidy table\n\n\n\n\n\n\nMirror P6–P14 for T05 with st05, R05, sun$T05.\n\n\n\n\n\n\nSaving Results\n\nExports (optional, end-only side effects) Controlled by export &lt;- TRUE/FALSE. When TRUE:\n\nCreate results_&lt;scen-name&gt;/{fig,tab,ras}.\nSave baseline and tuned plots (previews, CV blocks/diag, truth/pred, variograms, U-curves, panels).\nSave tables (metrics, U-curve grid, scales L50/L95/R*, error budgets).\nSave rasters (E, R14, R05, lc if present).\nWrite sessionInfo().\n\n\n\n\n\n\n\n\nDocumentation & Export\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nStep\nFunction/Call\nPurpose\nKey args (examples)\nOutput\n\n\n\n\nD1\nfn_fig(\"name\"), fn_ras(\"name\")\nBuild output paths\nstems, ext\nfile paths\n\n\nD2\nsave_plot_min(p, fn_fig(\"plot_name\"))\nSave plot safely\nggplot, size, dpi\nPNG\n\n\nD3\nsafe_save_plot(p, fn_fig(\"plot_name\"))\nTry-save without aborting\nplot, path\nPNG (best-effort)\n\n\nD4\nsave_table_readable(df, file_stem, title=..., digits=3)\nSave tables\ndata.frame, stem\nCSV/HTML/XLSX\n\n\nD5\nsave_raster_min(r, fn_ras(\"raster_name\"))\nSave raster (GeoTIFF)\nSpatRaster, path\nTIF\n\n\nD6\nterra::writeRaster(r, fn_ras(\"name\"), overwrite=TRUE)\nLow-level raster write\nraster, path\nTIF\n\n\nD7\nsaveRDS(sessionInfo(), file.path(out_dir, sprintf(\"%s_sessionInfo.rds\", scen_name)))\nSession record\nscen name\nRDS",
    "crumbs": [
      "Block 4: Microclimate Data Retrieval & Spatial Data Interpolation",
      "PipeModel Idealized valley microclimate sandbox"
    ]
  },
  {
    "objectID": "block4_5/mc_2025_1.html#headline",
    "href": "block4_5/mc_2025_1.html#headline",
    "title": "PipeModel Idealized valley microclimate sandbox",
    "section": "Headline",
    "text": "Headline\n\nAccuracy: Day (T14) ~0.45 °C RMSE, Pre-dawn (T05) ~0.60 °C RMSE — similar before/after tuning.\nError structure:\n\nT14: sensors dominate; leftover split between micro and meso.\nT05: sensors ~half; remainder is mostly mesoscale (smooth nocturnal structure).",
    "crumbs": [
      "Block 4: Microclimate Data Retrieval & Spatial Data Interpolation",
      "PipeModel Idealized valley microclimate sandbox"
    ]
  },
  {
    "objectID": "block4_5/mc_2025_1.html#daytime-t14-1",
    "href": "block4_5/mc_2025_1.html#daytime-t14-1",
    "title": "PipeModel Idealized valley microclimate sandbox",
    "section": "Daytime (T14)",
    "text": "Daytime (T14)\nBaseline (LBO-CV)\n\nRMSE: 0.452 °C (Total var 0.206 °C²)\nInstrument: 0.116 °C² (~56%)\nMicroscale: 0.0496 °C² (~24%)\nMesoscale: 0.0401 °C² (~20%) Reading: After removing sensor noise, remaining error is fairly balanced between sub-grid heterogeneity (representativeness) and broader, unmodelled structure.\n\nTuned (R* features)\n\nRMSE: essentially unchanged (≈ 0.45 °C).\nBudget: slices stay close to baseline (minor shifts only). Reading: Tuning mainly harmonizes feature scale (R*) and improves map coherence; it doesn’t move headline error much.",
    "crumbs": [
      "Block 4: Microclimate Data Retrieval & Spatial Data Interpolation",
      "PipeModel Idealized valley microclimate sandbox"
    ]
  },
  {
    "objectID": "block4_5/mc_2025_1.html#pre-dawn-t05-1",
    "href": "block4_5/mc_2025_1.html#pre-dawn-t05-1",
    "title": "PipeModel Idealized valley microclimate sandbox",
    "section": "Pre-dawn (T05)",
    "text": "Pre-dawn (T05)\nBaseline (LBO-CV)\n\nRMSE: 0.600 °C (Total var 0.362 °C²)\nInstrument: 0.185 °C² (~51%)\nMicroscale: ≈ 0 °C² (~0%)\nMesoscale: 0.177 °C² (~49%) Reading: Nearly all non-instrument error is mesoscale — consistent with smooth nocturnal fields (drainage/advection, stratification) at your station spacing.\n\nTuned (R* features)\n\nRMSE: essentially unchanged (≈ 0.60 °C).\nBudget: instrument remains ~51%; remainder still meso-heavy. Reading: Tuning doesn’t change the picture: add process-scale structure rather than more smoothing.",
    "crumbs": [
      "Block 4: Microclimate Data Retrieval & Spatial Data Interpolation",
      "PipeModel Idealized valley microclimate sandbox"
    ]
  },
  {
    "objectID": "block4_5/mc_2025_1.html#what-to-do-next-precise-minimal",
    "href": "block4_5/mc_2025_1.html#what-to-do-next-precise-minimal",
    "title": "PipeModel Idealized valley microclimate sandbox",
    "section": "What to do next (precise & minimal)",
    "text": "What to do next (precise & minimal)\n\nSensors (both times): sanity-check σ_inst using co-location/specs; improve shielding/siting if instrument slice looks high.\nT14 (micro + meso):\n\nMicro: enrich multi-scale terrain/roughness/canopy features; consider slightly smaller R* or multi-radius features.\nMeso: add/strengthen process covariates (radiation states, exposure), and allow anisotropy/cost in residuals where flow/wind channel influence.\n\nT05 (meso-dominated): favor flow-aligned neighborhoods, barriers across ridgelines, and advection/drainage proxies; if RF/GAM residuals remain correlated, add residual kriging (possibly anisotropic/cost-based).",
    "crumbs": [
      "Block 4: Microclimate Data Retrieval & Spatial Data Interpolation",
      "PipeModel Idealized valley microclimate sandbox"
    ]
  },
  {
    "objectID": "block4_5/mc_2025_1.html#how-to-present-clear-story",
    "href": "block4_5/mc_2025_1.html#how-to-present-clear-story",
    "title": "PipeModel Idealized valley microclimate sandbox",
    "section": "How to present (clear story)",
    "text": "How to present (clear story)\n\nPer time slice: metrics table → obs-pred scatter → block-wise error boxplots → truth vs prediction maps → stacked error-budget bars (Instrument / Micro / Meso).\nThen show the same row for tuned to highlight that scale harmonization improved map consistency while headline RMSE stayed stable.\n\nBottom line: Your models are already accurate at T14 and reasonable at T05. The limiting factor during the day is representativeness + mild missing structure; at night it’s large-scale process not yet in the mean/residual model. Focus on process-aware features and anisotropic/cost residuals, not more smoothing.\n\nError budget — idea, concept, and purpose\nIdea. Turn the overall CV error (\\(\\sigma_{\\text{cv}}^2 \\approx \\text{RMSE}^2\\)) into a story of where it comes from:\n\nInstrument (sensor noise you assume),\nMicroscale (sub-grid, representativeness),\nMesoscale (larger-scale, unmodelled structure).\n\nConcept.\n\nFirst peel off a fixed instrument part using your sensor SD (sigma_inst).\nSplit the leftover between micro and meso with α.\n\nPrefer α from the residual variogram’s nugget fraction (on CV residuals): nugget ≈ micro.\nOtherwise use a site heuristic (open 0.2–0.4; mixed 0.4–0.6; complex 0.6–0.8).\n\n\nWhat for.\n\nDiagnose limits: Is error dominated by instrument, micro (representativeness), or meso (missing process/scale)?\nGuide action:\n\nBig instrument → sensor QA, shielding, calibration.\nBig micro → finer scale (R*), denser stations, better canopy/roughness features.\nBig meso → add process covariates (drift), anisotropy/cost metrics, rethink neighborhood/blocks.\n\nCompare models/scenarios: Same RMSE can hide very different error structures.\nCommunicate uncertainty: Bars in °C² (optionally show SD by √).\n\nWhy σ_inst and α.\n\nσ_inst externalizes known noise (you choose it from specs/co-location).\nα makes the remaining variance interpretable by scale: micro (&lt; ~R*/2) vs meso (≳ ~R*).\n\nPractical workflow.\n\nCompute CV errors → overall variance.\nSet σ_inst.\nGet α from residual variogram nugget fraction (per time/model), fallback to heuristic.\nReport instrument / micro / meso components; act on the largest slice.\n\nHere’s the plain-English read of your results.\n\n\nT14 (daytime)\n\nTotal variance: 0.206 °C² (RMSE ≈ 0.452 °C).\nInstrument: 0.116 °C² → ~56% of total (SD ≈ 0.34 °C).\nMicroscale: 0.0496 °C² → ~24% (SD ≈ 0.22 °C).\nMesoscale: 0.0401 °C² → ~20% (SD ≈ 0.20 °C).\n\nInterpretation: Most error is explained by sensor noise. The remaining ~44% splits fairly evenly between sub-grid/representativeness (micro) and larger-scale unmodeled structure (meso). Both small-scale heterogeneity and some broader process signal are still in play.\n\n\nT05 (pre-dawn)\n\nTotal variance: 0.362 °C² (RMSE ≈ 0.600 °C).\nInstrument: 0.185 °C² → ~51% of total (SD ≈ 0.43 °C).\nMicroscale: ≈ 0.\nMesoscale: 0.177 °C² → ~49% (SD ≈ 0.42 °C).\n\nInterpretation: About half the error is sensor noise. Nearly all of the leftover is mesoscale, i.e., smoother, broader structure that the mean/residual model hasn’t captured (e.g., nocturnal drainage patterns, advection, or missing covariates). A near-zero micro slice is plausible at night when fields are smoother at the station spacing.\n\n\nNet takeaway\n\nT14: Sensor noise dominates; the rest is a balanced mix of micro and meso → both better micro-scale features (or finer R*) and some added process covariates could help.\nT05: Sensor ~half; remainder is meso-dominated → focus on process/scale (e.g., flow-aligned/anisotropic metrics, cost distance, additional nocturnal drivers).",
    "crumbs": [
      "Block 4: Microclimate Data Retrieval & Spatial Data Interpolation",
      "PipeModel Idealized valley microclimate sandbox"
    ]
  },
  {
    "objectID": "block2_bas_dob/lidar_forest_structure.html",
    "href": "block2_bas_dob/lidar_forest_structure.html",
    "title": "Processing of 3D Lidar data to assess forest structure",
    "section": "",
    "text": "check out the manual of the lidR package: https://r-lidar.github.io/lidRbook/",
    "crumbs": [
      "Block 2: Processing of 3D LiDAR data to assess forest structure",
      "Processing of 3D Lidar data to assess forest structure"
    ]
  },
  {
    "objectID": "block2_bas_dob/lidar_forest_structure.html#setup",
    "href": "block2_bas_dob/lidar_forest_structure.html#setup",
    "title": "Processing of 3D Lidar data to assess forest structure",
    "section": "0. Setup",
    "text": "0. Setup\n\nInstall and load required libraries\n\n## install libraries\n# install.packges(\"lidR\")\n# install.packages(\"terra\")\n# install.packages(\"sf\")\n# install.packages(\"rTwig\")\n# install.packages('lidRviewer', repos = c('https://r-lidar.r-universe.dev'))\n\n\nlibrary(lidR)\nlibrary(terra)\nlibrary(sf)\nlibrary(lidRviewer)\nlibrary(dplyr)\nlibrary(rTwig)\n\n\n\ndownload the data\ndownload lidar point cloud and ground truth data (BI) from owncloud.\n\n# Data access\nurl_las &lt;- \"https://cloud.hawk.de/index.php/s/pB4RRmLb4Xxy4Qj/download\"\ndownload.file(url_las, destfile = \"uls_goewa.laz\", mode = \"wb\")\n\nurl_bi &lt;- \"https://cloud.hawk.de/index.php/s/5npprfZYLjg5ip5/download\"\ndownload.file(url_bi, destfile = \"trees_bi.gpkg\", mode = \"wb\")\n\n\n\nimport the data\n\nlas &lt;- readLAS(\"uls_goewa.laz\")\ntrees_bi &lt;- st_read(\"trees_bi.gpkg\")\n\nlet´s inspect the data. 1) whats the point density of the lidar data? 2) whats the total number of points and pulses and what is the difference between the two? 3) is there any classification in the point cloud? 4) how many trees were measured in the BI? 5) which tree species are present in the plot?\n\nprint(las)\n\nclass        : LAS (v1.2 format 3)\nmemory       : 313.2 Mb \nextent       : 572445.4, 572496.1, 5709020, 5709071 (xmin, xmax, ymin, ymax)\ncoord. ref.  : WGS 84 / UTM zone 32N \narea         : 2602 m²\npoints       : 5.13 million points\ntype         : terrestrial\ndensity      : 1971.94 points/m²\ndensity      : 1660.17 pulses/m²\n\nplot(las)\nprint(trees_bi)\n\nSimple feature collection with 69 features and 17 fields\nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: 572446.5 ymin: 5709021 xmax: 572495.8 ymax: 5709069\nProjected CRS: WGS 84 / UTM zone 32N\nFirst 10 features:\n   IDPlots          Name Plots.Bem ID      X_m     Y_m    Z_m Species Spec_txt\n1        1 GoeWald Fla 1      &lt;NA&gt;  1 572468.6 5709043 -0.448      22       Bu\n2        1 GoeWald Fla 1      &lt;NA&gt;  2 572467.4 5709044 -0.466      22       Bu\n3        1 GoeWald Fla 1      &lt;NA&gt;  3 572462.7 5709041 -0.443      22       Bu\n4        1 GoeWald Fla 1      &lt;NA&gt;  4 572459.8 5709039 -0.657      22       Bu\n5        1 GoeWald Fla 1      &lt;NA&gt;  5 572455.0 5709045 -0.971      22       Bu\n6        1 GoeWald Fla 1      &lt;NA&gt;  6 572455.1 5709048 -0.978      22       Bu\n7        1 GoeWald Fla 1      &lt;NA&gt;  7 572459.6 5709050 -0.723      22       Bu\n8        1 GoeWald Fla 1      &lt;NA&gt;  8 572460.5 5709052 -0.920      22       Bu\n9        1 GoeWald Fla 1      &lt;NA&gt;  9 572463.9 5709053 -0.747      22       Bu\n10       1 GoeWald Fla 1      &lt;NA&gt; 10 572468.1 5709048 -0.619      22       Bu\n   DBH_mm Vit Bruch                Schirm Schiefer.B Trees.Bem   x_lok  y_lok\n1     354 leb  nein geringe �berschirmung       Nein      &lt;NA&gt;  -2.197 -2.543\n2     345 leb  nein geringe �berschirmung       Nein      &lt;NA&gt;  -3.371 -1.518\n3     518 leb  nein geringe �berschirmung       Nein      &lt;NA&gt;  -8.087 -3.932\n4     373 leb  nein    Hohe �berschirmung       Nein      &lt;NA&gt; -11.067 -6.176\n5     350 leb  nein    Hohe �berschirmung       Nein      &lt;NA&gt; -15.776 -0.631\n6     388 leb  nein geringe �berschirmung       Nein      &lt;NA&gt; -15.576  2.703\n7     264 tot    ja                  &lt;NA&gt;       Nein      &lt;NA&gt; -11.070  4.367\n8     404 leb  nein geringe �berschirmung       Nein      &lt;NA&gt; -10.178  6.296\n9     464 leb  nein geringe �berschirmung       Nein      &lt;NA&gt;  -6.741  7.432\n10    291 leb    ja  Komplett �berschirmt       Nein      &lt;NA&gt;  -2.589  2.832\n                       geom\n1  POINT (572468.6 5709043)\n2  POINT (572467.4 5709044)\n3  POINT (572462.7 5709041)\n4  POINT (572459.8 5709039)\n5    POINT (572455 5709045)\n6  POINT (572455.1 5709048)\n7  POINT (572459.6 5709050)\n8  POINT (572460.5 5709052)\n9  POINT (572463.9 5709053)\n10 POINT (572468.1 5709048)",
    "crumbs": [
      "Block 2: Processing of 3D LiDAR data to assess forest structure",
      "Processing of 3D Lidar data to assess forest structure"
    ]
  },
  {
    "objectID": "block2_bas_dob/lidar_forest_structure.html#calculating-terrain-models",
    "href": "block2_bas_dob/lidar_forest_structure.html#calculating-terrain-models",
    "title": "Processing of 3D Lidar data to assess forest structure",
    "section": "1. Calculating Terrain Models",
    "text": "1. Calculating Terrain Models\nNext we are calculating terrain models, using triangulation (TIN = Triangulated Irregular Network). Since the data is already ground classified we can skip the classification step. The DEM is using the ground return points to interpolate the surface. In contrast, the digital surface model is using the highest lidar returns to represent the top of any object above the ground. Subtracting the two gives us the canopy height.\ncheck out the documentation of the rasterize_terrain algorithm. Try out different interpolation algorithms and different resolutions. Compare the results visually.\n\ndem &lt;- rasterize_terrain(las, res = 0.5, algorithm = tin())\ndsm &lt;- rasterize_canopy(las, res = 0.5, algorithm = dsmtin(max_edge = 8))\nchm &lt;- dsm - dem\nchm &lt;- terra::focal(chm, w = 3, fun = mean, na.rm = TRUE) # smoothing results \n\npar(mfrow = c(1,3))\nplot(dem, main = \"digital elevation model\")\nplot(dsm, main = \"digital surface model\")\nplot(chm, main = \"canopy heigt model\")\n\n\n\n\n\n\n\n#writeRaster(dem, \"./data/output/dem.tif\", overwrite=TRUE)\n#writeRaster(dsm, \"./data/output/dsm.tif\", overwrite=TRUE)\n#writeRaster(chm, \"./data/output/chm.tif\", overwrite=TRUE)",
    "crumbs": [
      "Block 2: Processing of 3D LiDAR data to assess forest structure",
      "Processing of 3D Lidar data to assess forest structure"
    ]
  },
  {
    "objectID": "block2_bas_dob/lidar_forest_structure.html#individual-tree-detection",
    "href": "block2_bas_dob/lidar_forest_structure.html#individual-tree-detection",
    "title": "Processing of 3D Lidar data to assess forest structure",
    "section": "2. Individual Tree Detection",
    "text": "2. Individual Tree Detection\nIndividual Tree Detection (ITD) is the process of spatially locating trees (f.i to extract height information). Tree tops can be detected by applying a Local Maximum Filter (LMF) on the loaded data set. The number of detected trees is correlated to the window size (ws) argument. Small windows sizes usually gives more trees, while large windows size generally miss smaller trees that are “hidden” by big trees that contain the highest points in the neighbourhood. We will use the Tree detection function with variable window size. Any points below 2 m will equate to a window size of 3 m, while points above 20 meters equate to a window size of 5 m. Anything between 2 and 20 meter will have a non-linear relationship.\n\n## Function for Local Maximum Filter with variable windows size\n\nf &lt;- function(x) {\n  y &lt;- 2.6 * (-(exp(-0.08*(x-2)) - 1)) + 3 \n  # from https://r-lidar.github.io/lidRbook/itd.html\n  y[x &lt; 2] &lt;- 3\n  y[x &gt; 20] &lt;- 5\n  return(y)\n}\n\nheights &lt;- seq(-5,35,0.5)\nws &lt;- f(heights)\n\nplot(heights, ws, type = \"l\",  ylim = c(0,5))\n\n\n\n\n\n\n\n\nLet´s run the tree detection algorithm using the user-defined ws function and the CHM created beforehand. Compare the results with the ground truth BI data. - How many of the trees could be detected in the lidar data?\n- what could be the reasons for that?\n\n#ttops &lt;- locate_trees(las, lmf(f)) # only run this if you have a fast computer! \nttops &lt;- locate_trees(chm, lmf(f)) \n\n# plot results \nplot(chm, col = height.colors(50))\nplot(sf::st_geometry(trees_bi), add = TRUE, pch = 2, col =\"blue\")\nplot(sf::st_geometry(ttops), add = TRUE, pch = 3, col = \"black\")\n\n\n\n\n\n\n\n# 3D plot\nlas_norm &lt;- normalize_height(las, knnidw()) # normalize point cloud for this vizualisation \n\nInverse distance weighting: [==================================----------------] 68% (6 threads)\nInverse distance weighting: [==================================----------------] 69% (6 threads)\nInverse distance weighting: [===================================---------------] 70% (6 threads)\nInverse distance weighting: [===================================---------------] 71% (6 threads)\nInverse distance weighting: [====================================--------------] 72% (6 threads)\nInverse distance weighting: [====================================--------------] 73% (6 threads)\nInverse distance weighting: [=====================================-------------] 74% (6 threads)\nInverse distance weighting: [=====================================-------------] 75% (6 threads)\nInverse distance weighting: [======================================------------] 76% (6 threads)\nInverse distance weighting: [======================================------------] 77% (6 threads)\nInverse distance weighting: [=======================================-----------] 78% (6 threads)\nInverse distance weighting: [=======================================-----------] 79% (6 threads)\nInverse distance weighting: [========================================----------] 80% (6 threads)\nInverse distance weighting: [========================================----------] 81% (6 threads)\nInverse distance weighting: [=========================================---------] 82% (6 threads)\nInverse distance weighting: [=========================================---------] 83% (6 threads)\nInverse distance weighting: [==========================================--------] 84% (6 threads)\nInverse distance weighting: [==========================================--------] 85% (6 threads)\nInverse distance weighting: [===========================================-------] 86% (6 threads)\nInverse distance weighting: [===========================================-------] 87% (6 threads)\nInverse distance weighting: [============================================------] 88% (6 threads)\nInverse distance weighting: [============================================------] 89% (6 threads)\nInverse distance weighting: [=============================================-----] 90% (6 threads)\nInverse distance weighting: [=============================================-----] 91% (6 threads)\nInverse distance weighting: [==============================================----] 92% (6 threads)\nInverse distance weighting: [==============================================----] 93% (6 threads)\nInverse distance weighting: [===============================================---] 94% (6 threads)\nInverse distance weighting: [===============================================---] 95% (6 threads)\nInverse distance weighting: [================================================--] 96% (6 threads)\nInverse distance weighting: [================================================--] 97% (6 threads)\nInverse distance weighting: [=================================================-] 98% (6 threads)\nInverse distance weighting: [=================================================-] 99% (6 threads)\nInverse distance weighting: [==================================================] 100% (6 threads)\n\nx &lt;- plot(las_norm, bg = \"white\", size = 4)\nadd_treetops3d(x, ttops)\n\n#writeVector(vect(ttops), \"./data/output/ttops_chm_.gpkg\", overwrite=TRUE)",
    "crumbs": [
      "Block 2: Processing of 3D LiDAR data to assess forest structure",
      "Processing of 3D Lidar data to assess forest structure"
    ]
  },
  {
    "objectID": "block2_bas_dob/lidar_forest_structure.html#individual-tree-segmentation",
    "href": "block2_bas_dob/lidar_forest_structure.html#individual-tree-segmentation",
    "title": "Processing of 3D Lidar data to assess forest structure",
    "section": "3. Individual Tree Segmentation",
    "text": "3. Individual Tree Segmentation\nIndividual Tree Segmentation (ITS) is the process of individually delineating detected trees. Even when the algorithm is raster-based (which is the case of dalponte2016()), lidR segments the point cloud and assigns an ID to each point by inserting a new attribute named treeID in the LAS object. This means that every point is associated with a particular tree.\n\nalgo &lt;- dalponte2016(chm, ttops)\nlas_seg &lt;- segment_trees(las_norm, algo) # segment point cloud\nx &lt;- plot(las_seg, bg = \"white\", size = 4, color = \"treeID\") # visualize trees\nadd_treetops3d(x, ttops)",
    "crumbs": [
      "Block 2: Processing of 3D LiDAR data to assess forest structure",
      "Processing of 3D Lidar data to assess forest structure"
    ]
  },
  {
    "objectID": "block2_bas_dob/lidar_forest_structure.html#deriving-metrics-using-the-area-based-approach",
    "href": "block2_bas_dob/lidar_forest_structure.html#deriving-metrics-using-the-area-based-approach",
    "title": "Processing of 3D Lidar data to assess forest structure",
    "section": "4. Deriving Metrics using the area-based approach",
    "text": "4. Deriving Metrics using the area-based approach\nthe Area-Based Approach (ABA) allows the creation of wall-to-wall predictions of forest inventory attributes (e.g. basal area or total volume per hectare) by linking ALS variables with field measured references.\n\nr_metr &lt;- pixel_metrics(las, res = 0.5, func = .stdmetrics)\nplot(r_metr)",
    "crumbs": [
      "Block 2: Processing of 3D LiDAR data to assess forest structure",
      "Processing of 3D Lidar data to assess forest structure"
    ]
  },
  {
    "objectID": "block2_bas_dob/lidar_forest_structure.html#forest-structural-complexity",
    "href": "block2_bas_dob/lidar_forest_structure.html#forest-structural-complexity",
    "title": "Processing of 3D Lidar data to assess forest structure",
    "section": "5.Forest structural complexity",
    "text": "5.Forest structural complexity\n(Fractal complexity analysis/ voxel-based box-count dimension or box dimension (Db) method)\nThe box dimension quantifies structural complexity of point clouds using a fractal box-counting approach.\nIt is defined as the slope of the regression between log box (voxel) count and log inverse box (voxel) size, with higher R² values indicating stronger self-similarity. Reliable estimates require high-resolution (≤1 cm) point clouds with minimal occlusion.\n\n# Read data, check and pre-process with lidR\n#data &lt;- readLAS(\"uls_goewa.laz\")\nprint(las)\n\nclass        : LAS (v1.2 format 3)\nmemory       : 313.2 Mb \nextent       : 572445.4, 572496.1, 5709020, 5709071 (xmin, xmax, ymin, ymax)\ncoord. ref.  : WGS 84 / UTM zone 32N \narea         : 2602 m²\npoints       : 5.13 million points\ntype         : terrestrial\ndensity      : 1971.94 points/m²\ndensity      : 1660.17 pulses/m²\n\nlas_check(las) \n\n\n Checking the data\n  - Checking coordinates...\u001b[0;32m ✓\u001b[0m\n  - Checking coordinates type...\u001b[0;32m ✓\u001b[0m\n  - Checking coordinates range...\u001b[0;32m ✓\u001b[0m\n  - Checking coordinates quantization...\u001b[0;32m ✓\u001b[0m\n  - Checking attributes type...\u001b[0;32m ✓\u001b[0m\n  - Checking ReturnNumber validity...\u001b[0;32m ✓\u001b[0m\n  - Checking NumberOfReturns validity...\u001b[0;32m ✓\u001b[0m\n  - Checking ReturnNumber vs. NumberOfReturns...\u001b[0;32m ✓\u001b[0m\n  - Checking RGB validity...\u001b[0;32m ✓\u001b[0m\n  - Checking absence of NAs...\u001b[0;32m ✓\u001b[0m\n  - Checking duplicated points...\u001b[0;32m ✓\u001b[0m\n  - Checking degenerated ground points...\u001b[0;32m ✓\u001b[0m\n  - Checking attribute population...\n \u001b[0;32m   🛈 'PointSourceID' attribute is not populated\u001b[0m\n \u001b[0;32m   🛈 'ScanDirectionFlag' attribute is not populated\u001b[0m\n \u001b[0;32m   🛈 'EdgeOfFlightline' attribute is not populated\u001b[0m\n  - Checking gpstime incoherances\u001b[0;32m ✓\u001b[0m\n  - Checking flag attributes...\u001b[0;32m ✓\u001b[0m\n  - Checking user data attribute...\u001b[0;32m ✓\u001b[0m\n Checking the header\n  - Checking header completeness...\u001b[0;32m ✓\u001b[0m\n  - Checking scale factor validity...\u001b[0;32m ✓\u001b[0m\n  - Checking point data format ID validity...\u001b[0;32m ✓\u001b[0m\n  - Checking extra bytes attributes validity...\u001b[0;32m ✓\u001b[0m\n  - Checking the bounding box validity...\u001b[0;32m ✓\u001b[0m\n  - Checking coordinate reference system...\u001b[0;32m ✓\u001b[0m\n Checking header vs data adequacy\n  - Checking attributes vs. point format...\u001b[0;32m ✓\u001b[0m\n  - Checking header bbox vs. actual content...\u001b[0;32m ✓\u001b[0m\n  - Checking header number of points vs. actual content...\u001b[0;32m ✓\u001b[0m\n  - Checking header return number vs. actual content...\u001b[0;32m ✓\u001b[0m\n Checking coordinate reference system...\n  - Checking if the CRS was understood by R...\u001b[0;32m ✓\u001b[0m\n Checking preprocessing already done \n  - Checking ground classification...\u001b[0;32m yes\u001b[0m\n  - Checking normalization...\u001b[0;31m no\u001b[0m\n  - Checking negative outliers...\u001b[0;32m ✓\u001b[0m\n  - Checking flightline classification...\u001b[0;31m no\u001b[0m\n Checking compression\n  - Checking attribute compression...\n   -  ScanDirectionFlag is compressed\n   -  EdgeOfFlightline is compressed\n   -  Synthetic_flag is compressed\n   -  Keypoint_flag is compressed\n   -  Withheld_flag is compressed\n   -  UserData is compressed\n   -  PointSourceID is compressed\n\nlas_norm &lt;- normalize_height(las = las, \n                        algorithm = tin(), \n                        use_class = 2)\n\nlas_check(las_norm) # check negative outliers\n\n\n Checking the data\n  - Checking coordinates...\u001b[0;32m ✓\u001b[0m\n  - Checking coordinates type...\u001b[0;32m ✓\u001b[0m\n  - Checking coordinates range...\u001b[0;32m ✓\u001b[0m\n  - Checking coordinates quantization...\u001b[0;32m ✓\u001b[0m\n  - Checking attributes type...\u001b[0;32m ✓\u001b[0m\n  - Checking ReturnNumber validity...\u001b[0;32m ✓\u001b[0m\n  - Checking NumberOfReturns validity...\u001b[0;32m ✓\u001b[0m\n  - Checking ReturnNumber vs. NumberOfReturns...\u001b[0;32m ✓\u001b[0m\n  - Checking RGB validity...\u001b[0;32m ✓\u001b[0m\n  - Checking absence of NAs...\u001b[0;32m ✓\u001b[0m\n  - Checking duplicated points...\u001b[0;32m ✓\u001b[0m\n  - Checking degenerated ground points...\u001b[0;32m ✓\u001b[0m\n  - Checking attribute population...\n \u001b[0;32m   🛈 'PointSourceID' attribute is not populated\u001b[0m\n \u001b[0;32m   🛈 'ScanDirectionFlag' attribute is not populated\u001b[0m\n \u001b[0;32m   🛈 'EdgeOfFlightline' attribute is not populated\u001b[0m\n  - Checking gpstime incoherances\u001b[0;32m ✓\u001b[0m\n  - Checking flag attributes...\u001b[0;32m ✓\u001b[0m\n  - Checking user data attribute...\u001b[0;32m ✓\u001b[0m\n Checking the header\n  - Checking header completeness...\u001b[0;32m ✓\u001b[0m\n  - Checking scale factor validity...\u001b[0;32m ✓\u001b[0m\n  - Checking point data format ID validity...\u001b[0;32m ✓\u001b[0m\n  - Checking extra bytes attributes validity...\u001b[0;32m ✓\u001b[0m\n  - Checking the bounding box validity...\u001b[0;32m ✓\u001b[0m\n  - Checking coordinate reference system...\u001b[0;32m ✓\u001b[0m\n Checking header vs data adequacy\n  - Checking attributes vs. point format...\u001b[0;32m ✓\u001b[0m\n  - Checking header bbox vs. actual content...\u001b[0;32m ✓\u001b[0m\n  - Checking header number of points vs. actual content...\u001b[0;32m ✓\u001b[0m\n  - Checking header return number vs. actual content...\u001b[0;32m ✓\u001b[0m\n Checking coordinate reference system...\n  - Checking if the CRS was understood by R...\u001b[0;32m ✓\u001b[0m\n Checking preprocessing already done \n  - Checking ground classification...\u001b[0;32m yes\u001b[0m\n  - Checking normalization...\u001b[0;32m yes\u001b[0m\n  - Checking negative outliers...\n \u001b[1;33m   ⚠ 51 points below 0\u001b[0m\n  - Checking flightline classification...\u001b[0;31m no\u001b[0m\n Checking compression\n  - Checking attribute compression...\n   -  ScanDirectionFlag is compressed\n   -  EdgeOfFlightline is compressed\n   -  Synthetic_flag is compressed\n   -  Keypoint_flag is compressed\n   -  Withheld_flag is compressed\n   -  UserData is compressed\n   -  PointSourceID is compressed\n\nview(las_norm)\n            #Rotate with left mouse button\n            #Zoom with mouse wheel\n            #Pan with right mouse button\n            #Keyboard r or g or b to color with RGB\n            #Keyboard z to color with Z\n            #Keyboard i to color with Intensity\n            #Keyboard c to color with Classification\n            #Keyboard + or - to change the point size\n            #Keyboard l to enable/disable eyes-dome lightning\n\n\nlas_norm@data[Z&lt;0, 1:3] # Here options are either remove all or assign all to 0, However...\n\n           X       Y       Z\n       &lt;num&gt;   &lt;num&gt;   &lt;num&gt;\n 1: 572471.7 5709038 -0.0183\n 2: 572469.0 5709021 -0.0044\n 3: 572475.5 5709041 -0.0315\n 4: 572477.3 5709041 -0.0004\n 5: 572475.2 5709040 -0.0013\n 6: 572476.9 5709041 -0.0088\n 7: 572459.1 5709064 -0.0035\n 8: 572477.3 5709041 -0.0041\n 9: 572460.3 5709058 -0.0034\n10: 572458.5 5709062 -0.0015\n11: 572462.5 5709046 -0.1952\n12: 572480.8 5709064 -0.0056\n13: 572483.4 5709069 -0.0163\n14: 572483.8 5709068 -0.0514\n15: 572483.8 5709068 -0.0330\n16: 572483.4 5709068 -0.0129\n17: 572488.9 5709023 -0.0133\n18: 572476.2 5709047 -0.0031\n19: 572451.9 5709020 -0.0029\n20: 572483.6 5709068 -0.0488\n21: 572484.9 5709062 -0.0040\n22: 572452.1 5709020 -0.0102\n23: 572452.0 5709021 -0.0030\n24: 572488.8 5709054 -0.0037\n25: 572476.1 5709032 -0.0070\n26: 572452.2 5709020 -0.0227\n27: 572491.1 5709069 -0.0163\n28: 572488.9 5709023 -0.0496\n29: 572483.9 5709068 -0.0246\n30: 572483.4 5709068 -0.0099\n31: 572484.7 5709063 -0.0017\n32: 572477.9 5709064 -0.0101\n33: 572478.2 5709065 -0.0129\n34: 572482.9 5709060 -0.0112\n35: 572494.0 5709053 -0.0001\n36: 572491.3 5709055 -0.0017\n37: 572476.1 5709047 -0.0046\n38: 572475.6 5709050 -0.0004\n39: 572491.0 5709039 -0.0019\n40: 572473.3 5709046 -0.0006\n41: 572478.1 5709039 -0.0100\n42: 572493.1 5709032 -0.0122\n43: 572470.5 5709037 -0.0157\n44: 572490.8 5709032 -0.0080\n45: 572477.5 5709041 -0.0002\n46: 572474.1 5709035 -0.0131\n47: 572493.0 5709032 -0.0166\n48: 572477.7 5709040 -0.0088\n49: 572476.5 5709052 -0.0075\n50: 572473.1 5709038 -0.0014\n51: 572474.0 5709035 -0.0297\n           X       Y       Z\n\n# Forest structural complexity (Box dimension)\n\ncloud = las_norm@data[Z&gt;0.5, 1:3] # Here, all points above 0.5 meter and only X,Y,z coordinates \n\ndb &lt;- box_dimension(cloud = cloud, \n                    lowercutoff = 0.01, \n                    rm_int_box = FALSE, \n                    plot = FALSE )\nstr(db)\n\nList of 2\n $ :Classes 'tidytable', 'tbl', 'data.table' and 'data.frame':  13 obs. of  2 variables:\n  ..$ log.box.size: num [1:13] 0 0.693 1.386 2.079 2.773 ...\n  ..$ log.voxels  : num [1:13] 1.39 2.89 4.32 6.04 7.56 ...\n  ..- attr(*, \".internal.selfref\")=&lt;externalptr&gt; \n $ :Classes 'tidytable', 'tbl', 'data.table' and 'data.frame':  1 obs. of  4 variables:\n  ..$ r.squared    : num 0.964\n  ..$ adj.r.squared: num 0.96\n  ..$ intercept    : num 2.24\n  ..$ slope        : num 1.84\n\n# Box Dimension (slope)\ndb[[2]]$slope\n\n[1] 1.838747\n\ndb[[2]]$r.squared # show similarity\n\n[1] 0.9636752\n\n\n\n# Visualization\n# 2D Plot\nbox_dimension(cloud[, 1:3], plot = \"2D\")\n\n\n\n\n\n\n\n\n[[1]]\n# A tidytable: 13 × 2\n   log.box.size log.voxels\n          &lt;dbl&gt;      &lt;dbl&gt;\n 1        0           1.39\n 2        0.693       2.89\n 3        1.39        4.32\n 4        2.08        6.04\n 5        2.77        7.56\n 6        3.47        9.11\n 7        4.16       10.6 \n 8        4.85       12.1 \n 9        5.55       13.6 \n10        6.24       14.8 \n11        6.93       15.3 \n12        7.62       15.4 \n13        8.32       15.4 \n\n[[2]]\n# A tidytable: 1 × 4\n  r.squared adj.r.squared intercept slope\n      &lt;dbl&gt;         &lt;dbl&gt;     &lt;dbl&gt; &lt;dbl&gt;\n1     0.964         0.960      2.24  1.84\n\n# 3D Plot\nbox_dimension(cloud[, 1:3], plot = \"3D\")\n\nPanning plot on rgl device: 4\n\n\n[[1]]\n# A tidytable: 13 × 2\n   log.box.size log.voxels\n          &lt;dbl&gt;      &lt;dbl&gt;\n 1        0           1.39\n 2        0.693       2.89\n 3        1.39        4.32\n 4        2.08        6.04\n 5        2.77        7.56\n 6        3.47        9.11\n 7        4.16       10.6 \n 8        4.85       12.1 \n 9        5.55       13.6 \n10        6.24       14.8 \n11        6.93       15.3 \n12        7.62       15.4 \n13        8.32       15.4 \n\n[[2]]\n# A tidytable: 1 × 4\n  r.squared adj.r.squared intercept slope\n      &lt;dbl&gt;         &lt;dbl&gt;     &lt;dbl&gt; &lt;dbl&gt;\n1     0.964         0.960      2.24  1.84",
    "crumbs": [
      "Block 2: Processing of 3D LiDAR data to assess forest structure",
      "Processing of 3D Lidar data to assess forest structure"
    ]
  },
  {
    "objectID": "block1_magdon/01_TrainingDataCollection.html",
    "href": "block1_magdon/01_TrainingDataCollection.html",
    "title": "Collection of training data for remote sensing model building",
    "section": "",
    "text": "#install.packages(\"devtools\")\n#devtools::install_github(\"bleutner/RStoolbox\")\nlibrary(sf)\n\nLinking to GEOS 3.13.1, GDAL 3.11.0, PROJ 9.6.0; sf_use_s2() is TRUE\n\nlibrary(RStoolbox)\n\nThis is version 1.0.2.1 of RStoolbox\n\nlibrary(terra)\n\nterra 1.8.60\n\nlibrary(ggplot2)\nlibrary(mapview)\nlibrary(kableExtra)\nlibrary(dplyr)\n\n\nAttache Paket: 'dplyr'\n\n\nDas folgende Objekt ist maskiert 'package:kableExtra':\n\n    group_rows\n\n\nDie folgenden Objekte sind maskiert von 'package:terra':\n\n    intersect, union\n\n\nDie folgenden Objekte sind maskiert von 'package:stats':\n\n    filter, lag\n\n\nDie folgenden Objekte sind maskiert von 'package:base':\n\n    intersect, setdiff, setequal, union\n\nlibrary(rprojroot)\nlibrary(patchwork)\n\n\nAttache Paket: 'patchwork'\n\n\nDas folgende Objekt ist maskiert 'package:terra':\n\n    area\n\nlibrary(rmarkdown)\nlibrary(tidyr)\n\n\nAttache Paket: 'tidyr'\n\n\nDas folgende Objekt ist maskiert 'package:terra':\n\n    extract\n\nlibrary(tibble)",
    "crumbs": [
      "Block 1: Reference Data Collection",
      "Collection of training data for remote sensing modelling based on spectral variability"
    ]
  },
  {
    "objectID": "block1_magdon/01_TrainingDataCollection.html#dimension-reduction-pca",
    "href": "block1_magdon/01_TrainingDataCollection.html#dimension-reduction-pca",
    "title": "Collection of training data for remote sensing model building",
    "section": "Dimension reduction (PCA)",
    "text": "Dimension reduction (PCA)\nIn a fist step we reduce the dimensions of the 9 Sentinel-2 bands while maintaining most of the information, using a principal component analysis (PCA).\n\n# Calculation of the principlal components using the RStoolbox\npca&lt;-RStoolbox::rasterPCA(s2,nSamples = 5000, spca=TRUE )\n\n\n|---------|---------|---------|---------|\n=========================================\n                                          \n\n# Extracting the first three components\nrgb_raster &lt;- subset(pca$map, 1:3)\n\n# Function to scale the pixel values to 0-255\nscale_fun &lt;- function(x) {\n  # Calculation of the 2% and 98% quantile\n  q &lt;- quantile(x, c(0.02, 0.98), na.rm = TRUE)\n  \n  # scaling the values\n  x &lt;- (x - q[1]) / (q[2] - q[1]) * 255\n  \n  # restrict the values to 0-255\n  x &lt;- pmin(pmax(x, 0), 255)\n  \n  return(x)\n}\n\n# Scaling of each band\nfor (i in 1:3) {\n  rgb_raster[[i]] &lt;- app(rgb_raster[[i]], scale_fun)\n}\n\n# Plot the first three principal components as RGB\nplotRGB(rgb_raster, r = 1, g = 2, b = 3)\n\n\n\n\n\n\n\n# Show importance of componentes\nsummary(pca$model)\n\nImportance of components:\n                          Comp.1    Comp.2    Comp.3     Comp.4     Comp.5\nStandard deviation     2.2346500 1.8079296 0.6737201 0.38366893 0.26048690\nProportion of Variance 0.5548512 0.3631788 0.0504332 0.01635576 0.00753927\nCumulative Proportion  0.5548512 0.9180300 0.9684632 0.98481898 0.99235825\n                            Comp.6      Comp.7       Comp.8       Comp.9\nStandard deviation     0.177485712 0.162607669 0.0812795411 0.0650151820\nProportion of Variance 0.003500131 0.002937917 0.0007340404 0.0004696638\nCumulative Proportion  0.995858379 0.998796296 0.9995303362 1.0000000000\n\n\nFrom the output of the PCA we see that we can capture 92% of the variability with the first two components. Thus we will only use the PC1 and PC2 for the subsequent analysis.",
    "crumbs": [
      "Block 1: Reference Data Collection",
      "Collection of training data for remote sensing modelling based on spectral variability"
    ]
  },
  {
    "objectID": "block1_magdon/01_TrainingDataCollection.html#unsupervised-clustering",
    "href": "block1_magdon/01_TrainingDataCollection.html#unsupervised-clustering",
    "title": "Collection of training data for remote sensing model building",
    "section": "Unsupervised clustering",
    "text": "Unsupervised clustering\nIn the next step we run an unsupervised classification of the PC1 and PC2 to get a clustered map. For the unsupervised classification we need to take a decision on the number of classes/clusters to be created. Here we will take \\(n=5\\) classes. However, depending on the target variable this value need to be adjusted.\n\nset.seed(2222)\ncluster &lt;- RStoolbox::unsuperClass(pca$map[[c('PC1','PC2')]], nSamples = 100, nClasses = 5, nStarts = 5)\n\n\n## Plots\ncolors &lt;- rainbow(5)\nplot(cluster$map, col = colors, legend = TRUE, axes = TRUE, box =TRUE)\n\n\n\n\n\n\n\n\nThe map shows a clear spatial patterns related to the elevation, tree species and vitality status of the National Park forests.",
    "crumbs": [
      "Block 1: Reference Data Collection",
      "Collection of training data for remote sensing modelling based on spectral variability"
    ]
  },
  {
    "objectID": "block1_magdon/01_TrainingDataCollection.html#implement-a-plot-design",
    "href": "block1_magdon/01_TrainingDataCollection.html#implement-a-plot-design",
    "title": "Collection of training data for remote sensing model building",
    "section": "Implement a plot design",
    "text": "Implement a plot design\nTo extract pixel values for the sample location we need to define a plot design. For this exercise we will simulate a circular fixed area plot with a radius of 13 m.\n\n# Create a training by extracting the mean value of all pixels touching\n# a buffered area with 13m around the plot center\n\nplots &lt;- sf::st_buffer(sf_samples,dist = 13)\ntrain&lt;-terra::extract(s2,plots,fun='mean',bind=FALSE,na.rm=TRUE)\n\nplots &lt;- plots %&gt;% mutate(ID=row_number())\ntrain &lt;- plots %&gt;% left_join(train, by= \"ID\")\nmapview::mapview(train, zcol=\"class_unsupervised\",\n        map.types = c(\"Esri.WorldShadedRelief\", \"OpenStreetMap.DE\"))+\n  mapview(np_boundary,alpha.regions = 0.2, aplha = 1)",
    "crumbs": [
      "Block 1: Reference Data Collection",
      "Collection of training data for remote sensing modelling based on spectral variability"
    ]
  },
  {
    "objectID": "block1_magdon/02_ValidationDataCollection.html",
    "href": "block1_magdon/02_ValidationDataCollection.html",
    "title": "Collection of validation data in the context of remote sensing based forest monitoring",
    "section": "",
    "text": "rm(list=ls())\nlibrary(sf)\n\nLinking to GEOS 3.13.1, GDAL 3.11.0, PROJ 9.6.0; sf_use_s2() is TRUE\n\nlibrary(terra)\n\nterra 1.8.60\n\nlibrary(ggplot2)\nlibrary(rprojroot)\nlibrary(patchwork)\n\n\nAttache Paket: 'patchwork'\n\n\nDas folgende Objekt ist maskiert 'package:terra':\n\n    area\n\ndata_dir=paste0(find_rstudio_root_file(),\"/block1_magdon/data/\")",
    "crumbs": [
      "Block 1: Reference Data Collection",
      "Validation of continuous maps using design-based sampling methods"
    ]
  },
  {
    "objectID": "block1_magdon/02_ValidationDataCollection.html#systematic-sample-to-collect-reference-data-for-map-validation",
    "href": "block1_magdon/02_ValidationDataCollection.html#systematic-sample-to-collect-reference-data-for-map-validation",
    "title": "Collection of validation data in the context of remote sensing based forest monitoring",
    "section": "Systematic sample to collect reference data for map validation",
    "text": "Systematic sample to collect reference data for map validation\nTo validate the map we use a systematic sample grid. In a real world application we do not know the true population values. Therefore, field work would be needed to collect reference data at the selected sample points. In this workshop we assume that the agp_pop map represents the true value without any errors. Thus, we don’t need to go to field but we can sample the data by extracting the true values from the map at the sample locations.\n\n# we will use n=100 sample plots\nn=100\np1 = sf::st_sample(np_boundary,size=n,type='regular')\n\nggplot()+geom_sf(data=np_boundary,fill=NA)+\n  geom_sf(data=p1)\n\n\n\n\n\n\n\n\nAt each sample point we extract the predicted and observed AGB value.\n\nobs &lt;- terra::extract(agb_pop,vect(p1))\nnames(obs)&lt;-c('ID','obs')\n\npred &lt;- terra::extract(agb_model,vect(p1))\nnames(pred)&lt;-c('ID','pred')\nvalidation&lt;-data.frame(observed=obs$obs, predicted=pred$pred)\n\n# we need to remove the na values from this dataframe. In real world applications\n# such NA values can,  occur for example at inaccessible field plots.\n\nvalidation&lt;-validation[complete.cases(validation),]",
    "crumbs": [
      "Block 1: Reference Data Collection",
      "Validation of continuous maps using design-based sampling methods"
    ]
  },
  {
    "objectID": "block1_magdon/02_ValidationDataCollection.html#assessment-of-the-abg-model-performance",
    "href": "block1_magdon/02_ValidationDataCollection.html#assessment-of-the-abg-model-performance",
    "title": "Collection of validation data in the context of remote sensing based forest monitoring",
    "section": "Assessment of the ABG-model performance",
    "text": "Assessment of the ABG-model performance\n\nggplot(data=validation,aes(x=observed, y=predicted))+\n  geom_point(alpha=0.5)+\n  xlab('Observed AGB t/ha')+ylab('Predicted AGB t/ha')\n\n\n\n\n\n\n\n\n\nSample RMSE\nAgain we can use the RMSE to express the mean difference between observed and predicted AGB.\n\nRMSE_sample = sqrt(sum((validation$observed-validation$predicted)^2)/nrow(validation))\n\nThe sample RMSE is 38.89* t/ha. To better compare the values between different target variables and models is can also express as a proportion relative to the mean value of the predictions.\n\nrRMSE = RMSE_sample/mean(validation$predicted)\n\nOn average we expect that the AGB estimate of our model has an error of 24.1 %.\n\n\nError distribution\nBut is this RMSE valid for the entire range of the observed values or do we expect higher errors for higher AGB values?\nTo see how the model performs over target value range we can use the following analysis plots.\n\nvalidation$resid&lt;-validation$observed-validation$predicted\n\np1&lt;-ggplot(data=validation,aes(x=observed, y=predicted))+\n  geom_point(alpha=0.5)+\n  xlab('Observed AGB t/ha')+ylab('Predicted AGB t/ha')+\n  xlim(0,250)+ylim(0,250)+\n  geom_abline(slope=1,intercept = 0)+\n  stat_summary(fun.data= mean_cl_normal) + \n  geom_smooth(method='lm')\n\n\n\np2&lt;-ggplot(data=validation,aes(x=observed, y=resid))+\n  geom_point(alpha=0.5)+\n  xlab('Observed AGB t/ha')+ylab('Residuals')+\n  xlim(0,250)+ylim(-50,+50)+\n  geom_abline(slope=0,intercept = 1)\n\np3&lt;-ggplot(data=validation,aes(x=resid))+\n  geom_histogram(aes(y=..density..),fill='grey',binwidth=10)+\n  xlab('Observed AGB t/ha')+ylab('Density')+\n  xlim(-150,150)+\n  stat_function(fun = dnorm, geom=\"polygon\",args = list(mean = mean(validation$resid), sd = sd(validation$resid)),color='blue',alpha=0.4,fill='blue')+\n  geom_vline(xintercept=0,color='blue')+\n  geom_vline(xintercept=mean(validation$resid),color='red')\np1+p2+p3+plot_layout(ncol=3)\n\nWarning: Removed 1 row containing non-finite outside the scale range\n(`stat_summary()`).\n\n\nWarning: Computation failed in `stat_summary()`.\nCaused by error in `fun.data()`:\n! The package \"Hmisc\" is required.\n\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\nWarning: Removed 1 row containing non-finite outside the scale range\n(`stat_smooth()`).\n\n\nWarning: Removed 1 row containing missing values or values outside the scale range\n(`geom_point()`).\n\n\nWarning: Removed 18 rows containing missing values or values outside the scale range\n(`geom_point()`).\n\n\nWarning: The dot-dot notation (`..density..`) was deprecated in ggplot2 3.4.0.\nℹ Please use `after_stat(density)` instead.\n\n\nWarning: Removed 2 rows containing missing values or values outside the scale range\n(`geom_bar()`).",
    "crumbs": [
      "Block 1: Reference Data Collection",
      "Validation of continuous maps using design-based sampling methods"
    ]
  },
  {
    "objectID": "block4_5/mc_2025_0.html",
    "href": "block4_5/mc_2025_0.html",
    "title": "Spatial Interpolation",
    "section": "",
    "text": "Quantitative spatial methods rely on the dependence structure of the phenomenon. While Euclidean proximity is a common default, process-defined neighborhoods and relevant covariates often explain spatial patterns more effectively, especially for gap-filling.\n\nLets look at our project region.\n\n\n\nPolsterhaus excursion research area stations 2023 (stars) 2024 (crosses)",
    "crumbs": [
      "Block 4: Microclimate Data Retrieval & Spatial Data Interpolation",
      "Spatial Interpolation"
    ]
  },
  {
    "objectID": "block4_5/mc_2025_0.html#why-do-we-want-to-fill-spatial-gaps",
    "href": "block4_5/mc_2025_0.html#why-do-we-want-to-fill-spatial-gaps",
    "title": "Spatial Interpolation",
    "section": "",
    "text": "Quantitative spatial methods rely on the dependence structure of the phenomenon. While Euclidean proximity is a common default, process-defined neighborhoods and relevant covariates often explain spatial patterns more effectively, especially for gap-filling.\n\nLets look at our project region.\n\n\n\nPolsterhaus excursion research area stations 2023 (stars) 2024 (crosses)",
    "crumbs": [
      "Block 4: Microclimate Data Retrieval & Spatial Data Interpolation",
      "Spatial Interpolation"
    ]
  },
  {
    "objectID": "block4_5/mc_2025_0.html#distance-and-data-representation",
    "href": "block4_5/mc_2025_0.html#distance-and-data-representation",
    "title": "Spatial Interpolation",
    "section": "Distance and data representation",
    "text": "Distance and data representation\nLet’s look more closely at proximity. What is it, exactly, and how can we express it so that space becomes analytically meaningful?\nIn general, spatial relationships are described by neighborhoods (positional) and distances (metric). For spatial analysis or prediction, we also need to quantify spatial influence—how strongly one location affects another. Tobler’s First Law of Geography captures one common objective: near things are more related than distant things. In many real cases spatial influence cannot be measured directly and must be estimated.\n\nNeighborhood\nNeighborhood is a foundational concept. Higher‑dimensional geo‑objects (areas) are neighbors if they touch (e.g., adjacent countries). For points (0‑D objects), neighborhood is commonly defined by distance, however choose k and radius to reflect process scale; consider directional or network-constrained neighborhoods (e.g., k nearest neighbors within a search radius).\n\n\nDistance\nProximity analyses often ask about areas of influence or catchments—the spatial footprints of effects or processes.\n\nInfluence kernels. Specify a weight function \\(w(d)\\) (e.g., exponential or Gaussian) or a covariance model \\(C(h)\\). Estimate parameters (range/sill/nugget) from the data; allow anisotropy if effects propagate preferentially (e.g., along slope or wind). Combine process covariates for the mean, plus a spatial residual for remaining structure.\n\nBecause space can be discretized as vector or raster, distance computation differs accordingly. When contextual constraints are unknown, it is useful to begin with a simple metric such as *Euclidean distance and later refine to network distance, travel time, or cost distance** as the question demands. Also consider feature-space proximity (similar covariates) when processes are driven by environment more than location.\nThe key is operationalization: qualitative notions like “near” and “far” must be translated into a distance metric and units (meters, minutes, etc.). There is rarely a single “correct” choice—only measures more or less suitable for the objective—so defining a meaningful neighborhood relation for the objects of interest is critical.",
    "crumbs": [
      "Block 4: Microclimate Data Retrieval & Spatial Data Interpolation",
      "Spatial Interpolation"
    ]
  },
  {
    "objectID": "block4_5/mc_2025_0.html#filling-spatial-gaps---the-concepts-of-doing-it",
    "href": "block4_5/mc_2025_0.html#filling-spatial-gaps---the-concepts-of-doing-it",
    "title": "Spatial Interpolation",
    "section": "Filling spatial gaps - the concepts of doing it",
    "text": "Filling spatial gaps - the concepts of doing it\nWith the basics of distance and neighborhood in place, we can turn to interpolation/prediction of values in space.\nFor decades, deterministic techniques such as nearest neighbor, inverse distance weighting, and spline methods have been popular.\nIn contrast, geostatistical (stochastic) methods like kriging model spatial autocorrelation explicitly. Extensions such as external‑drift kriging and regression kriging combine covariates with the spatial variogram model.\nMore recently, machine learning (ML) approaches (e.g., random forest) have become common for environmental prediction, thanks to their ability to capture non‑linear and complex relationships. In addition spatial structure can be incorporated via coordinates, distance‑based features, spatial cross‑validation, and residual modeling, complementing geostatistical tools rather than replacing them.",
    "crumbs": [
      "Block 4: Microclimate Data Retrieval & Spatial Data Interpolation",
      "Spatial Interpolation"
    ]
  },
  {
    "objectID": "block4_5/mc_2025_0.html#proximity-a-general-framework",
    "href": "block4_5/mc_2025_0.html#proximity-a-general-framework",
    "title": "Spatial Interpolation",
    "section": "Proximity: a general framework",
    "text": "Proximity: a general framework\nTo predict in space we must first define who can influence whom and how strongly:\n\nNeighborhood — which locations are eligible to interact (e.g., k-nearest, radius, directional, network/flow-aligned, feature-space).\nMetric — how separation is measured (Euclidean, anisotropic/geodesic, cost, network time, feature-space, spatio-temporal).\nInfluence — how effect decays with separation (kernel weights or covariance/variogram), with optional anisotropy and barriers.\n\nProcess knowledge enters as:\n\na mean component (covariates encoding mechanisms), and\na spatial residual (autocorrelation left after the mean), which you model with an influence function consistent with the process and its characteristic scales.",
    "crumbs": [
      "Block 4: Microclimate Data Retrieval & Spatial Data Interpolation",
      "Spatial Interpolation"
    ]
  },
  {
    "objectID": "block4_5/mc_2025_0.html#stepwise-integration-of-process-and-scale",
    "href": "block4_5/mc_2025_0.html#stepwise-integration-of-process-and-scale",
    "title": "Spatial Interpolation",
    "section": "Stepwise integration of process and scale",
    "text": "Stepwise integration of process and scale\nStep 0 — Baseline geometry. Start with simple, isotropic neighborhoods and Euclidean distance. Choose grid resolution consistent with observation support. Report a preliminary characteristic scale (search radius, k, or empirical range).\n\nVoronoi polygons — dividing space geometrically\nVoronoi polygons (Thiessen polygons) provide an elementary geometric definition of proximity. They partition space into regions that are closest to each generator point. In 2D, every location within a polygon is nearer to its seed point than to any other.\n\n\n\nThe blue dots are irregularly distributed stations (rain gauges in Switzerland). The overlaid polygons are the corresponding Voronoi cells that define the closest geometric areas (gisma 2021).\n\n\nVoronoi tessellations mirror organizational principles seen in nature (e.g., plant cells) and human geography (e.g., central places per Christaller). Two assumptions usually apply: (1) nothing else is known about the space between sampled locations; and (2) the boundary between two samples is an approximation—a convenient abstraction rather than a physical discontinuity. Within each polygon, the attribute is implicitly treated as homogeneous.\n\nIf we know more about spatial relationships, we can go beyond purely geometric proximity.\n\nStep 1 — Process in the mean. Add covariates that carry mechanism (elevation, radiation/SVF, LAD, land cover, distance to ridge, flow accumulation). This captures broad gradients and reduces bias.\nStep 2 — Scale and direction in the residual. Fit a residual influence structure that matches the process: directional/anisotropic where flow or wind channels influence; cost/geodesic where uphill/roughness penalizes spread; barriers across ridges, walls, or water.\nStep 3 — Nonstationarity where needed. Allow parameters to vary (local variograms, moving windows) or use flexible smoothers to accommodate gradual changes in scale or orientation.\nStep 4 — Validation and applicability. Use spatial/block CV aligned with process (e.g., flow-aligned folds). Check area of applicability in feature space to avoid “near in XY, far in process.” Sensitivity-test metric, kernel, and scale choices.",
    "crumbs": [
      "Block 4: Microclimate Data Retrieval & Spatial Data Interpolation",
      "Spatial Interpolation"
    ]
  },
  {
    "objectID": "block4_5/mc_2025_0.html#what-each-method-encodes-concept-process-scale",
    "href": "block4_5/mc_2025_0.html#what-each-method-encodes-concept-process-scale",
    "title": "Spatial Interpolation",
    "section": "What each method encodes (concept, process, scale)",
    "text": "What each method encodes (concept, process, scale)\nNearest Neighbor (NN)\n\nProximity: Voronoi cells (closest site wins).\nProcess assumption: Piecewise constant; no smoothing.\nScale control: Implicitly set by station spacing.\nUse when: Quick gap diagnostic or very sharp local patchiness.\n\nInverse Distance Weighted (IDW)\n\nProximity: Monotone kernel in Euclidean distance.\nProcess assumption: Isotropic decay; no explicit uncertainty.\nScale control: Power and search radius (higher power = shorter effective range).\nUse when: Simple, transparent weighting with mild smoothing.\n\nKriging (OK)\n\nProximity: Variogram/covariance defines influence.\nProcess assumption: Second-order stationarity of residual; supports anisotropy.\nScale control: Variogram range(s), nugget; directional ranges for anisotropy.\nUse when: You want uncertainty and a principled spatial model.\n\nKriging with External Drift (KED)\n\nProximity: Covariates in the mean + variogram on residuals.\nProcess assumption: Mechanisms captured by covariates; remaining structure is correlated noise.\nScale control: Covariate smoothness sets large-scale trend; residual variogram sets local scale.\nUse when: Strong drivers are known (radiation, elevation, LAD), but local correlation persists.\n\nThin Plate Spline Regression (TPS)\n\nProximity: Smoothness prior via spline kernel; influence decays with distance and penalty.\nProcess assumption: Broad, smooth fields (trend-like behavior).\nScale control: Smoothing parameter (penalty) and basis complexity.\nUse when: Continuous, gently varying surfaces; robust baseline trend.\n\nTriangular Irregular Surface (TIN)\n\nProximity: Local planar facets on the Delaunay triangulation.\nProcess assumption: Locally planar continuity with sharp breaks possible along edges.\nScale control: Triangle density (data spacing) and any enforced smoothing.\nUse when: Elevation-like surfaces or where local planes are appropriate.\n\nGeneralized Additive Model (GAM)\n\nProximity: Encoded through smooth functions of covariates (and optionally s(x,y)).\nProcess assumption: Nonlinear process relationships in the mean; residuals ideally weakly correlated.\nScale control: Basis dimension and penalties per smoother (feature-space scale); add spatial residual modeling if needed.\nUse when: Rich covariates, nonlinear effects, and desire to keep spatial residuals small.\n\nAdd this right after Generalized Additive Model (GAM):\nRandom Forest (RF)\n\nProximity: Encoded indirectly via covariates (you can include s(x,y), distances, neighborhood stats, cost distances, etc. as features). Spatial dependence handled post-hoc via residual mapping if needed.\nProcess assumption: Complex nonlinear relationships and interactions in the mean; no explicit spatial covariance or uncertainty model. Extrapolation beyond training feature space is weak—check area of applicability.\nScale control: Through feature engineering (window sizes, multi-scale covariates), and tree hyperparameters (min node size, mtry, tree depth). Effective spatial scale emerges from the features you provide.\nUse when: Many and rich covariates, strong nonlinearities, and you want robust prediction without specifying a parametric mean. Pair with residual kriging if spatial autocorrelation remains or if you need uncertainty maps.",
    "crumbs": [
      "Block 4: Microclimate Data Retrieval & Spatial Data Interpolation",
      "Spatial Interpolation"
    ]
  },
  {
    "objectID": "block4_5/mc_2025_0.html#minimal-decision-guide",
    "href": "block4_5/mc_2025_0.html#minimal-decision-guide",
    "title": "Spatial Interpolation",
    "section": "Minimal decision guide",
    "text": "Minimal decision guide\n\nOnly positions, need a fast baseline: NN → IDW → TIN (if local planes fit).\nSmooth trend dominates: TPS or GAM (covariates preferred).\nClear drivers + local correlation: KED (covariates) + residual kriging.\nUncertainty mapping required / correlation central: Kriging family (OK/KED).\nDirectional/channelized processes: Use anisotropic/cost-based metrics in the residual stage.\nRich covariates, nonlinear effects: GAM or RF (prefer RF when interactions dominate; add residual kriging if spatial correlation persists).\n\nThe figure below contrasts six interpolation methods for precipitation in Switzerland (plus the Voronoi tessellation for reference).\n\n\n\n\n\n\nBlue dots: station locations; dot size: precipitation (mm). Overlaid polygons: Voronoi cells (gisma 2021). Top‑left: k‑nearest neighbor (k = 3–5); Top‑right: IDW; Middle‑left: automatic kriging (default settings); Middle‑right: thin‑plate spline regression; Bottom‑left: TIN surface; Bottom‑right: generalized additive model (GAM).\n\n\n\nChoosing an interpolation method: scales & processes\nThink in terms of the process you want to capture and the scale at which it operates.\n\nProcess & characteristic scale: What is the expected range/decay length of spatial dependence? Sample spacing should be finer than this scale; choose kernels/variogram ranges to match it and avoid aliasing.\nSupport & resolution: Are observations point‑like or area‑averaged? Align prediction grid (cell size) and smoothing with observation support; be explicit about change‑of‑support.\nStationarity & anisotropy: If trends or directional effects exist (e.g., along valleys/ridges), include drift/covariates, use local models, or fit directional variograms—don’t assume isotropy.\nSampling layout vs gradients: Ensure coverage across major drivers (elevation, aspect, land cover). Space‑filling/stratified designs beat clustered convenience samples.\nSample size vs model complexity: Parameter‑rich models (nested variograms, many ML features) need more data. Keep models as simple as the data allow.\nUncertainty & validation: Use spatial cross‑validation; map predictions and uncertainty (kriging variance, ensemble spread). Report MAE/RMSE at the scale of use.\nScale interactions (MAUP): State analysis scale explicitly; resample/aggregate covariates consistently to avoid scale mismatch.",
    "crumbs": [
      "Block 4: Microclimate Data Retrieval & Spatial Data Interpolation",
      "Spatial Interpolation"
    ]
  },
  {
    "objectID": "block4_5/mc_2025_0.html#handson-our-data",
    "href": "block4_5/mc_2025_0.html#handson-our-data",
    "title": "Spatial Interpolation",
    "section": "Hands‑on: our data",
    "text": "Hands‑on: our data\nDownload the exercises repository (as a ZIP), then unpack it locally. Open the .Rproj in RStudio and start with the provided exercises.",
    "crumbs": [
      "Block 4: Microclimate Data Retrieval & Spatial Data Interpolation",
      "Spatial Interpolation"
    ]
  },
  {
    "objectID": "block4_5/mc_2025_pipemodel.html",
    "href": "block4_5/mc_2025_pipemodel.html",
    "title": "PipeModel Idealized valley microclimate sandbox",
    "section": "",
    "text": "The PipeModel is a deliberately idealized yet physically plausible valley scenario. It distills terrain to the essentials (parabolic cross-valley profile) and optional features (left-side hill, right-side pond or hollow), so that dominant microclimate drivers become visible and testable:\n\nRadiation via terrain exposure cos(i) from slope & aspect\nElevation: daytime negative lapse; pre-dawn weak inversion\nCold-air pooling along the valley axis (Gaussian trough)\nSurface type / land-cover (grass / forest / water / bare soil / maize) alters heating, shading, roughness and nocturnal behaviour\n\nYou can sample synthetic stations, train interpolators (IDW, Kriging variants, RF, GAM), and assess them with spatial LBO-CV.\n\n🔧 This document keeps the previous behaviour but extends the physics with a modular land-cover layer that feeds into both daytime and night fields.\n\n\n\n\nCode\n# Crisp figures\n# EN: Crisp figures\nknitr::opts_chunk$set(fig.width = 9, fig.height = 6, dpi = 150)\n# Alle Chunk-Meldungen global weg\n# EN: Silence messages/warnings in all chunks\nknitr::opts_chunk$set(message = FALSE, warning = FALSE)\n\n# Hilfsfunktion: gstat & Co. ruhigstellen\n# EN: Helper: silence gstat & friends\nquiet &lt;- function(expr) suppressWarnings(suppressMessages(force(expr)))\n\n\n\n\n\n\n\n\n\n\n\nFunction\nRole\n\n\n\n\nbuild_topography()\nCreates elevation (E), optional hill & pond footprints (+ slope/aspect).\n\n\nbuild_landcover()\nBuilds categorical land-cover raster (grass/forest/water/bare/maize).\n\n\nbuild_physics_fields()\nComputes T14 & T05 from topo + land-cover + sun + noise.\n\n\nbuild_scenario()\nOne-stop wrapper returning all rasters (E, R14, R05, lc, etc.).\n\n\nmake_blocks_and_assign()\nBuilds grid blocks and assigns station points for LBO-CV.\n\n\npred_*()\nPoint-wise predictors: Voronoi, IDW, OK, KED, RF, GAM.\n\n\nrun_lbo_cv()\nLeave-Block-Out cross-validation driver (per-block holdout).\n\n\npredict_maps()\nGrid predictions for each model; returns df + ready-made maps.\n\n\nbuild_panels_with_errors()\nTruth | predictions / error panels with CV residuals overlay.\n\n\nmake_obs_pred_scatter()\nObserved vs predicted scatter per model.\n\n\nblock_metrics_long()\nPer-block RMSE/MAE long table for box/scatter plots.\n\n\nmake_block_metric_box()\nBoxplots of block-wise RMSE/MAE per model.\n\n\nmake_abs_error_box()\nBoxplots of per-station absolute error per model.\n\n\nmake_residual_density()\nResidual density (per model) quick diagnostic.\n\n\nrun_for_time()\nSmall wrapper to run CV + maps + panel for one time slot."
  },
  {
    "objectID": "block4_5/mc_2025_pipemodel.html#helper-function-cheat-sheet",
    "href": "block4_5/mc_2025_pipemodel.html#helper-function-cheat-sheet",
    "title": "PipeModel Idealized valley microclimate sandbox",
    "section": "",
    "text": "Function\nRole\n\n\n\n\nbuild_topography()\nCreates elevation (E), optional hill & pond footprints (+ slope/aspect).\n\n\nbuild_landcover()\nBuilds categorical land-cover raster (grass/forest/water/bare/maize).\n\n\nbuild_physics_fields()\nComputes T14 & T05 from topo + land-cover + sun + noise.\n\n\nbuild_scenario()\nOne-stop wrapper returning all rasters (E, R14, R05, lc, etc.).\n\n\nmake_blocks_and_assign()\nBuilds grid blocks and assigns station points for LBO-CV.\n\n\npred_*()\nPoint-wise predictors: Voronoi, IDW, OK, KED, RF, GAM.\n\n\nrun_lbo_cv()\nLeave-Block-Out cross-validation driver (per-block holdout).\n\n\npredict_maps()\nGrid predictions for each model; returns df + ready-made maps.\n\n\nbuild_panels_with_errors()\nTruth | predictions / error panels with CV residuals overlay.\n\n\nmake_obs_pred_scatter()\nObserved vs predicted scatter per model.\n\n\nblock_metrics_long()\nPer-block RMSE/MAE long table for box/scatter plots.\n\n\nmake_block_metric_box()\nBoxplots of block-wise RMSE/MAE per model.\n\n\nmake_abs_error_box()\nBoxplots of per-station absolute error per model.\n\n\nmake_residual_density()\nResidual density (per model) quick diagnostic.\n\n\nrun_for_time()\nSmall wrapper to run CV + maps + panel for one time slot."
  },
  {
    "objectID": "block4_5/mc_2025_pipemodel.html#d.1-generated-rasters-derived-fields",
    "href": "block4_5/mc_2025_pipemodel.html#d.1-generated-rasters-derived-fields",
    "title": "PipeModel Idealized valley microclimate sandbox",
    "section": "5.1 D.1 Generated rasters & derived fields",
    "text": "5.1 D.1 Generated rasters & derived fields\n\n\n\n\n\n\n\n\n\nName\nUnit\nWhat it is\nHow it’s built\n\n\n\n\nE (elev)\nm\nGround elevation\nParabolic “half-pipe” across y; + optional hill; − optional pond/hollow\n\n\nslp, asp\nrad\nSlope, aspect\nterra::terrain(E, \"slope\"/\"aspect\", \"radians\")\n\n\nI14, I05\n–\nCosine solar incidence at 14/05 UTC\ncosi_fun(alt, az, slp, asp), clamped to [0,1]\n\n\nlc\ncat\nLand-cover class\n{Forest, Water, Bare Soil, Maize}; rules from hill/slope/water masks\n\n\nhillW\n0–1\nHill weight (1 inside footprint)\nDisk/Gaussian on left third; combines main + optional micro-hills\n\n\nlake\n0/1\nWater mask\n1 only when lake_choice == \"water\" (disk on right third)\n\n\nI14_eff\n–\nShaded incidence (day)\nI14 * shade_fac_by_lc[lc]\n\n\nαI(lc)\n–\nDaytime solar sensitivity by LC\nLook-up from alpha_I_by_lc\n\n\ndawn_bias(lc)\n°C\nAdditive pre-dawn bias by LC\nLook-up from dawn_bias_by_lc\n\n\npool_fac(lc)\n–\nPooling multiplier by LC\nLook-up from pool_fac_by_lc\n\n\nR14 (T14)\n°C\nDaytime “truth” temperature field\nEq. (below)\n\n\nR05 (T05)\n°C\nPre-dawn “truth” temperature field\nEq. (below)"
  },
  {
    "objectID": "block4_5/mc_2025_pipemodel.html#d.2-governing-equations",
    "href": "block4_5/mc_2025_pipemodel.html#d.2-governing-equations",
    "title": "PipeModel Idealized valley microclimate sandbox",
    "section": "5.2 D.2 Governing equations",
    "text": "5.2 D.2 Governing equations\nLet $$ be the domain-mean elevation. Define the cross-valley cold-pool kernel\n\\[\n\\texttt{pool\\_base} \\;=\\; A \\exp\\!\\left[-(d_y/w)^2\\right],\\quad d_y=|y-y_0|,\n\\]\nblocked over the hill by (1 − pool_block_gain * hillW).\nDay (14 UTC)\n\\[\nT_{14} \\;=\\; T0_{14} \\;+\\; \\texttt{lapse\\_14}\\,(E-\\overline{E})\n\\;+\\; \\alpha_I(\\texttt{lc})\\, I_{14}^{\\text{eff}}\n\\;+\\; \\varepsilon_{14},\n\\quad\nI_{14}^{\\text{eff}} = I_{14}\\cdot \\texttt{shade\\_fac}(\\texttt{lc}).\n\\]\nPre-dawn (05 UTC)\n\\[\nT_{05} \\;=\\; T0_{05} \\;+\\; \\texttt{inv\\_05}\\,(E-\\overline{E})\n\\;+\\; \\eta_{\\text{slope}}\\;\\texttt{slp}\n\\;-\\; \\texttt{pool\\_base}\\cdot(1-\\texttt{pool\\_block\\_gain}\\cdot\\texttt{hillW})\\cdot \\texttt{pool\\_fac}(\\texttt{lc})\n\\;+\\; \\texttt{dawn\\_bias}(\\texttt{lc})\n\\;+\\; \\varepsilon_{05}.\n\\]\nNoise $_{14},_{05} (0,,0.3^2)$ i.i.d.\n\nNote vs. predecessor: the former warm_bias_water_dawn * lake term is now folded into dawn_bias(lc) (class “Water”); daytime α_map became αI(lc) * I14_eff with explicit canopy shading."
  },
  {
    "objectID": "block4_5/mc_2025_pipemodel.html#d.3-dials-what-you-can-tweak",
    "href": "block4_5/mc_2025_pipemodel.html#d.3-dials-what-you-can-tweak",
    "title": "PipeModel Idealized valley microclimate sandbox",
    "section": "5.3 D.3 Dials (what you can tweak)",
    "text": "5.3 D.3 Dials (what you can tweak)\n\n5.3.1 Global scalars\n\n\n\n\n\n\n\n\n\n\nParameter\nDefault\nSensible range\nAffects\nVisual signature (+)\n\n\n\n\nT0_14\n26.0 °C\n20–35\nT14 baseline\nUniform warming\n\n\nlapse_14\n−0.0065 °C/m\n−0.01…−0.002\nT14 vs elevation\nCooler rims, warmer floor\n\n\nT0_05\n8.5 °C\n3–15\nT05 baseline\nUniform warming\n\n\ninv_05\n+0.003 °C/m\n0–0.008\nT05 vs elevation\nRims warmer vs floor\n\n\nη_slope\n0.6\n0–1.5\nT05 slope flow proxy\nSteeper slopes a bit warmer at dawn\n\n\npool_base amplitude\n4.0 K\n1–8\nT05 pooling depth\nStronger blue band on valley axis\n\n\nw_pool\n70 m\n40–150\nT05 pooling width\nNarrower/broader cold band\n\n\npool_block_gain\n0.4\n0–1\nHill blocking\nWarm “tongue” over hill at dawn\n\n\nnoise σ\n0.3 K\n0–1\nBoth\nFine speckle/random texture\n\n\n\n\n\n5.3.2 Land-cover coefficients (by class)\nDefaults used in the code:\n\n\n\n\n\n\n\n\n\n\nLC class\nalpha_I_by_lc\nshade_fac_by_lc\ndawn_bias_by_lc (°C)\npool_fac_by_lc\n\n\n\n\nForest\n3.5\n0.6\n+0.3\n0.7\n\n\nWater\n1.5\n1.0\n+1.2\n0.8\n\n\nBare Soil\n6.0\n1.0\n−0.5\n1.1\n\n\nMaize\n4.5\n0.9\n+0.1\n1.0\n\n\n\nInterpretation: Bare Soil heats most by day and enhances pooling (factor &gt; 1) and cool bias at dawn; Forest damps radiation by day (shading) and reduces pooling (factor &lt; 1); Water heats little by day, gets a positive dawn bias and reduced pooling; Maize sits between grass and forest.\n\n\n5.3.3 Geometry/toggles\n\n\n\n\n\n\n\n\n\nParameter\nDefault\nOptions / range\nEffect\n\n\n\n\nlake_choice\n\"water\"\n\"none\", \"water\", \"hollow\"\nControls depression; only \"water\" sets LC=Water (thermal effects).\n\n\nhill_choice\n\"bump\"\n\"none\", \"bump\"\nAdds blocking & relief.\n\n\nlake_diam_m\n80\n40–150\nSize of pond/hollow.\n\n\nlake_depth_m\n10\n5–30\nDepression depth.\n\n\nhill_diam_m\n80\n40–150\nHill footprint.\n\n\nhill_height_m\n50\n10–120\nHill relief.\n\n\nsmooth_edges\nFALSE\nbool\nSoft pond rim if TRUE.\n\n\nhill_smooth\nFALSE\nbool\nGaussian hill if TRUE.\n\n\n(optional) micro-hills\noff\nrandom_hills, micro_*\nAdds sub-footprint relief; included in hillW."
  },
  {
    "objectID": "block4_5/mc_2025_pipemodel.html#d.4-quick-recipes",
    "href": "block4_5/mc_2025_pipemodel.html#d.4-quick-recipes",
    "title": "PipeModel Idealized valley microclimate sandbox",
    "section": "5.4 D.4 Quick “recipes”",
    "text": "5.4 D.4 Quick “recipes”\n\nCloud/haze day → ↓ alpha_I_by_lc (all classes, esp. Bare/Maize) → daytime LC contrasts fade; models lean on elevation/smoothness.\nHotter afternoon → ↑ T0_14 (+1…+3 K) → uniform bias shift; rankings unchanged.\nStronger pooling → ↑ pool_base and/or ↓ w_pool → sharper, deeper trough; drift-aware models gain.\nWater vs hollow → \"water\" sets LC=Water → ↓ daytime heating, ↑ dawn warm bias, ↓ pooling; \"hollow\" keeps only geometry (no water thermals).\nHill blocking → ↑ pool_block_gain → warm dawn tongue over hill; harder CV across blocks.\nCover swaps (what if): set a patch to Bare Soil → warmer day, colder dawn & stronger pooling; to Forest → cooler day, weaker pooling & slight dawn warm-up."
  },
  {
    "objectID": "block4_5/mc_2025_pipemodel.html#d.5-geometry-at-a-glance",
    "href": "block4_5/mc_2025_pipemodel.html#d.5-geometry-at-a-glance",
    "title": "PipeModel Idealized valley microclimate sandbox",
    "section": "5.5 D.5 Geometry at a glance",
    "text": "5.5 D.5 Geometry at a glance\n\nValley: $E (y-y_0)^2$ — U-shape across y, uniform along x.\nHill (left third): disk/Gaussian of hill_height_m, diameter hill_diam_m; contributes to hillW.\nPond/Hollow (right third): disk depression of lake_depth_m; LC becomes Water only if lake_choice == \"water\"."
  },
  {
    "objectID": "block4_5/mc_2025_pipemodel.html#d.6-what-each-term-looks-like-on-maps",
    "href": "block4_5/mc_2025_pipemodel.html#d.6-what-each-term-looks-like-on-maps",
    "title": "PipeModel Idealized valley microclimate sandbox",
    "section": "5.6 D.6 What each term looks like on maps",
    "text": "5.6 D.6 What each term looks like on maps\n\n\n\n\n\n\n\nTerm\nMap signature\n\n\n\n\nlapse_14 * (E-Ȇ)\nSubtle cool rims / warm floor (day)\n\n\nαI(lc) * I14_eff\nWarm sun-facing slopes; damped under forest/water\n\n\ninv_05 * (E-Ȇ)\nRims warmer vs pooled floor (dawn inversion)\n\n\nη_slope * slp\nSlight dawn warm bias on steeper slopes\n\n\n− pool_base * (1−gain*hillW) * pool_fac(lc)\nBlue band on axis; weaker over forest/water, stronger bare\n\n\n+ dawn_bias(lc)\nLocal dawn warm spots (water/forest), cool bias (bare)"
  },
  {
    "objectID": "block4_5/mc_2025_pipemodel.html#d.8-settings-of-the-current-example",
    "href": "block4_5/mc_2025_pipemodel.html#d.8-settings-of-the-current-example",
    "title": "PipeModel Idealized valley microclimate sandbox",
    "section": "5.7 D.8 Settings of the current example",
    "text": "5.7 D.8 Settings of the current example\n\nalpha_I_by_lc = c(Forest=3.5, Water=1.5, Bare=6.0, Maize=4.5), shade_fac_by_lc = c(0.6,1.0,1.0,0.9), dawn_bias_by_lc = c(+0.3,+1.2,−0.5,+0.1), pool_fac_by_lc = c(0.7,0.8,1.1,1.0)."
  },
  {
    "objectID": "block4_5/mc_2025_pipemodel.html#truth-predictions-error-panels",
    "href": "block4_5/mc_2025_pipemodel.html#truth-predictions-error-panels",
    "title": "PipeModel Idealized valley microclimate sandbox",
    "section": "8.1 Truth, predictions & error panels",
    "text": "8.1 Truth, predictions & error panels\n\n\nCode\n# --- Variogram/Scale utilities -----------------------------------------------\n# ------------------------------------------------------------------------------\n\n# ------------------------------------------------------------------------------\n# plot_variogram_with_scales(vg, L50, L95, sill, title)\n# Purpose:\n#   Quick ggplot helper to display the empirical variogram with dotted sill and\n#   dashed vertical markers at L50 and L95 for interpretation.\n# Returns: a ggplot object.\n# ------------------------------------------------------------------------------\nplot_variogram_with_scales &lt;- function(vg, L50, L95, sill, title = \"Empirical variogram\") {\n  df &lt;- as.data.frame(vg)\n  ggplot2::ggplot(df, ggplot2::aes(dist, gamma)) +\n    ggplot2::geom_point(size = 1.4) +\n    ggplot2::geom_line(alpha = 0.5) +\n    ggplot2::geom_hline(yintercept = sill, linetype = \"dotted\", linewidth = 0.4) +\n    ggplot2::geom_vline(xintercept = L50, colour = \"#2b8cbe\", linetype = \"dashed\") +\n    ggplot2::geom_vline(xintercept = L95, colour = \"#de2d26\", linetype = \"dashed\") +\n    ggplot2::annotate(\"text\", x = L50, y = 0, vjust = -0.5,\n                      label = sprintf(\"L50 = %.0f m\", L50)) +\n    ggplot2::annotate(\"text\", x = L95, y = 0, vjust = -0.5,\n                      label = sprintf(\"L95 = %.0f m\", L95), colour = \"#de2d26\") +\n    ggplot2::theme_minimal() +\n    ggplot2::labs(title = title, x = \"Distance (m)\", y = \"Semivariance\")\n}\n\n# --- DEM smoothing + sun geometry --------------------------------------------\n# ------------------------------------------------------------------------------\n# gaussian_focal(r, radius_m, sigma_m = NULL)\n# Purpose:\n#   Build a separable, normalized 2D Gaussian kernel (in pixels) from a target\n#   radius in meters, then return the kernel matrix to be used with terra::focal.\n# Inputs:\n#   - r: reference raster to read pixel size from.\n#   - radius_m: target smoothing radius (meters).\n#   - sigma_m: optional sigma; by default half the radius.\n# Returns:\n#   A normalized kernel matrix suitable for terra::focal() smoothing.\n# ------------------------------------------------------------------------------\ngaussian_focal &lt;- function(r, radius_m, sigma_m = NULL) {\n  resx &lt;- terra::res(r)[1]\n  if (is.null(sigma_m)) sigma_m &lt;- radius_m / 2\n  rad_px   &lt;- max(1L, round(radius_m / resx))\n  sigma_px &lt;- max(0.5, sigma_m / resx)\n  xs &lt;- -rad_px:rad_px\n  k1 &lt;- exp(-0.5 * (xs / sigma_px)^2); k1 &lt;- k1 / sum(k1)\n  K  &lt;- outer(k1, k1); K / sum(K)\n}\n\n# ------------------------------------------------------------------------------\n# smooth_dem_and_derive(E, alt, az, radius_m)\n# Purpose:\n#   Smooth the DEM at a given metric radius and recompute slope and\n#   cosine-of-incidence for a specified sun position (alt/az).\n# Returns:\n#   list(Es = smoothed DEM, slp = slope, cosi = cosine-of-incidence)\n# ------------------------------------------------------------------------------\nsmooth_dem_and_derive &lt;- function(E, alt, az, radius_m) {\n  K   &lt;- gaussian_focal(E, radius_m)\n  Es  &lt;- terra::focal(E, w = K, fun = mean, na.policy = \"omit\", pad = TRUE)\n  slp &lt;- terra::terrain(Es, v = \"slope\",  unit = \"radians\")\n  asp &lt;- terra::terrain(Es, v = \"aspect\", unit = \"radians\")\n  zen &lt;- (pi/2 - alt)\n  ci  &lt;- cos(slp)*cos(zen) + sin(slp)*sin(zen)*cos(az - asp)\n  ci  &lt;- terra::ifel(ci &lt; 0, 0, ci)\n  list(Es = Es, slp = slp, cosi = ci)\n}\n\n# --- helpers to cap k by available info --------------------------------\n.k_for_xy &lt;- function(n, n_xy) max(3, min(60, n_xy - 1L, floor(n * 0.8)))\n.kcap_unique &lt;- function(x, kmax) {\n  ux &lt;- unique(x[is.finite(x)])\n  nu &lt;- length(ux)\n  if (nu &lt;= 3) return(0L)                # treat as constant/near-constant\n  max(4L, min(kmax, nu - 1L))\n}\n\n# --- CV of GAM with R-smoothed predictors (robust k) -------------------\n# ------------------------------------------------------------------------------\n# cv_gam_with_R(stn_sf, E, alt, az, R, block_size_m)\n# Purpose:\n#   Leave-block-out CV of a GAM whose predictors are computed from a DEM\n#   smoothed at radius R (meters). This aligns the drift scale to the process\n#   scale before fitting, then evaluates predictive skill via blocked holdouts.\n# Notes:\n#   - Contains guards for low sample size and dynamic k to avoid mgcv errors.\n# Returns:\n#   list(cv = per-point CV table, RMSE = numeric)\n# ------------------------------------------------------------------------------\ncv_gam_with_R &lt;- function(stn_sf, E, alt = NULL, az = NULL, R, block_size_m = NULL) {\n  \n  # ---- 0) Blockgröße sauber auflösen (tuning-fähig)\n  bs &lt;- suppressWarnings(as.numeric(block_size_m)[1])             # bevorzugt: Tuning\n  if (!is.finite(bs) || bs &lt;= 0) {\n    bs &lt;- suppressWarnings(as.numeric(get0(\"block_size\",          # Fallback: global\n                                           ifnotfound = NA_real_)))\n  }\n  if (!is.finite(bs) || bs &lt;= 0)\n    stop(\"cv_gam_with_R(): keine gültige Blockgröße gefunden (Tuning oder global).\")\n  \n  # ---- 1) R-gesmoothete Raster bauen (wie bei dir)\n  zR   &lt;- smooth_mean_R(E, R)\n  slpR &lt;- terra::terrain(zR, v = \"slope\",  unit = \"radians\")\n  aspR &lt;- terra::terrain(zR, v = \"aspect\", unit = \"radians\")\n  cosiR &lt;- if (!is.null(alt) && !is.null(az)) {\n    ci &lt;- cos(slpR)*cos(pi/2 - alt) + sin(slpR)*sin(pi/2 - alt)*cos(az - aspR)\n    terra::ifel(ci &lt; 0, 0, ci)\n  } else NULL\n  \n  # ---- 2) Werte an Stationen extrahieren (wie bei dir)\n  if (!all(c(\"x\",\"y\") %in% names(stn_sf))) {\n    xy &lt;- sf::st_coordinates(stn_sf); stn_sf$x &lt;- xy[,1]; stn_sf$y &lt;- xy[,2]\n  }\n  fill_med &lt;- function(v) { m &lt;- stats::median(v[is.finite(v)], na.rm = TRUE); ifelse(is.finite(v), v, m) }\n  stn_sf$z_surf_R &lt;- fill_med(.extract_to_pts(zR,   stn_sf))\n  stn_sf$slp_R    &lt;- fill_med(.extract_to_pts(slpR, stn_sf))\n  stn_sf$cosi_R   &lt;- if (is.null(cosiR)) rep(NA_real_, nrow(stn_sf)) else fill_med(.extract_to_pts(cosiR, stn_sf))\n  \n  # ---- 3) Blöcke bauen und zuordnen (einheitlich mit bs)\n  bb_poly &lt;- sf::st_as_sfc(sf::st_bbox(stn_sf), crs = sf::st_crs(stn_sf))\n  blocks  &lt;- sf::st_make_grid(bb_poly, cellsize = c(bs, bs), what = \"polygons\")\n  blocks  &lt;- sf::st_sf(block_id = seq_along(blocks), geometry = blocks)\n  \n  stn_blk &lt;- sf::st_join(stn_sf, blocks, join = sf::st_intersects, left = TRUE)\n  if (anyNA(stn_blk$block_id)) {\n    i &lt;- is.na(stn_blk$block_id)\n    stn_blk$block_id[i] &lt;- blocks$block_id[sf::st_nearest_feature(stn_blk[i,], blocks)]\n  }\n  \n  if (!all(c(\"x\",\"y\") %in% names(stn_blk))) {\n    xy &lt;- sf::st_coordinates(stn_blk); stn_blk$x &lt;- xy[,1]; stn_blk$y &lt;- xy[,2]\n  }\n  \n  # ---- 4) CV-Schleife (dein Code unverändert weiter)\n  bids  &lt;- sort(unique(stn_blk$block_id))\n  preds &lt;- vector(\"list\", length(bids)); j &lt;- 0L\n  for (b in bids) {\n    te &lt;- stn_blk[stn_blk$block_id == b, ]\n    tr &lt;- stn_blk[stn_blk$block_id != b, ]\n    \n    dtr &lt;- sf::st_drop_geometry(tr)\n    need &lt;- c(\"temp\",\"x\",\"y\",\"z_surf_R\",\"slp_R\",\"cosi_R\")\n    dtr  &lt;- dtr[stats::complete.cases(dtr[, intersect(need, names(dtr)), drop = FALSE]), , drop = FALSE]\n    if (nrow(dtr) &lt; 10) next\n    \n    n_xy &lt;- dplyr::n_distinct(paste0(round(dtr$x,3), \"_\", round(dtr$y,3)))\n    k_xy &lt;- .k_for_xy(nrow(dtr), n_xy)\n    k_z  &lt;- .kcap_unique(dtr$z_surf_R, 20)\n    k_sl &lt;- .kcap_unique(dtr$slp_R,    12)\n    k_ci &lt;- .kcap_unique(dtr$cosi_R,   12)\n    \n    terms &lt;- c()\n    terms &lt;- c(terms, if (n_xy &gt;= 4) sprintf(\"s(x,y,bs='tp',k=%d)\", k_xy) else \"x + y\")\n    terms &lt;- c(terms, if (k_z  &gt;= 4) sprintf(\"s(z_surf_R,bs='tp',k=%d)\", k_z)  else \"z_surf_R\")\n    if (length(unique(dtr$slp_R[is.finite(dtr$slp_R)])) &gt; 1)\n      terms &lt;- c(terms, if (k_sl &gt;= 4) sprintf(\"s(slp_R,bs='tp',k=%d)\", k_sl) else \"slp_R\")\n    if (any(is.finite(dtr$cosi_R)) && length(unique(dtr$cosi_R[is.finite(dtr$cosi_R)])) &gt; 1)\n      terms &lt;- c(terms, if (k_ci &gt;= 4) sprintf(\"s(cosi_R,bs='tp',k=%d)\", k_ci) else \"cosi_R\")\n    \n    form &lt;- as.formula(paste(\"temp ~\", paste(terms, collapse = \" + \")))\n    gm &lt;- mgcv::gam(form, data = dtr, method = \"REML\", select = TRUE)\n    \n    dte &lt;- sf::st_drop_geometry(te)\n    ph  &lt;- try(stats::predict(gm, newdata = dte, type = \"response\"), silent = TRUE)\n    if (inherits(ph, \"try-error\")) ph &lt;- rep(NA_real_, nrow(dte))\n    \n    j &lt;- j + 1L\n    preds[[j]] &lt;- tibble::tibble(id = te$id, obs = te$temp, pred = as.numeric(ph), block_id = b)\n  }\n  \n  preds &lt;- preds[seq_len(j)]\n  if (!length(preds)) {\n    return(list(cv = tibble::tibble(id = integer(), obs = numeric(), pred = numeric(), block_id = integer()),\n                RMSE = NA_real_))\n  }\n  out  &lt;- dplyr::bind_rows(preds)\n  rmse &lt;- sqrt(mean((out$pred - out$obs)^2, na.rm = TRUE))\n  list(cv = out, RMSE = rmse)\n}\n\n# --- U-curve tuning -----------------------------------------------------------\n# ------------------------------------------------------------------------------\n# tune_Rstar_ucurve(stn_sf, E, alt, az, L50, L95, block_fallback, n_grid, extra)\n# Purpose:\n#   Scan candidate R values (around the L50–L95 interval) and pick R* that\n#   minimises blocked-CV RMSE. Returns the scan table and chosen R*.\n# Returns:\n#   list(grid = data.frame(R, RMSE), R_star, block_m)\n# ------------------------------------------------------------------------------\ntune_Rstar_ucurve &lt;- function(stn_sf, E, alt, az, L50, L95, block_fallback = 120, n_grid = 6, extra = c(0.8, 1.2)) {\n  L50 &lt;- as.numeric(L50); L95 &lt;- as.numeric(L95)\n  if (!is.finite(L50) || !is.finite(L95) || L95 &lt;= L50) {\n    e &lt;- terra::ext(E)\n    dom_diag &lt;- sqrt((terra::xmax(e)-terra::xmin(e))^2 + (terra::ymax(e)-terra::ymin(e))^2)\n    L50 &lt;- dom_diag/10; L95 &lt;- dom_diag/4\n  }\n  block_m &lt;- max(block_fallback, round(L50))\n  R_min &lt;- max(10, round(L50*extra[1])); R_max &lt;- round(L95*extra[2])\n  R_grid &lt;- unique(round(seq(R_min, R_max, length.out = n_grid)))\n  df &lt;- do.call(rbind, lapply(R_grid, function(R) { z &lt;- cv_gam_with_R(stn_sf, E, alt, az, R = R, block_size_m = block_m); c(R = R, RMSE = z$RMSE) })) |&gt; as.data.frame()\n  R_star &lt;- df$R[which.min(df$RMSE)]\n  list(grid = df, R_star = as.numeric(R_star), block_m = block_m)\n}\n\n# ------------------------------------------------------------------------------\n# plot_ucurve(df, R_star, title)\n# Purpose:\n#   Visual helper to display the U-curve of RMSE vs. drift radius R with a\n#   dashed marker at the selected R*.\n# Returns: a ggplot object.\n# ------------------------------------------------------------------------------\nplot_ucurve &lt;- function(df, R_star, title = \"U-curve: tune R\") {\n  ggplot2::ggplot(df, ggplot2::aes(R, RMSE)) +\n    ggplot2::geom_line() + ggplot2::geom_point() +\n    ggplot2::geom_vline(xintercept = R_star, linetype = \"dashed\", colour = \"#de2d26\") +\n    ggplot2::theme_minimal() + ggplot2::labs(title = title, x = \"Drift radius R (m)\", y = \"RMSE (block-CV)\")\n}\n\n# --- Factor alignment (robust) -----------------------------------------------\n.align_factor_to_model &lt;- function(x, lev_model) {\n  xs &lt;- as.character(x)\n  if (length(lev_model) == 0L) return(factor(rep(NA_character_, length(xs))))\n  y &lt;- factor(xs, levels = lev_model)\n  if (anyNA(y)) {\n    xs[is.na(y)] &lt;- lev_model[1]\n    y &lt;- factor(xs, levels = lev_model)\n  }\n  y\n}\n\n\n\n\nCode\n# ---------- SF-only learners ----------\npred_Voronoi &lt;- function(train_sf, test_sf) {\n  idx &lt;- sf::st_nearest_feature(test_sf, train_sf)\n  as.numeric(train_sf$temp)[idx]\n}\n\npred_IDW &lt;- function(train_sf, test_sf, idp = 2) {\n  pr &lt;- suppressWarnings(gstat::idw(temp ~ 1, locations = train_sf, newdata = test_sf, idp = idp))\n  as.numeric(pr$var1.pred)\n}\n\n.default_vgm &lt;- function(values, model = \"Exp\", range = 100) {\n  psill &lt;- stats::var(values, na.rm = TRUE); nug &lt;- 0.1 * psill\n  gstat::vgm(psill = psill, model = model, range = range, nugget = nug)\n}\n\npred_OK &lt;- function(train_sf, test_sf) {\n  vg      &lt;- suppressWarnings(gstat::variogram(temp ~ 1, data = train_sf))\n  vgm_fit &lt;- try(suppressWarnings(gstat::fit.variogram(vg, gstat::vgm(\"Exp\"))), silent = TRUE)\n  if (inherits(vgm_fit, \"try-error\")) vgm_fit &lt;- .default_vgm(train_sf$temp)\n  pr &lt;- suppressWarnings(gstat::krige(temp ~ 1, locations = train_sf, newdata = test_sf, model = vgm_fit))\n  as.numeric(pr$var1.pred)\n}\n\n.align_factor_to_model &lt;- function(x, lev_model) {\n  y &lt;- factor(as.character(x), levels = lev_model)\n  if (anyNA(y)) y[is.na(y)] &lt;- lev_model[1]\n  y\n}\n.fill_num_na_vec &lt;- function(x, ref) {\n  m &lt;- stats::median(ref[is.finite(ref)], na.rm = TRUE)\n  x[!is.finite(x)] &lt;- m\n  x\n}\n\n# --- sf-only KED (schluckt extra Args wie E=E) ----------------------\npred_KED &lt;- function(train_sf, test_sf, ...) {\n  stopifnot(inherits(train_sf, \"sf\"), inherits(test_sf, \"sf\"))\n  need &lt;- c(\"z_surf\",\"slp\",\"cosi\")\n  miss &lt;- setdiff(need, names(train_sf))\n  if (length(miss)) stop(\"pred_KED(): fehlende Drift-Spalten im Training: \",\n                         paste(miss, collapse = \", \"))\n  \n  # Optional LC als Faktor angleichen\n  use_lc &lt;- \"lc\" %in% names(train_sf) && \"lc\" %in% names(test_sf)\n  tr &lt;- train_sf\n  te &lt;- test_sf\n  if (use_lc) {\n    tr$lc &lt;- droplevels(factor(tr$lc))\n    te$lc &lt;- factor(as.character(te$lc), levels = levels(tr$lc))\n    te$lc[is.na(te$lc)] &lt;- levels(tr$lc)[1]\n  }\n  \n  # fehlende numerische Drifts im TEST mit Trainingsmedian auffüllen\n  for (nm in need) {\n    m &lt;- stats::median(tr[[nm]][is.finite(tr[[nm]])], na.rm = TRUE)\n    te[[nm]][!is.finite(te[[nm]])] &lt;- m\n  }\n  \n  # nur vollständige Trainingszeilen\n  keep_tr &lt;- c(\"temp\", need, if (use_lc) \"lc\")\n  dtr &lt;- sf::st_drop_geometry(tr)[, keep_tr, drop = FALSE]\n  ok  &lt;- stats::complete.cases(dtr)\n  tr  &lt;- tr[ok, ]\n  if (nrow(tr) &lt; 5) return(rep(NA_real_, nrow(te)))\n  \n  # Formel: lineare Drifts + optional LC\n  form &lt;- stats::as.formula(paste(\"temp ~\", paste(c(need, if (use_lc) \"lc\"), collapse = \" + \")))\n  \n  # Variogramm + robuster Fit\n  vg      &lt;- suppressWarnings(gstat::variogram(form, data = tr))\n  vgm_fit &lt;- try(suppressWarnings(gstat::fit.variogram(vg, gstat::vgm(\"Exp\"))), silent = TRUE)\n  if (inherits(vgm_fit, \"try-error\")) {\n    ps &lt;- stats::var(sf::st_drop_geometry(tr)$temp, na.rm = TRUE)\n    vgm_fit &lt;- gstat::vgm(psill = ps, model = \"Exp\", range = max(vg$dist, na.rm = TRUE)/3, nugget = 0.1*ps)\n  }\n  \n  # Kriging mit externen Drifts (UK/KED)\n  pr &lt;- suppressWarnings(gstat::krige(form, locations = tr, newdata = te, model = vgm_fit))\n  as.numeric(pr$var1.pred)\n}"
  },
  {
    "objectID": "block4_5/mc_2025_pipemodel.html#block-wise-and-per-station-errors",
    "href": "block4_5/mc_2025_pipemodel.html#block-wise-and-per-station-errors",
    "title": "PipeModel Idealized valley microclimate sandbox",
    "section": "8.2 Block-wise and per-station errors",
    "text": "8.2 Block-wise and per-station errors\n\n\nCode\n# --- Leave-Block-Out CV -------------------------------------------------------\n# ------------------------------------------------------------------------------\n# make_blocks_and_assign(pts_sf, E, block_size)\n# Purpose:\n#   Build a square grid of spatial blocks and assign each station to a block\n#   (nearest if on edge). Used by leave-block-out cross-validation.\n# Inputs:\n#   - pts_sf: station sf with geometry.\n#   - E: reference raster for domain extent/CRS.\n#   - block_size: block edge length in meters.\n# Returns:\n#   list(blocks = sf polygons, pts = station sf with block_id).\n# ------------------------------------------------------------------------------\nmake_blocks_and_assign &lt;- function(pts_sf, E, block_size = 100) {\n  bb &lt;- sf::st_as_sfc(sf::st_bbox(c(xmin = terra::xmin(E), ymin = terra::ymin(E), xmax = terra::xmax(E), ymax = terra::ymax(E)), crs = sf::st_crs(pts_sf)))\n  gr &lt;- sf::st_make_grid(bb, cellsize = c(block_size, block_size), what = \"polygons\")\n  blocks &lt;- sf::st_sf(block_id = seq_along(gr), geometry = gr)\n  pts_blk &lt;- sf::st_join(pts_sf, blocks, join = sf::st_intersects, left = TRUE)\n  if (any(is.na(pts_blk$block_id))) {\n    nearest &lt;- sf::st_nearest_feature(pts_blk[is.na(pts_blk$block_id), ], blocks)\n    pts_blk$block_id[is.na(pts_blk$block_id)] &lt;- blocks$block_id[nearest]\n  }\n  list(blocks = blocks, pts = pts_blk)\n}\n\n# saubere Farbskala für viele Blöcke\n.discrete_cols &lt;- function(n) scales::hue_pal()(n)\n\nplot_blocks_grid &lt;- function(blocks, pts_blk, title = \"Blocks & stations\") {\n  # Ziel-CRS = CRS der Daten (UTM32N), Achsen in Metern\n  crs_plot &lt;- sf::st_crs(pts_blk)\n  bb       &lt;- sf::st_bbox(blocks)\n  n_blocks &lt;- dplyr::n_distinct(pts_blk$block_id)\n  cols     &lt;- .discrete_cols(max(1, n_blocks))\n\n  ggplot() +\n    geom_sf(data = blocks, fill = NA, color = \"grey50\", linewidth = 0.25) +\n    geom_sf(data = pts_blk, aes(color = factor(block_id)), size = 2, alpha = 0.95) +\n    scale_color_manual(values = cols, name = \"Block\") +\n    coord_sf(\n      crs  = crs_plot,    # &lt;- erzwingt UTM32N als Plot-CRS (Meterachsen)\n      datum = NA,         # keine Gradnetz-Beschriftung\n      xlim = c(bb[\"xmin\"], bb[\"xmax\"]),\n      ylim = c(bb[\"ymin\"], bb[\"ymax\"]),\n      expand = FALSE\n    ) +\n    theme_minimal() +\n    labs(title = title, x = \"Easting (m)\", y = \"Northing (m)\")\n}\n\n\n# ------------------------------------------------------------------------------\n# run_lbo_cv(stn_sf, E, block_size, models)\n# Purpose:\n#   Perform leave-block-out cross-validation across the requested set of models.\n#   Each block is held out in turn; models are trained on the remainder and\n#   predictions are collected for the held-out stations.\n# Returns:\n#   list(cv = long per-point table, metrics = summary table, diag_plot, blocks_plot)\n# Notes:\n#   - No model settings are changed; this wrapper only orchestrates the CV.\n# ------------------------------------------------------------------------------\nrun_lbo_cv &lt;- function(stn_sf, E, block_size = 100, models = models_use) {\n  if (!all(c(\"x\",\"y\") %in% names(stn_sf))) { xy &lt;- sf::st_coordinates(stn_sf); stn_sf$x &lt;- xy[,1]; stn_sf$y &lt;- xy[,2] }\n  blk &lt;- make_blocks_and_assign(stn_sf, E, block_size = block_size)\n  blocks_sf &lt;- blk$blocks; stn_blk &lt;- blk$pts\n  restore &lt;- function(nm) if (!(nm %in% names(stn_blk))) stn_blk[[nm]] &lt;&lt;- stn_sf[[nm]][match(stn_blk$id, stn_sf$id)]\n  for (nm in c(\"temp\",\"z_surf\",\"slp\",\"cosi\",\"lc\",\"x\",\"y\")) restore(nm)\n\n  block_ids &lt;- sort(unique(stn_blk$block_id))\n  out_list &lt;- vector(\"list\", length(block_ids))\n  for (k in seq_along(block_ids)) {\n    b &lt;- block_ids[k]\n    test_idx  &lt;- which(stn_blk$block_id == b)\n    train_idx &lt;- which(stn_blk$block_id != b)\n    train_sf &lt;- stn_blk[train_idx, ]; test_sf &lt;- stn_blk[test_idx, ]\n    pred_tbl &lt;- lapply(models, function(m) {\n      p &lt;- switch(m,\n        \"Voronoi\" = pred_Voronoi(train_sf, test_sf),\n        \"IDW\"     = pred_IDW(train_sf, test_sf),\n        \"OK\"      = pred_OK(train_sf, test_sf),\n        \"KED\"     = pred_KED(train_sf, test_sf, E = E),\n        \"RF\"      = pred_RF(train_sf, test_sf),\n        \"GAM\"     = pred_GAM(train_sf, test_sf),\n        stop(\"Unknown model: \", m)\n      )\n      tibble::tibble(model = m, id = test_sf$id, obs = test_sf$temp, pred = p, block_id = b)\n    })\n    out_list[[k]] &lt;- dplyr::bind_rows(pred_tbl)\n  }\n\n  cv_tbl &lt;- dplyr::bind_rows(out_list)\n  metrics &lt;- cv_tbl %&gt;%\n    dplyr::group_by(model) %&gt;%\n    dplyr::summarise(\n      n    = dplyr::n(),\n      n_ok = sum(is.finite(obs) & is.finite(pred)),\n      MAE  = {i &lt;- is.finite(obs) & is.finite(pred); if (any(i)) mean(abs(pred[i]-obs[i])) else NA_real_},\n      RMSE = {i &lt;- is.finite(obs) & is.finite(pred); if (any(i)) sqrt(mean((pred[i]-obs[i])^2)) else NA_real_},\n      Bias = {i &lt;- is.finite(obs) & is.finite(pred); if (any(i)) mean(pred[i]-obs[i]) else NA_real_},\n      R2   = safe_r2(obs, pred),\n      .groups = \"drop\"\n    ) |&gt;\n    dplyr::arrange(RMSE)\n\n  diag_plot &lt;- ggplot(cv_tbl, aes(obs, pred)) +\n    geom_abline(slope=1, intercept=0, linetype=\"dashed\") +\n    geom_point(alpha=0.7) +\n    coord_equal() + theme_minimal() +\n    labs(title = sprintf(\"LBO-CV (block = %dm) — Observed vs Predicted\", block_size), x = \"Observed\", y = \"Predicted\") +\n    facet_wrap(~ model)\n\n  blocks_plot &lt;- plot_blocks_grid(blocks_sf, stn_blk, title = sprintf(\"Blocks (%.0f m) & stations\", block_size))\n  list(cv = cv_tbl, metrics = metrics, diag_plot = diag_plot, blocks_plot = blocks_plot)\n}\n\n\n\n\nCode\nmessage(\"Running LBO-CV and building maps for T14 ...\")\nlbo_cv_14_result &lt;- run_for_time(stn_sf_14, scen$R14, \"T14\")\n\n\n[inverse distance weighted interpolation]\n[using ordinary kriging]\n[using universal kriging]\n[inverse distance weighted interpolation]\n[using ordinary kriging]\n[using universal kriging]\n[inverse distance weighted interpolation]\n[using ordinary kriging]\n[using universal kriging]\n[inverse distance weighted interpolation]\n[using ordinary kriging]\n[using universal kriging]\n[inverse distance weighted interpolation]\n[using ordinary kriging]\n[using universal kriging]\n[inverse distance weighted interpolation]\n[using ordinary kriging]\n[using universal kriging]\n[inverse distance weighted interpolation]\n[using ordinary kriging]\n[using universal kriging]\n[inverse distance weighted interpolation]\n[using ordinary kriging]\n[using universal kriging]\n[inverse distance weighted interpolation]\n[using ordinary kriging]\n[using universal kriging]\n[inverse distance weighted interpolation]\n[using ordinary kriging]\n[using universal kriging]\n[inverse distance weighted interpolation]\n[using ordinary kriging]\n[using universal kriging]\n[inverse distance weighted interpolation]\n[using ordinary kriging]\n[using universal kriging]\n[inverse distance weighted interpolation]\n[using ordinary kriging]\n[using universal kriging]\n[inverse distance weighted interpolation]\n[using ordinary kriging]\n[using universal kriging]\n[inverse distance weighted interpolation]\n[using ordinary kriging]\n[using universal kriging]\n[inverse distance weighted interpolation]\n[using ordinary kriging]\n[using universal kriging]\n[inverse distance weighted interpolation]\n[using ordinary kriging]\n[using universal kriging]\n[inverse distance weighted interpolation]\n[using ordinary kriging]\n[using universal kriging]\n[inverse distance weighted interpolation]\n[using ordinary kriging]\n[using universal kriging]\n[inverse distance weighted interpolation]\n[using ordinary kriging]\n[using universal kriging]\n[inverse distance weighted interpolation]\n[using ordinary kriging]\n[using universal kriging]\n[inverse distance weighted interpolation]\n[using ordinary kriging]\n[using universal kriging]\n[inverse distance weighted interpolation]\n[using ordinary kriging]\n[using universal kriging]\n\n\nCode\nmessage(\"Running LBO-CV and building maps for T05 ...\")\nlbo_cv_05_result &lt;- run_for_time(stn_sf_05, scen$R05, \"T05\")\n\n\n[inverse distance weighted interpolation]\n[using ordinary kriging]\n[using universal kriging]\n[inverse distance weighted interpolation]\n[using ordinary kriging]\n[using universal kriging]\n[inverse distance weighted interpolation]\n[using ordinary kriging]\n[using universal kriging]\n[inverse distance weighted interpolation]\n[using ordinary kriging]\n[using universal kriging]\n[inverse distance weighted interpolation]\n[using ordinary kriging]\n[using universal kriging]\n[inverse distance weighted interpolation]\n[using ordinary kriging]\n[using universal kriging]\n[inverse distance weighted interpolation]\n[using ordinary kriging]\n[using universal kriging]\n[inverse distance weighted interpolation]\n[using ordinary kriging]\n[using universal kriging]\n[inverse distance weighted interpolation]\n[using ordinary kriging]\n[using universal kriging]\n[inverse distance weighted interpolation]\n[using ordinary kriging]\n[using universal kriging]\n[inverse distance weighted interpolation]\n[using ordinary kriging]\n[using universal kriging]\n[inverse distance weighted interpolation]\n[using ordinary kriging]\n[using universal kriging]\n[inverse distance weighted interpolation]\n[using ordinary kriging]\n[using universal kriging]\n[inverse distance weighted interpolation]\n[using ordinary kriging]\n[using universal kriging]\n[inverse distance weighted interpolation]\n[using ordinary kriging]\n[using universal kriging]\n[inverse distance weighted interpolation]\n[using ordinary kriging]\n[using universal kriging]\n[inverse distance weighted interpolation]\n[using ordinary kriging]\n[using universal kriging]\n[inverse distance weighted interpolation]\n[using ordinary kriging]\n[using universal kriging]\n[inverse distance weighted interpolation]\n[using ordinary kriging]\n[using universal kriging]\n[inverse distance weighted interpolation]\n[using ordinary kriging]\n[using universal kriging]\n[inverse distance weighted interpolation]\n[using ordinary kriging]\n[using universal kriging]\n[inverse distance weighted interpolation]\n[using ordinary kriging]\n[using universal kriging]\n[inverse distance weighted interpolation]\n[using ordinary kriging]\n[using universal kriging]\n\n\n\n\nCode\nlbo_cv_14_result$res$blocks_plot \n\n\n\n\n\n\n\n\n\nCode\nlbo_cv_14_result$res$diag_plot\n\n\n\n\n\n\n\n\n\nCode\nlbo_cv_05_result$res$blocks_plot  \n\n\n\n\n\n\n\n\n\nCode\nlbo_cv_05_result$res$diag_plot\n\n\n\n\n\n\n\n\n\nCode\nlbo_cv_14_result$panel\n\n\n$`1`\n\n\n\n\n\n\n\n\n\nCode\nlbo_cv_05_result$panel\n\n\n$`1`\n\n\n\n\n\n\n\n\n\nCode\np_block_box14  &lt;- make_block_metric_box(lbo_cv_14_result$res$cv, \"T14\")\np_abserr_box14 &lt;- make_abs_error_box(lbo_cv_14_result$res$cv,  \"T14\")\np_block_box05  &lt;- make_block_metric_box(lbo_cv_05_result$res$cv, \"T05\")\np_abserr_box05 &lt;- make_abs_error_box(lbo_cv_05_result$res$cv, \"T05\")\n\n(p_block_box14 | p_abserr_box14) / (p_block_box05 | p_abserr_box05)"
  },
  {
    "objectID": "block4_5/mc_2025_pipemodel.html#lbo-cv-metrics-and-residuals",
    "href": "block4_5/mc_2025_pipemodel.html#lbo-cv-metrics-and-residuals",
    "title": "PipeModel Idealized valley microclimate sandbox",
    "section": "8.3 LBO-CV metrics and residuals",
    "text": "8.3 LBO-CV metrics and residuals\n\n\nCode\nknitr::kable(lbo_cv_14_result$res$metrics, digits = 3, caption = \"LBO-CV metrics — T14\")\n\n\n\nLBO-CV metrics — T14\n\n\nmodel\nn\nn_ok\nMAE\nRMSE\nBias\nR2\n\n\n\n\nGAM\n60\n60\n0.275\n0.323\n-0.005\n0.798\n\n\nKED\n60\n60\n0.276\n0.333\n0.000\n0.784\n\n\nRF\n60\n60\n0.375\n0.480\n0.011\n0.553\n\n\nIDW\n60\n60\n0.541\n0.711\n0.088\n0.032\n\n\nOK\n60\n60\n0.548\n0.716\n0.049\n0.016\n\n\nVoronoi\n60\n60\n0.667\n0.897\n0.205\n0.039\n\n\n\n\n\nCode\nknitr::kable(lbo_cv_05_result$res$metrics, digits = 3, caption = \"LBO-CV metrics — T05\")\n\n\n\nLBO-CV metrics — T05\n\n\nmodel\nn\nn_ok\nMAE\nRMSE\nBias\nR2\n\n\n\n\nGAM\n60\n60\n0.232\n0.315\n-0.013\n0.933\n\n\nRF\n60\n60\n0.250\n0.323\n-0.036\n0.932\n\n\nOK\n60\n60\n0.624\n0.933\n-0.067\n0.424\n\n\nIDW\n60\n60\n0.792\n1.024\n-0.127\n0.358\n\n\nVoronoi\n60\n60\n0.728\n1.169\n-0.165\n0.346\n\n\nKED\n60\n60\n0.794\n1.633\n0.218\n0.225\n\n\n\n\n\nCode\nmake_obs_pred_scatter(lbo_cv_14_result$res$cv, \"T14\")\n\n\n\n\n\n\n\n\n\nCode\nmake_obs_pred_scatter(lbo_cv_05_result$res$cv, \"T05\")\n\n\n\n\n\n\n\n\n\nCode\nmake_residual_density(lbo_cv_14_result$res$cv, \"T14\")\n\n\n\n\n\n\n\n\n\nCode\nmake_residual_density(lbo_cv_05_result$res$cv, \"T05\")"
  },
  {
    "objectID": "block4_5/mc_2025_pipemodel.html#g-diagnostic",
    "href": "block4_5/mc_2025_pipemodel.html#g-diagnostic",
    "title": "PipeModel Idealized valley microclimate sandbox",
    "section": "8.4 G Diagnostic",
    "text": "8.4 G Diagnostic\nAwesome—let’s read your baseline (no R*) results explicitly through the lens of process (what drives T) and scale (over what distances the drivers operate), model-by-model and time-by-time, then close with a scale+process summary and concrete upgrades."
  },
  {
    "objectID": "block4_5/mc_2025_pipemodel.html#summary",
    "href": "block4_5/mc_2025_pipemodel.html#summary",
    "title": "PipeModel Idealized valley microclimate sandbox",
    "section": "12.1 Summary",
    "text": "12.1 Summary\nDaytime temperature is controlled by very local facet and LC effects layered over a gentle lapse; models that encode those drivers at the right (small) scale—notably GAM, then RF—generalize across blocks with low error.\nPre-dawn temperature is anisotropic with a short cross-valley pooling scale, slope, and LC offsets; RF captures these thresholdy interactions best, with GAM second. Purely spatial smoothers (OK/IDW/Voronoi) underperform because their smoothing scale and mean process are mismatched.\nBring kriging back into contention by giving it the right drifts (cos(i), LC, distance-to-axis, hill-block) at tuned feature scales (R*), and by acknowledging anisotropy at night; if you want the best of both worlds, use regression-kriging with the learned mean from GAM/RF and an anisotropic residual field."
  },
  {
    "objectID": "block4_5/mc_2025_pipemodel.html#i.-scale-analysis-l50l95-tuned-ked-drift-r",
    "href": "block4_5/mc_2025_pipemodel.html#i.-scale-analysis-l50l95-tuned-ked-drift-r",
    "title": "PipeModel Idealized valley microclimate sandbox",
    "section": "17.1 I. Scale analysis — L50/L95 & tuned KED drift (R*)",
    "text": "17.1 I. Scale analysis — L50/L95 & tuned KED drift (R*)\nThis section adds a four-stage pipeline:\n\nScale inference: global variogram → L50/L95\n\nScale-matched predictors: drift from smoothed E at radius R\n\nTune R* with blocked CV (U-curve)\n\nDiagnostics: full benchmark + simple error budget\n\n\nWhy: Matching the model scale to the process scale reduces scale-mismatch error and makes gains attributable to scale rather than algorithm choice.\n\n\n\nCode\n# --- Scale inference (variogram -&gt; L50/L95) ----------------------------------\n# --- SCALE → TUNING → FEATURES @R* → CV → MAPS → PANELS ----------------------\n# add smoothed predictors to both station sets\nadd_drifts_at_R &lt;- function(stn_sf, E, alt, az, R, lc = NULL, lc_levels = NULL) {\n  # R-geglättete Prädiktor-Raster bauen (E*, slp*, cosi*)\n  dr &lt;- smooth_dem_and_derive(E, alt, az, radius_m = R)\n  XY &lt;- sf::st_coordinates(stn_sf)\n  \n  # Extraktion der bei R geglätteten Prädiktoren\n  stn_sf$z_surf &lt;- as.numeric(terra::extract(dr$Es,   XY)[,1])\n  stn_sf$slp    &lt;- as.numeric(terra::extract(dr$slp,  XY)[,1])\n  stn_sf$cosi   &lt;- as.numeric(terra::extract(dr$cosi, XY)[,1])\n  \n  # Optional: Landnutzung als Faktor (nicht geglättet, aber konsistent gemappt)\n  if (!is.null(lc)) {\n    lc_codes &lt;- as.integer(terra::extract(lc, XY)[,1])\n    if (!is.null(lc_levels)) {\n      lc_codes[is.na(lc_codes)] &lt;- 1L\n      lc_codes &lt;- pmax(1L, pmin(lc_codes, length(lc_levels)))\n      stn_sf$lc &lt;- factor(lc_levels[lc_codes], levels = lc_levels)\n    } else {\n      stn_sf$lc &lt;- factor(lc_codes)\n    }\n  }\n  \n  stn_sf\n}\n\n# ---- SF Variogram Scales (ohne sp::) --------------------------------\ncompute_Ls_from_points &lt;- function(stn_sf, value_col = \"temp\",\n                                   maxdist = NULL, nlag = 18, smooth_k = 3) {\n  stopifnot(inherits(stn_sf, \"sf\"), value_col %in% names(stn_sf))\n  pts &lt;- stn_sf[is.finite(stn_sf[[value_col]]), ]\n  if (is.null(maxdist)) {\n    bb &lt;- sf::st_bbox(pts)\n    dom_diag &lt;- sqrt((bb[\"xmax\"]-bb[\"xmin\"])^2 + (bb[\"ymax\"]-bb[\"ymin\"])^2)\n    maxdist &lt;- dom_diag / 2\n  }\n  form &lt;- stats::as.formula(sprintf(\"%s ~ 1\", value_col))\n  vg  &lt;- gstat::variogram(form, data = pts, cutoff = maxdist, width = maxdist/nlag)\n  \n  if (nrow(vg) &gt;= smooth_k) {\n    vg$gamma &lt;- stats::filter(vg$gamma, rep(1/smooth_k, smooth_k), sides = 2)\n    vg$gamma[!is.finite(vg$gamma)] &lt;- zoo::na.approx(vg$gamma, na.rm = FALSE)\n    vg$gamma &lt;- zoo::na.locf(zoo::na.locf(vg$gamma, fromLast = TRUE))\n  }\n  sill &lt;- max(vg$gamma, na.rm = TRUE)\n  if (!is.finite(sill) || sill &lt;= 0) sill &lt;- stats::median(vg$gamma, na.rm = TRUE)\n  \n  L_at_q &lt;- function(q) {\n    thr &lt;- q * sill\n    i   &lt;- which(vg$gamma &gt;= thr)[1]\n    if (is.na(i)) return(NA_real_)\n    if (i == 1) return(vg$dist[1])\n    d0 &lt;- vg$dist[i-1]; d1 &lt;- vg$dist[i]\n    g0 &lt;- vg$gamma[i-1]; g1 &lt;- vg$gamma[i]\n    if (!is.finite(d0) || !is.finite(d1) || g1 == g0) return(d1)\n    d0 + (thr - g0) * (d1 - d0) / (g1 - g0)\n  }\n  list(vg = vg, sill = sill, L50 = L_at_q(0.5), L95 = L_at_q(0.95), cutoff = maxdist)\n}\n\n\n## 1) Skalen (Variogramm) aus den Stationspunkten\nLs14 &lt;- compute_Ls_from_points(stn_sf_14, value_col = \"temp\")\nLs05 &lt;- compute_Ls_from_points(stn_sf_05, value_col = \"temp\")\n\n## (optional) Variogramm-Plots\np_vg14 &lt;- plot_variogram_with_scales(Ls14$vg, Ls14$L50, Ls14$L95, Ls14$sill,\n                                     title = \"T14 — empirical variogram with L50/L95\")\np_vg05 &lt;- plot_variogram_with_scales(Ls05$vg, Ls05$L50, Ls05$L95, Ls05$sill,\n                                     title = \"T05 — empirical variogram with L50/L95\")\n\n## 2) R* via U-Kurve mit GAM@R (geblockte CV; Blockgröße an globalem block_size)\ntune14 &lt;- tune_Rstar_ucurve(\n  stn_sf = stn_sf_14, E = scen$E, alt = sun14$alt, az = sun14$az,\n  L50 = Ls14$L50, L95 = Ls14$L95, block_fallback = block_size, n_grid = 6\n)\ntune05 &lt;- tune_Rstar_ucurve(\n  stn_sf = stn_sf_05, E = scen$E, alt = sun05$alt, az = sun05$az,\n  L50 = Ls05$L50, L95 = Ls05$L95, block_fallback = block_size, n_grid = 6\n)\n\n\n## (optional) U-Kurven plotten\nprint(plot_ucurve(tune14$grid, tune14$R_star, title = \"T14 — U-curve\"))\n\n\n\n\n\n\n\n\n\nCode\nprint(plot_ucurve(tune05$grid, tune05$R_star, title = \"T05 — U-curve\"))\n\n\n\n\n\n\n\n\n\nCode\nmessage(sprintf(\"Chosen R* — T14: %d m | blocks ≈ %d m\", tune14$R_star, tune14$block_m))\nmessage(sprintf(\"Chosen R* — T05: %d m | blocks ≈ %d m\", tune05$R_star, tune05$block_m))\n\n## 3) Feature-Raster @R* (E*, slp*, cosi*)\nfr14 &lt;- smooth_dem_and_derive(scen$E, sun14$alt, sun14$az, radius_m = tune14$R_star)\nfr05 &lt;- smooth_dem_and_derive(scen$E, sun05$alt, sun05$az, radius_m = tune05$R_star)\n\n## 4) Stations-Features @R* (inkl. LC, falls vorhanden)\nstn14_R &lt;- add_drifts_at_R(stn_sf_14, scen$E, sun14$alt, sun14$az, tune14$R_star,\n                           lc = scen$lc, lc_levels = scen$lc_levels)\nstn05_R &lt;- add_drifts_at_R(stn_sf_05, scen$E, sun05$alt, sun05$az, tune05$R_star,\n                           lc = scen$lc, lc_levels = scen$lc_levels)\n\n\n## 5) Block-CV gegen E* (nicht gegen rohes E)\nbench14 &lt;- run_lbo_cv(stn14_R, E = fr14$Es, block_size = block_size, models = models_use)\n\n\n[inverse distance weighted interpolation]\n[using ordinary kriging]\n[using universal kriging]\n[inverse distance weighted interpolation]\n[using ordinary kriging]\n[using universal kriging]\n[inverse distance weighted interpolation]\n[using ordinary kriging]\n[using universal kriging]\n[inverse distance weighted interpolation]\n[using ordinary kriging]\n[using universal kriging]\n[inverse distance weighted interpolation]\n[using ordinary kriging]\n[using universal kriging]\n[inverse distance weighted interpolation]\n[using ordinary kriging]\n[using universal kriging]\n[inverse distance weighted interpolation]\n[using ordinary kriging]\n[using universal kriging]\n[inverse distance weighted interpolation]\n[using ordinary kriging]\n[using universal kriging]\n[inverse distance weighted interpolation]\n[using ordinary kriging]\n[using universal kriging]\n[inverse distance weighted interpolation]\n[using ordinary kriging]\n[using universal kriging]\n[inverse distance weighted interpolation]\n[using ordinary kriging]\n[using universal kriging]\n[inverse distance weighted interpolation]\n[using ordinary kriging]\n[using universal kriging]\n[inverse distance weighted interpolation]\n[using ordinary kriging]\n[using universal kriging]\n[inverse distance weighted interpolation]\n[using ordinary kriging]\n[using universal kriging]\n[inverse distance weighted interpolation]\n[using ordinary kriging]\n[using universal kriging]\n[inverse distance weighted interpolation]\n[using ordinary kriging]\n[using universal kriging]\n[inverse distance weighted interpolation]\n[using ordinary kriging]\n[using universal kriging]\n[inverse distance weighted interpolation]\n[using ordinary kriging]\n[using universal kriging]\n[inverse distance weighted interpolation]\n[using ordinary kriging]\n[using universal kriging]\n[inverse distance weighted interpolation]\n[using ordinary kriging]\n[using universal kriging]\n[inverse distance weighted interpolation]\n[using ordinary kriging]\n[using universal kriging]\n[inverse distance weighted interpolation]\n[using ordinary kriging]\n[using universal kriging]\n\n\nCode\nbench05 &lt;- run_lbo_cv(stn05_R, E = fr05$Es, block_size = block_size, models = models_use)\n\n\n[inverse distance weighted interpolation]\n[using ordinary kriging]\n[using universal kriging]\n[inverse distance weighted interpolation]\n[using ordinary kriging]\n[using universal kriging]\n[inverse distance weighted interpolation]\n[using ordinary kriging]\n[using universal kriging]\n[inverse distance weighted interpolation]\n[using ordinary kriging]\n[using universal kriging]\n[inverse distance weighted interpolation]\n[using ordinary kriging]\n[using universal kriging]\n[inverse distance weighted interpolation]\n[using ordinary kriging]\n[using universal kriging]\n[inverse distance weighted interpolation]\n[using ordinary kriging]\n[using universal kriging]\n[inverse distance weighted interpolation]\n[using ordinary kriging]\n[using universal kriging]\n[inverse distance weighted interpolation]\n[using ordinary kriging]\n[using universal kriging]\n[inverse distance weighted interpolation]\n[using ordinary kriging]\n[using universal kriging]\n[inverse distance weighted interpolation]\n[using ordinary kriging]\n[using universal kriging]\n[inverse distance weighted interpolation]\n[using ordinary kriging]\n[using universal kriging]\n[inverse distance weighted interpolation]\n[using ordinary kriging]\n[using universal kriging]\n[inverse distance weighted interpolation]\n[using ordinary kriging]\n[using universal kriging]\n[inverse distance weighted interpolation]\n[using ordinary kriging]\n[using universal kriging]\n[inverse distance weighted interpolation]\n[using ordinary kriging]\n[using universal kriging]\n[inverse distance weighted interpolation]\n[using ordinary kriging]\n[using universal kriging]\n[inverse distance weighted interpolation]\n[using ordinary kriging]\n[using universal kriging]\n[inverse distance weighted interpolation]\n[using ordinary kriging]\n[using universal kriging]\n[inverse distance weighted interpolation]\n[using ordinary kriging]\n[using universal kriging]\n[inverse distance weighted interpolation]\n[using ordinary kriging]\n[using universal kriging]\n[inverse distance weighted interpolation]\n[using ordinary kriging]\n[using universal kriging]\n\n\nCode\n## 6) Karten mit feature_rasters = {E*, slp*, cosi*}\nmaps14_tuned &lt;- predict_maps(\n  stn_sf = stn14_R, truth_raster = scen$R14, which_time = \"T14\",\n  scen = scen, models = models_use, lc_levels = scen$lc_levels,\n  feature_rasters = list(E = fr14$Es, slp = fr14$slp, cosi = fr14$cosi)\n)\n\n\n[inverse distance weighted interpolation]\n[using ordinary kriging]\n[using universal kriging]\n\n\nCode\nmaps05_tuned &lt;- predict_maps(\n  stn_sf = stn05_R, truth_raster = scen$R05, which_time = \"T05\",\n  scen = scen, models = models_use, lc_levels = scen$lc_levels,\n  feature_rasters = list(E = fr05$Es, slp = fr05$slp, cosi = fr05$cosi)\n)\n\n\n[inverse distance weighted interpolation]\n[using ordinary kriging]\n[using universal kriging]\n\n\nCode\n## 7) Panels (Truth | Predictions | Error/Residuals) – horizontal, gut lesbar\npanel_pages_T14 &lt;- build_panels_truth_preds_errors_paged(\n  maps          = maps14_tuned,      # list with $pred_rasters etc.\n  truth_raster  = scen$R14,\n  cv_tbl        = bench14$cv,\n  which_time    = \"T14\",\n  models_per_page     = 7,            # all models on one page\n  scatter_next_to_truth = TRUE,\n  top_widths           = c(1.1, 0.9), # optional\n  show_second_legend   = FALSE        # keep only one °C legend\n)\n# render the (only) page\npanel_pages_T05 &lt;- build_panels_truth_preds_errors_paged(\n  maps          = maps05_tuned,      # list with $pred_rasters etc.\n  truth_raster  = scen$R05,\n  cv_tbl        = bench05$cv,\n  which_time    = \"T05\",\n  models_per_page     = 7,            # all models on one page\n  scatter_next_to_truth = TRUE,\n  top_widths           = c(1.1, 0.9), # optional\n  show_second_legend   = FALSE        # keep only one °C legend\n)\n# render the (only) page\nprint(panel_pages_T14[[1]])\n\n\n\n\n\n\n\n\n\nCode\nprint(panel_pages_T05[[1]])\n\n\n\n\n\n\n\n\n\nCode\nprint(bench14)\n\n\n$cv\n# A tibble: 360 × 5\n   model      id   obs  pred block_id\n   &lt;chr&gt;   &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt;    &lt;int&gt;\n 1 Voronoi    20  28.6  28.3        1\n 2 Voronoi    45  28.3  28.4        1\n 3 Voronoi    52  28.4  28.3        1\n 4 IDW        20  28.6  28.4        1\n 5 IDW        45  28.3  28.2        1\n 6 IDW        52  28.4  28.2        1\n 7 OK         20  28.6  28.5        1\n 8 OK         45  28.3  28.4        1\n 9 OK         52  28.4  28.3        1\n10 KED        20  28.6  28.8        1\n# ℹ 350 more rows\n\n$metrics\n# A tibble: 6 × 7\n  model       n  n_ok   MAE  RMSE     Bias     R2\n  &lt;chr&gt;   &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt;  &lt;dbl&gt;\n1 KED        60    58 0.285 0.368  0.0280  0.752 \n2 GAM        60    57 0.301 0.464 -0.0535  0.641 \n3 RF         60    57 0.378 0.495 -0.00124 0.545 \n4 IDW        60    60 0.541 0.711  0.0880  0.0316\n5 OK         60    60 0.548 0.716  0.0493  0.0159\n6 Voronoi    60    60 0.667 0.897  0.205   0.0393\n\n$diag_plot\n\n\n\n\n\n\n\n\n\n\n$blocks_plot\n\n\n\n\n\n\n\n\n\nCode\nprint(bench05)\n\n\n$cv\n# A tibble: 360 × 5\n   model      id   obs  pred block_id\n   &lt;chr&gt;   &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt;    &lt;int&gt;\n 1 Voronoi    20  8.60  9.15        1\n 2 Voronoi    45  8.86  9.03        1\n 3 Voronoi    52  9.03  9.15        1\n 4 IDW        20  8.60  8.32        1\n 5 IDW        45  8.86  8.56        1\n 6 IDW        52  9.03  8.80        1\n 7 OK         20  8.60  7.84        1\n 8 OK         45  8.86  8.44        1\n 9 OK         52  9.03  8.77        1\n10 KED        20  8.60 NA           1\n# ℹ 350 more rows\n\n$metrics\n# A tibble: 6 × 7\n  model       n  n_ok    MAE   RMSE    Bias     R2\n  &lt;chr&gt;   &lt;int&gt; &lt;int&gt;  &lt;dbl&gt;  &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt;\n1 GAM        60    43  0.370  0.462  0.0530  0.881\n2 RF         60    43  0.369  0.561  0.0180  0.834\n3 OK         60    60  0.624  0.933 -0.0671  0.424\n4 IDW        60    60  0.792  1.02  -0.127   0.358\n5 Voronoi    60    60  0.728  1.17  -0.165   0.346\n6 KED        60     0 NA     NA     NA      NA    \n\n$diag_plot\n\n\n\n\n\n\n\n\n\n\n$blocks_plot\n\n\n\n\n\n\n\n\n\n\n17.1.1 Reading the outputs\n\nVariogram: dotted sill; dashed L50/L95 → scale anchors for smoothing and block sizes.\n\nU-curve: R* at lowest blocked-CV RMSE; include R = 0 so the tuner can prefer the raw drift.\n\nBenchmark: compare OK / KED / GAM / RF / IDW / Voronoi under the same blocked CV; document block size and R*.\n\nError budget (illustrative): OK → KED(base) → KED(R*) shows gains from drift and from scale matching.\n\n\nFrom concept to practice (pipeline mapping).\n\nEstimate scales: variogram \\(\\rightarrow\\) \\(\\sigma_{\\text{proc}}^2\\), \\(L_{50}\\), \\(L_{95}\\).\nCouple scales: smooth predictors / choose grids according to \\(R_{\\text{micro}}\\), \\(R_{\\text{local}}\\).\nTune \\(R^*\\): block‑CV, U‑curve \\(\\rightarrow\\) stable drift radius.\nBenchmark methods: compare OK/KED/GAM/RF/Trend/IDW/Voronoi at \\(R^*\\) (RMSE/MAE/Bias, document block size).\nProducts: write maps/grids at \\(R^*\\) (and optionally \\(L_{95}\\)); report the error budget.\n\n\n\nKey takeaway: The “smartest” algorithm doesn’t win — the one whose scale matches the process does.\n\n\n\n17.1.2 I.5 Reading the outputs (tables & plots)\nThis section explains how to interpret the key tables and figures produced by the pipeline and how to turn them into a model choice and a scale statement.\n\n17.1.2.1 1) Variogram & scale table (chunk scale-Ls)\n\nWhat you see: Empirical variogram points/line, horizontal dotted line at the (structural) sill, and vertical dashed lines at L50 and L95.\nHow to read it:\n\nNugget (near‑zero intercept) ≈ measurement/microscale noise. A large nugget means close points differ substantially; no method can beat this noise floor.\nSill (plateau) ≈ total variance once pairs are effectively uncorrelated.\nL50 / L95 ≈ pragmatic correlation distances (half vs. ~all structure spent). They are your scale anchors for smoothing radii, neighborhood ranges, and CV block sizes.\n\nQuality checks:\n\nIf no clear plateau: trend/non‑stationarity is likely → consider a drift (elev/sun terms) or a larger domain.\nIf L95 is near the domain size: scales are long; block sizes should be generous to avoid leakage.\nIf the variogram is noisy at large lags: rely more on L50 and the U‑curve outcome.\n\n\n\n\n17.1.2.2 2) U‑curve for tuned drift (chunk scale-tune)\n\nWhat you see: A line plot of RMSE vs. smoothing radius R for KED under blocked CV.\nDecision rule: R* is the radius with the lowest CV‑RMSE.\nWhat shapes mean:\n\nLeft side high (too small R): drift carries microscale noise → overfitting → higher CV error.\nRight side high (too large R): drift is oversmoothed → loses meaningful gradient → bias ↑.\nFlat bottom/plateau: a range of R values are equivalent → pick the smallest R on the plateau for parsimony.\n\nEdge cases: If the minimum sits at the search boundary, widen the R grid and re‑run; if still at the boundary, the field may be trend‑dominated or the covariate is weak.\n\n\n\n17.1.2.3 3) LBO‑CV metrics table (res$metrics)\nFor each model (Voronoi, IDW, OK, KED, GAM, RF) we report:\n\nRMSE (primary): square‑error penalty; most sensitive to outliers. Use this to rank models.\nMAE: median‑like robustness; a useful tie‑breaker alongside RMSE.\nBias (mean error): systematic over/under‑prediction; prefer |Bias| close to 0.\nR²: variance explained in held‑out blocks; interpret cautiously under spatial CV.\nn: number of held‑out predictions contributing.\n\nChoosing a winner:\n\nRank by lowest RMSE under the tuned configuration.\nIf RMSEs are within ~5–10%: prefer the model with lower MAE, lower |Bias|, and more stable block‑wise errors (see next point).\nIf KED (R*) ≈ OK: the drift adds little; the covariate is weak or the process is long‑range. If GAM/RF wins, the relationship is nonlinear or interaction‑rich.\n\n\n\n17.1.2.4 4) Block‑wise diagnostics\n\nBlock error boxes/scatter: Look for narrow distributions (stable across space). Large spread or outliers indicate location‑dependent performance.\nStability index (optional): CV_rmse = sd(RMSE_block) / mean(RMSE_block). Values &lt; 0.25 are typically stable; &gt; 0.4 suggests uneven performance.\nObs vs Pred scatter: Slope ~1 and tight cloud = good calibration; bowed patterns imply bias or missing drift terms.\n\n\n\n17.1.2.5 5) Error budget table (make_simple_error_budget)\nThree rows show how error decreases as structure is added and matched:\n\nBaseline (OK): no drift; sets a structure‑free reference.\nAdd drift (KED base): uses raw covariate; improvement here quantifies signal in the covariate.\nScale‑match drift (KED R*): covariate smoothed at R*; additional gain isolates scale alignment. The Gain_vs_prev column is the incremental improvement at each step.\n\n\nIf KED base ~ KED R*, scale matching adds little (either the raw drift is already at a compatible scale, or the field is insensitive to R). If OK &gt; KED base, the covariate may inject noise or the drift term is mis‑specified.\n\n\n\n\n17.1.3 I.6 Deciding on the best model (and documenting the scale)\nUse this practical, auditable rule set:\n\nPrimary criterion: Lowest CV‑RMSE under blocked CV.\nTie‑breakers: Lower MAE, smaller |Bias|, and better block‑stability.\nParsimony: If multiple models tie, choose the simplest (OK/KED &lt; GAM &lt; RF).\nScale sanity check: Report L50/L95 and verify that R* lies roughly in [L50, 1.5·L95]. If not, discuss why (e.g., strong trend, weak covariate, anisotropy).\nReproducibility: Record the block size, R grid, winning R*, and the full metrics table.\n\n\n\n17.1.4 I.7 Typical patterns & what they imply\n\nHigh nugget, short L50: Expect modest absolute accuracy; prefer coarser R and conservative models. IDW/OK with tight neighborhoods can perform on par with KED.\nLong L95, clear sill: Favor larger neighborhoods and smoother drifts; KED (R*) often dominates.\nGAM/RF &gt; KED: Nonlinear covariate effects or interactions (e.g., slope×aspect). Still align covariates to R* to avoid noise chasing.\nOK ~ KED: Elevation (or chosen drift) is weak for this synthetic setup; consider enriching covariates (slope/aspect/TRI) at matched scales.\n\n\n\n17.1.5 I.8 Checklist before you trust the numbers\n\nBlock size reflects correlation scale (≈ L95).\nU‑curve scanned a broad enough R range; minimum not at boundary.\nR* reported along with L50/L95.\nWinner chosen by blocked CV (not random folds).\nBias near zero; residuals pattern‑free in space.\nFigures/tables archived for reproducibility.\n\n\n\nCode\n# ====================== EXPORT: Plots, Tabellen, Raster ======================\n# Ordnerstruktur\nout_dir &lt;- \"exports\"\nfig_dir &lt;- file.path(out_dir, \"figs\")\ntab_dir &lt;- file.path(out_dir, \"tables\")\nras_dir &lt;- file.path(out_dir, \"rasters\")\ndat_dir &lt;- file.path(out_dir, \"data\")\ndir.create(out_dir, showWarnings = FALSE)\nfor (d in c(fig_dir, tab_dir, ras_dir, dat_dir)) dir.create(d, showWarnings = FALSE)\n\n# Helfer\nsafe_save_plot &lt;- function(p, file, w = 9, h = 6, dpi = 300) {\n  if (!is.null(p) && inherits(p, c(\"gg\",\"ggplot\",\"patchwork\"))) {\n    try(ggplot2::ggsave(filename = file, plot = p, width = w, height = h,\n                        dpi = dpi, bg = \"white\"), silent = TRUE)\n  }\n}\nsafe_write_csv &lt;- function(x, file) {\n  if (!is.null(x)) try(utils::write.csv(x, file, row.names = FALSE), silent = TRUE)\n}\nsafe_save_kable &lt;- function(tab, file_html, self_contained = TRUE) {\n  # normalize and create the parent directory if needed\n  out_path &lt;- normalizePath(file_html, winslash = \"/\", mustWork = FALSE)\n  out_dir  &lt;- dirname(out_path)\n  if (!dir.exists(out_dir)) dir.create(out_dir, recursive = TRUE, showWarnings = FALSE)\n\n  # try saving via kableExtra; on failure, write the raw HTML as a fallback\n  tryCatch(\n    {\n      kableExtra::save_kable(tab, out_path, self_contained = self_contained)\n      message(\"Saved table: \", out_path)\n    },\n    error = function(e) {\n      warning(\"save_kable failed (\", conditionMessage(e), \"). Writing raw HTML fallback.\")\n      cat(as.character(tab), file = out_path)\n      message(\"Fallback table written: \", out_path)\n    }\n  )\n\n  invisible(out_path)\n}\n\n\n\n\n# ====================== EXPORT: Plots, Tabellen, Raster ======================\n# Ordnerstruktur\n\nout_dir &lt;- \"block4_5/exports\"\nfig_dir &lt;- file.path(out_dir, \"figs\")\ntab_dir &lt;- file.path(out_dir, \"tables\")\nras_dir &lt;- file.path(out_dir, \"rasters\")\ndat_dir &lt;- file.path(out_dir, \"data\")\ndir.create(out_dir, showWarnings = FALSE)\nfor (d in c(fig_dir, tab_dir, ras_dir, dat_dir)) dir.create(d, showWarnings = FALSE)\n\n# Helfer\nsafe_save_plot &lt;- function(p, file, w = 9, h = 6, dpi = 300) {\n  if (!is.null(p) && inherits(p, c(\"gg\",\"ggplot\",\"patchwork\"))) {\n    try(ggplot2::ggsave(filename = file, plot = p, width = w, height = h,\n                        dpi = dpi, bg = \"white\"), silent = TRUE)\n  }\n}\nsafe_write_csv &lt;- function(x, file) {\n  if (!is.null(x)) try(utils::write.csv(x, file, row.names = FALSE), silent = TRUE)\n}\nsafe_save_kable &lt;- function(df, file_html, caption = NULL) {\n  if (!is.null(df) && requireNamespace(\"kableExtra\", quietly = TRUE)) {\n    tab &lt;- knitr::kable(df, digits = 3, caption = caption, format = \"html\")\n    kableExtra::save_kable(tab, file_html, self_contained = TRUE)\n  }\n}\n\n# ---------- Plots sammeln & speichern ----------\n# ---- Plots sicher speichern (fixte Referenzen) ----\nplots &lt;- list(\n  # Panels (tuned)\n  \"T14_panel_tuned.png\"  = if (exists(\"panel14_tuned\")) panel14_tuned else NULL,\n  \"T05_panel_tuned.png\"  = if (exists(\"panel05_tuned\")) panel05_tuned else NULL,\n  \n  # LBO-CV Übersicht (BLOCKS + DIAG)\n  \"T14_blocks.png\"       = if (exists(\"out14\")) out14$res$blocks_plot else NULL,\n  \"T14_diag.png\"         = if (exists(\"out14\")) out14$res$diag_plot   else NULL,\n  \"T05_blocks.png\"       = if (exists(\"out05\")) out05$res$blocks_plot else NULL,\n  \"T05_diag.png\"         = if (exists(\"out05\")) out05$res$diag_plot   else NULL,\n  \n  # Truth/Pred (baseline)\n  \"T14_truth.png\"        = if (exists(\"out14\")) out14$maps$p_truth else NULL,\n  \"T14_pred.png\"         = if (exists(\"out14\")) out14$maps$p_pred  else NULL,\n  \"T05_truth.png\"        = if (exists(\"out05\")) out05$maps$p_truth else NULL,\n  \"T05_pred.png\"         = if (exists(\"out05\")) out05$maps$p_pred  else NULL,\n  \n  # Truth/Pred (tuned @ R*)\n  \"T14_truth_TUNED.png\"  = if (exists(\"maps14_tuned\")) maps14_tuned$p_truth else NULL,\n  \"T14_pred_TUNED.png\"   = if (exists(\"maps14_tuned\")) maps14_tuned$p_pred  else NULL,\n  \"T05_truth_TUNED.png\"  = if (exists(\"maps05_tuned\")) maps05_tuned$p_truth else NULL,\n  \"T05_pred_TUNED.png\"   = if (exists(\"maps05_tuned\")) maps05_tuned$p_pred  else NULL,\n  \n  # Boxplots, Scatter, Density (aus CV)\n  \"T14_block_box.png\"    = if (exists(\"p_block_box14\"))  p_block_box14  else NULL,\n  \"T14_abserr_box.png\"   = if (exists(\"p_abserr_box14\")) p_abserr_box14 else NULL,\n  \"T05_block_box.png\"    = if (exists(\"p_block_box05\"))  p_block_box05  else NULL,\n  \"T05_abserr_box.png\"   = if (exists(\"p_abserr_box05\")) p_abserr_box05 else NULL,\n  \"T14_obs_pred.png\"     = if (exists(\"p_scatter14\")) p_scatter14 else NULL,\n  \"T05_obs_pred.png\"     = if (exists(\"p_scatter05\")) p_scatter05 else NULL,\n  \"T14_resid_density.png\"= if (exists(\"p_dens14\"))    p_dens14    else NULL,\n  \"T05_resid_density.png\"= if (exists(\"p_dens05\"))    p_dens05    else NULL,\n  \n  # Variogramme & U-Kurven\n  \"T14_variogram.png\"    = if (exists(\"p_vg14\")) p_vg14 else NULL,\n  \"T05_variogram.png\"    = if (exists(\"p_vg05\")) p_vg05 else NULL,\n  \"T14_ucurve.png\"       = if (exists(\"p_uc14\")) p_uc14 else NULL,\n  \"T05_ucurve.png\"       = if (exists(\"p_uc05\")) p_uc05 else NULL,\n  \n  # Landuse / Terrain-Overview aus Teil 1\n  \"landcover_vertical.png\" = if (exists(\"p_landcover_vert\")) p_landcover_vert else NULL,\n  \"overview_2x2.png\"       = if (exists(\"p_overview2x2\"))    p_overview2x2    else NULL\n)\n\nfor (nm in names(plots)) safe_save_plot(plots[[nm]], file.path(fig_dir, nm), w = 9, h = 6, dpi = 300)\n\n# ---------- Tabellen & Daten exportieren ----------\n# CV-Punktvorhersagen\nif (exists(\"out14\")) safe_write_csv(out14$res$cv, file.path(dat_dir, \"cv_points_T14.csv\"))\nif (exists(\"out05\")) safe_write_csv(out05$res$cv, file.path(dat_dir, \"cv_points_T05.csv\"))\n\n# Grid-Vorhersagen (Truth/Pred) – “pred_df” der Map-Objekte\nif (exists(\"out14\")) safe_write_csv(out14$maps$pred_df, file.path(dat_dir, \"grid_pred_T14.csv\"))\nif (exists(\"out05\")) safe_write_csv(out05$maps$pred_df, file.path(dat_dir, \"grid_pred_T05.csv\"))\nif (exists(\"maps14_tuned\")) safe_write_csv(maps14_tuned$pred_df, file.path(dat_dir, \"grid_pred_T14_TUNED.csv\"))\nif (exists(\"maps05_tuned\")) safe_write_csv(maps05_tuned$pred_df, file.path(dat_dir, \"grid_pred_T05_TUNED.csv\"))\n\n# Metriken\nif (exists(\"out14\")) {\n  safe_write_csv(out14$res$metrics, file.path(tab_dir, \"metrics_T14_base.csv\"))\n  safe_save_kable(out14$res$metrics, file.path(tab_dir, \"metrics_T14_base.html\"), \"LBO-CV metrics — T14\")\n}\nif (exists(\"out05\")) {\n  safe_write_csv(out05$res$metrics, file.path(tab_dir, \"metrics_T05_base.csv\"))\n  safe_save_kable(out05$res$metrics, file.path(tab_dir, \"metrics_T05_base.html\"), \"LBO-CV metrics — T05\")\n}\nif (exists(\"bench14\")) {\n  safe_write_csv(bench14$metrics, file.path(tab_dir, \"metrics_T14_tuned.csv\"))\n  safe_save_kable(bench14$metrics, file.path(tab_dir, \"metrics_T14_tuned.html\"), \"Metrics — tuned @ R* (T14)\")\n}\nif (exists(\"bench05\")) {\n  safe_write_csv(bench05$metrics, file.path(tab_dir, \"metrics_T05_tuned.csv\"))\n  safe_save_kable(bench05$metrics, file.path(tab_dir, \"metrics_T05_tuned.html\"), \"Metrics — tuned @ R* (T05)\")\n}\n\n# Skalen & R*\nif (exists(\"tune14\") && exists(\"Ls14\")) {\n  safe_write_csv(tune14$table, file.path(tab_dir, \"Ucurve_T14.csv\"))\n  safe_write_csv(data.frame(L50 = Ls14$L50, L95 = Ls14$L95, R_star = tune14$R_star),\n                 file.path(tab_dir, \"scales_T14.csv\"))\n}\nif (exists(\"tune05\") && exists(\"Ls05\")) {\n  safe_write_csv(tune05$table, file.path(tab_dir, \"Ucurve_T05.csv\"))\n  safe_write_csv(data.frame(L50 = Ls05$L50, L95 = Ls05$L95, R_star = tune05$R_star),\n                 file.path(tab_dir, \"scales_T05.csv\"))\n}\n\n# Optional: Error-Budget (falls berechnet)\nif (exists(\"eb14\")) safe_write_csv(eb14, file.path(tab_dir, \"error_budget_T14.csv\"))\nif (exists(\"eb05\")) safe_write_csv(eb05, file.path(tab_dir, \"error_budget_T05.csv\"))\n\n# ---------- Raster exportieren ----------\nif (exists(\"scen\")) {\n  try(terra::writeRaster(scen$E,   file.path(ras_dir, \"E_dem.tif\"),   overwrite = TRUE), silent = TRUE)\n  try(terra::writeRaster(scen$R14, file.path(ras_dir, \"R14_truth.tif\"), overwrite = TRUE), silent = TRUE)\n  try(terra::writeRaster(scen$R05, file.path(ras_dir, \"R05_truth.tif\"), overwrite = TRUE), silent = TRUE)\n  if (\"lc\" %in% names(scen)) try(terra::writeRaster(scen$lc, file.path(ras_dir, \"landcover.tif\"),\n                                                    overwrite = TRUE), silent = TRUE)\n}\nif (exists(\"bench14\") && \"E_star\" %in% names(bench14))\n  try(terra::writeRaster(bench14$E_star, file.path(ras_dir, \"E_star_T14.tif\"), overwrite = TRUE), silent = TRUE)\nif (exists(\"bench05\") && \"E_star\" %in% names(bench05))\n  try(terra::writeRaster(bench05$E_star, file.path(ras_dir, \"E_star_T05.tif\"), overwrite = TRUE), silent = TRUE)\n\n# ---------- Sessioninfo als Referenz ----------\ntry(saveRDS(sessionInfo(), file.path(out_dir, \"sessionInfo.rds\")), silent = TRUE)\n\nmessage(\"✔ Export fertig. Siehe Ordner: \", normalizePath(out_dir))\n# ========================================================================"
  },
  {
    "objectID": "block4_5/mc_2025_tec.html",
    "href": "block4_5/mc_2025_tec.html",
    "title": "Microclimate Sensors & Power-Supply Units",
    "section": "",
    "text": "Froggit shop [DE]\n  \n  \n    \n     Ecowitt shop [US]\n  \n  \n    \n     Fine Offset\n  \n\n      \nThe sensors from Fine Offset are re-branded and partly modified by the resellers. This article deals with sensors from the german re-seller froggit and the US re-seller ecowitt. More precise the DP-/GW SmartHubs WiFi Gateway with temperature, humidity & Pressure which is developed by fine offset. The unique selling point of the LoRa-Wifi gateway is the extraordinarily extensive possibility of connecting radio-bound sensors.",
    "crumbs": [
      "Block 4: Microclimate Data Retrieval & Spatial Data Interpolation",
      "Microclimate Sensors & Power-Supply Units"
    ]
  },
  {
    "objectID": "block4_5/mc_2025_tec.html#calibration-concept",
    "href": "block4_5/mc_2025_tec.html#calibration-concept",
    "title": "Microclimate Sensors & Power-Supply Units",
    "section": "Calibration Concept",
    "text": "Calibration Concept\nThe low budget sensors are usually lacking of a stable measurement quality. To obtain reliable micro climate data a two step calibration process is suggested.\n\nThe measurements of all sensors (preferably in a climate chamber) will be statistically analysed to identify sensor which produce systematic and significant outliers.\nThe sensors are calibrated against an operational running high price reference station in the field.",
    "crumbs": [
      "Block 4: Microclimate Data Retrieval & Spatial Data Interpolation",
      "Microclimate Sensors & Power-Supply Units"
    ]
  },
  {
    "objectID": "index.html#sensors-equipment",
    "href": "index.html#sensors-equipment",
    "title": "Welcome EON Summer School 2025 (31.08-05.09.2025)",
    "section": "Sensors & Equipment",
    "text": "Sensors & Equipment\n\nHAWK:\n\nMavic 3 (Enterprise + Thermal + Multispectral)\nMatrics RTK350 incl. L1, P1 & Micasence\nGNSS (Emlid, Garmin)\nTablets (Android)\nForest Measurement Devices (diameter tapes, calipers, vertex, laser range finders, …)\nGeoSlam Mobile Laser Scanner\n\nUni Münster:\n\nDrill & Drop\nDii mini\n\nUni Marburg:\n\nMavic 3 (Enterprise + Thermal + Multispectral)\nMavic 3 Mini Pro\nLoRa based real time climate sensors",
    "crumbs": [
      "Welcome",
      "Welcome EON Summer School 2025 (31.08-05.09.2025)"
    ]
  }
]