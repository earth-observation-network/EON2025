[
  {
    "objectID": "ml_session/ML_AOA.html",
    "href": "ml_session/ML_AOA.html",
    "title": "Machine learning for remote sensing applications",
    "section": "",
    "text": "Slides and data are on https://github.com/earth-observation-network/EON2025/tree/main/ml_session",
    "crumbs": [
      "Block 3: Machine Learning",
      "Machine learning for remote sensing applications"
    ]
  },
  {
    "objectID": "ml_session/ML_AOA.html#how-to-start",
    "href": "ml_session/ML_AOA.html#how-to-start",
    "title": "Machine learning for remote sensing applications",
    "section": "How to start",
    "text": "How to start\nFor this tutorial we need the terra package for processing of the satellite data as well as the caret package as a wrapper for machine learning (here: randomForest) algorithms. Sf is used for handling of the training data available as vector data (polygons). Mapview is used for spatial visualization of the data. CAST will be used to account for spatial dependencies during model validation as well as for the estimation of the AOA.\n\nlibrary(CAST)\nlibrary(terra)\nlibrary(caret)\nlibrary(randomForest)\nlibrary(mapview)\nlibrary(sf)\nlibrary(CAST)\nlibrary(tmap)",
    "crumbs": [
      "Block 3: Machine Learning",
      "Machine learning for remote sensing applications"
    ]
  },
  {
    "objectID": "ml_session/ML_AOA.html#data-preparation",
    "href": "ml_session/ML_AOA.html#data-preparation",
    "title": "Machine learning for remote sensing applications",
    "section": "Data preparation",
    "text": "Data preparation\nTo start with, let’s load and explore the remote sensing raster data as well as the vector data that include the training sites.\n\nRaster data (predictor variables)\n\nmof_sen &lt;- rast(\"data/sentinel_uniwald.grd\")\nprint(mof_sen)\n\nclass       : SpatRaster \ndimensions  : 522, 588, 10  (nrow, ncol, nlyr)\nresolution  : 10, 10  (x, y)\nextent      : 474200, 480080, 5629540, 5634760  (xmin, xmax, ymin, ymax)\ncoord. ref. : +proj=utm +zone=32 +datum=WGS84 +units=m +no_defs \nsource      : sentinel_uniwald.grd \nnames       : T32UM~1_B02, T32UM~1_B03, T32UM~1_B04, T32UM~1_B05, T32UM~1_B06, T32UM~1_B07, ... \nmin values  :         723,         514,         294,    341.8125,    396.9375,    440.8125, ... \nmax values  :        8325,        9087,       13810,   7368.7500,   8683.8125,   9602.3125, ... \n\n\nThe raster data contain a subset of the optical data from Sentinel-2 (see band information here: https://en.wikipedia.org/wiki/Sentinel-2) given in scaled reflectances (B02-B11). In addition,the NDVI was calculated. Let’s plot the data to get an idea how the variables look like.\n\nplot(mof_sen)\n\n\n\n\n\n\n\nplotRGB(mof_sen,r=3,g=2,b=1,stretch=\"lin\")\n\n\n\n\n\n\n\n\n\n\nVector data (Response variable)\nThe vector file is read as sf object. It contains the training sites that will be regarded here as a ground truth for the land cover classification.\n\ntrainSites &lt;- read_sf(\"data/trainingsites_LUC.gpkg\")\n\nUsing mapview we can visualize the aerial image channels in the geographical context and overlay it with the polygons. Click on the polygons to see which land cover class is assigned to a respective polygon.\n\nmapview(mof_sen[[1]], map.types = \"Esri.WorldImagery\") +\n  mapview(trainSites)\n\n\n\n\n\n\n\nDraw training samples and extract raster information\nIn order to train a machine learning model between the spectral properties and the land cover class, we first need to create a data frame that contains the predictor variables at the location of the training sites as well as the corresponding class information. However, using each pixel overlapped by a polygon would lead to a overly huge dataset, therefore, we first draw training samples from the polygon. Let’s use 1000 randomly sampled (within the polygons) pixels as training data set.\n\ntrainlocations &lt;- st_sample(trainSites,1000)\ntrainlocations &lt;- st_join(st_sf(trainlocations), trainSites)\nmapview(trainlocations)\n\n\n\n\n\nNext, we can extract the raster values for these locations. The resulting data frame contains the predictor variables for each training location that we can merged with the information on the land cover class from the sf object.\n\ntrainDat &lt;- extract(mof_sen, trainlocations, df=TRUE)\ntrainDat &lt;- data.frame(trainDat, trainlocations)\nhead(trainDat)\n\n  ID T32UMB_20170510T103031_B02 T32UMB_20170510T103031_B03\n1  1                       1243                       1169\n2  2                        885                        784\n3  3                        827                        785\n4  4                        915                        848\n5  5                        804                        689\n6  6                        923                       1054\n  T32UMB_20170510T103031_B04 T32UMB_20170510T103031_B05\n1                        926                   1143.500\n2                        568                    935.000\n3                        501                   1077.750\n4                        731                   1147.438\n5                        445                    901.500\n6                        733                   1260.812\n  T32UMB_20170510T103031_B06 T32UMB_20170510T103031_B07\n1                   2252.312                   2552.812\n2                   2242.188                   2766.938\n3                   2148.500                   2483.250\n4                   1893.750                   2129.688\n5                   1747.125                   2002.500\n6                   3734.625                   4639.625\n  T32UMB_20170510T103031_B08 T32UMB_20170510T103031_B11\n1                       2925                   1878.812\n2                       2644                   1523.062\n3                       2385                   1359.500\n4                       2114                   1788.312\n5                       1812                   1183.062\n6                       4769                   1205.312\n  T32UMB_20170510T103031_B12      NDVI id  LN   Type           trainlocations\n1                  1303.5625 0.5190859 64 503 Wasser POINT (476609.7 5632431)\n2                   824.1250 0.6463263 NA 106 Felder POINT (478523.8 5631097)\n3                   635.2500 0.6528066 NA   2  Buche POINT (477490.4 5632300)\n4                  1003.6250 0.4861160 NA   2  Buche POINT (477605.7 5631966)\n5                   558.5625 0.6056712 NA   1  Eiche POINT (476782.9 5632857)\n6                   523.8125 0.7335514 NA 102 Felder POINT (478600.8 5631513)",
    "crumbs": [
      "Block 3: Machine Learning",
      "Machine learning for remote sensing applications"
    ]
  },
  {
    "objectID": "ml_session/ML_AOA.html#model-training",
    "href": "ml_session/ML_AOA.html#model-training",
    "title": "Machine learning for remote sensing applications",
    "section": "Model training",
    "text": "Model training\n\nPredictors and response\nFor model training we need to define the predictor and response variables. As predictors we can use basically all information from the raster stack as we might assume they could all be meaningful for the differentiation between the land cover classes. As response variable we use the “Label” column of the data frame.\n\npredictors &lt;- names(mof_sen)\nresponse &lt;- \"Type\"\n\n\n\nA first “default” model\nWe then train a Random Forest model to lean how the classes can be distinguished based on the predictors (note: other algorithms would work as well. See https://topepo.github.io/caret/available-models.html for a list of algorithms available in caret). Caret’s train function is doing this job.\nSo let’s see how we can then train a “default” random forest model. We specify “rf” as method, indicating that a Random Forest is applied. We reduce the number of trees (ntree) to 75 to speed things up. Note that usually a larger number (&gt;250) is appropriate.\n\nmodel &lt;- train(trainDat[,predictors],\n               trainDat[,response],\n               method=\"rf\",\n               ntree=75)\nmodel\n\nRandom Forest \n\n1000 samples\n  10 predictor\n  10 classes: 'Buche', 'Duglasie', 'Eiche', 'Felder', 'Fichte', 'Laerche', 'Siedlung', 'Strasse', 'Wasser', 'Wiese' \n\nNo pre-processing\nResampling: Bootstrapped (25 reps) \nSummary of sample sizes: 1000, 1000, 1000, 1000, 1000, 1000, ... \nResampling results across tuning parameters:\n\n  mtry  Accuracy   Kappa    \n   2    0.8309292  0.7895740\n   6    0.8341677  0.7942385\n  10    0.8271947  0.7855348\n\nAccuracy was used to select the optimal model using the largest value.\nThe final value used for the model was mtry = 6.\n\n\nTo perform the classification we can then use the trained model and apply it to each pixel of the raster stack using the predict function.\n\nprediction &lt;- predict(mof_sen,model)\n\nThen we can then create a map with meaningful colors of the predicted land cover using the tmap package.\n\ncols &lt;- rev(c(\"palegreen\", \"blue\", \"grey\", \"red\", \"lightgreen\", \"forestgreen\", \"beige\",\"brown\",\"darkgreen\",\"yellowgreen\"))\n\ntm_shape(prediction) +\n  tm_raster(palette = cols,title = \"LUC\")+\n  tm_scale_bar(bg.color=\"white\",bg.alpha=0.75)+\n  tm_layout(legend.bg.color = \"white\",\n            legend.bg.alpha = 0.75)\n\n\n\n\n\n\n\n\nBased on this we can now discuss more advanced aspects of cross-validation for performance assessment as well as spatial variable selection strategies.\n\n\nModel training with spatial CV and variable selection\nBefore starting model training we can specify some control settings using trainControl. For hyperparameter tuning (mtry) as well as for error assessment we use a spatial cross-validation. Here, the training data are split into 5 folds by trying to resemble the geographic distance distribution required when predicting the entire area from the trainign data,\n\nindices &lt;- knndm(trainlocations,mof_sen,k=5)\ngd &lt;- geodist(trainlocations,mof_sen,cvfolds = indices$indx_train)\nplot(gd)+ scale_x_log10(labels=round)\n\n\n\n\n\n\n\nctrl &lt;- trainControl(method=\"cv\", \n                     index = indices$indx_train,\n                     indexOut = indices$indx_test,\n                     savePredictions = TRUE)\n\nModel training is then again performed using caret’s train function. However we use a wrapper around it that is selecting the predictor variables which are relevant for making predictions to new spatial locations (forward feature selection, fss). We use the Kappa index as metric to select the best model.\n\n# train the model\nset.seed(100)\nmodel &lt;- ffs(trainDat[,predictors],\n             trainDat[,response],\n             method=\"rf\",\n             metric=\"Kappa\",\n             trControl=ctrl,\n             importance=TRUE,\n             ntree=100,\n             verbose=FALSE)\n\n\nprint(model)\n\nSelected Variables: \nT32UMB_20170510T103031_B05 T32UMB_20170510T103031_B06 T32UMB_20170510T103031_B02 T32UMB_20170510T103031_B11 T32UMB_20170510T103031_B12\n---\nRandom Forest \n\n1000 samples\n   5 predictor\n  10 classes: 'Buche', 'Duglasie', 'Eiche', 'Felder', 'Fichte', 'Laerche', 'Siedlung', 'Strasse', 'Wasser', 'Wiese' \n\nNo pre-processing\nResampling: Cross-Validated (10 fold) \nSummary of sample sizes: 768, 791, 712, 854, 875 \nResampling results across tuning parameters:\n\n  mtry  Accuracy   Kappa    \n  2     0.7562707  0.5637556\n  3     0.7492029  0.5575426\n  5     0.7344652  0.5369017\n\nKappa was used to select the optimal model using the largest value.\nThe final value used for the model was mtry = 2.\n\nplot(varImp(model))\n\n\n\n\n\n\n\n\n\n\nModel validation\nWhen we print the model (see above) we get a summary of the prediction performance as the average Kappa and Accuracy of the three spatial folds. Looking at all cross-validated predictions together we can get the “global” model performance.\n\n# get all cross-validated predictions:\ncvPredictions &lt;- model$pred[model$pred$mtry==model$bestTune$mtry,]\n# calculate cross table:\ntable(cvPredictions$pred,cvPredictions$obs)\n\n          \n           Buche Duglasie Eiche Felder Fichte Laerche Siedlung Strasse Wasser\n  Buche        4        0     2      2     11       0        0       0      0\n  Duglasie     0        0     0      1      3       0        0       0      0\n  Eiche        0        0     1      2      2       0        0       0      0\n  Felder       6        0     0    259      0       0        0       6      0\n  Fichte       0        0     0      3      0       0        0       0      0\n  Laerche      0        0     0      0      0       0        0       0      0\n  Siedlung     0        0     0      0      0       0        0       0      0\n  Strasse      0        0     0     16      0       0        0      25      0\n  Wasser       0        0     0      2      3       0        0       0      0\n  Wiese        0        0     0     30      0       0        0       9      0\n          \n           Wiese\n  Buche        0\n  Duglasie     0\n  Eiche        1\n  Felder       6\n  Fichte       0\n  Laerche      0\n  Siedlung     0\n  Strasse      8\n  Wasser       0\n  Wiese       78\n\n\n\n\nVisualize the final model predictions\n\nprediction &lt;- predict(mof_sen,model)\ncols &lt;- rev(c(\"palegreen\", \"blue\", \"grey\", \"red\", \"lightgreen\", \"forestgreen\", \"beige\",\"brown\",\"darkgreen\",\"yellowgreen\"))\n\ntm_shape(prediction) +\n  tm_raster(palette = cols,title = \"LUC\")+\n  tm_scale_bar(bg.color=\"white\",bg.alpha=0.75)+\n  tm_layout(legend.bg.color = \"white\",\n            legend.bg.alpha = 0.75)",
    "crumbs": [
      "Block 3: Machine Learning",
      "Machine learning for remote sensing applications"
    ]
  },
  {
    "objectID": "ml_session/ML_AOA.html#area-of-applicability",
    "href": "ml_session/ML_AOA.html#area-of-applicability",
    "title": "Machine learning for remote sensing applications",
    "section": "Area of Applicability",
    "text": "Area of Applicability\nWe have seen that technically, the trained model can be applied to the entire area of interest (and beyond…as long as the sentinel predictors are available which they are, even globally). But we should assess if we SHOULD apply our model to the entire area. The model should only be applied to locations that feature predictor properties that are comparable to those of the training data. If dissimilarity to the training data is larger than the dissimmilarity within the training data, the model should not be applied to this location.\n\nAOA &lt;- aoa(mof_sen,model,LPD=TRUE, verbose=FALSE)\nplot(AOA$AOA)\n\n\n\n\n\n\n\n\nThe result of the aoa function has two layers: the dissimilarity index (DI) and the area of applicability (AOA). The DI can take values from 0 to Inf, where 0 means that a location has predictor properties that are identical to properties observed in the training data. With increasing values the dissimilarity increases. The AOA has only two values: 0 and 1. 0 means that a location is outside the area of applicability, 1 means that the model is inside the area of applicability. As an option, we cal also calculate the Local Point Density (LPD), which tells us, for a prediction location, how MANY similar training data points were used during modle training.\n\nError profiles\nLet’s assume there is a relationship between the density of training data points in the predictor space (LPD) and the model performance. Let’s analyze that and use that to predict the prediction performance.\n\nplot(AOA$LPD)\n\n\n\n\n\n\n\nep &lt;- errorProfiles(model,AOA,variable=\"LPD\")\nplot(ep)\n\n\n\n\n\n\n\nplot(predict(AOA$LPD,ep))",
    "crumbs": [
      "Block 3: Machine Learning",
      "Machine learning for remote sensing applications"
    ]
  },
  {
    "objectID": "ml_session/ML_AOA.html#prepare-data",
    "href": "ml_session/ML_AOA.html#prepare-data",
    "title": "Machine learning for remote sensing applications",
    "section": "Prepare data",
    "text": "Prepare data\n\nmof_sen &lt;- rast(\"data/sentinel_uniwald.grd\")\nLAIdat &lt;- st_read(\"data/trainingsites_LAI.gpkg\")\n\nReading layer `trainingsites_LAI' from data source \n  `/home/hanna/Documents/Github/earth-observation-network/EON2025/ml_session/data/trainingsites_LAI.gpkg' \n  using driver `GPKG'\nSimple feature collection with 67 features and 10 fields\nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: 476350 ymin: 5631537 xmax: 478075 ymax: 5632765\nProjected CRS: WGS 84 / UTM zone 32N\n\ntrainDat &lt;- extract(mof_sen,LAIdat,na.rm=TRUE)\ntrainDat$LAI &lt;- LAIdat$LAI\n\n\nmeanmodel &lt;- mof_sen[[1]]\nvalues(meanmodel) &lt;- mean(trainDat$LAI)\nplot(meanmodel)\n\n\n\n\n\n\n\nrandommodel &lt;- mof_sen[[1]]\nvalues(randommodel)&lt;- runif(ncell(randommodel),min = 0,4)\n\nplot(randommodel)",
    "crumbs": [
      "Block 3: Machine Learning",
      "Machine learning for remote sensing applications"
    ]
  },
  {
    "objectID": "ml_session/ML_AOA.html#a-simple-linear-model",
    "href": "ml_session/ML_AOA.html#a-simple-linear-model",
    "title": "Machine learning for remote sensing applications",
    "section": "A simple linear model",
    "text": "A simple linear model\nAs a simple first approach we might develop a linear model. Let’s assume a linear relationship between the NDVI and the LAI\n\nplot(trainDat$NDVI,trainDat$LAI)\nmodel_lm &lt;- lm(LAI~NDVI,data=trainDat)\nsummary(model_lm)\n\n\nCall:\nlm(formula = LAI ~ NDVI, data = trainDat)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-1.87314 -0.52143 -0.03363  0.63668  2.25252 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)   \n(Intercept)  -0.8518     1.4732  -0.578  0.56515   \nNDVI          6.8433     2.3160   2.955  0.00435 **\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.8887 on 65 degrees of freedom\nMultiple R-squared:  0.1184,    Adjusted R-squared:  0.1049 \nF-statistic: 8.731 on 1 and 65 DF,  p-value: 0.004354\n\nabline(model_lm,col=\"red\")\n\n\n\n\n\n\n\nprediction_LAI &lt;- predict(mof_sen,model_lm,na.rm=T)\nplot(prediction_LAI)\n\n\n\n\n\n\n\nlimodelpred &lt;- -0.8518+mof_sen$NDVI*6.8433\nmapview(limodelpred)",
    "crumbs": [
      "Block 3: Machine Learning",
      "Machine learning for remote sensing applications"
    ]
  },
  {
    "objectID": "ml_session/ML_AOA.html#the-machine-learning-way",
    "href": "ml_session/ML_AOA.html#the-machine-learning-way",
    "title": "Machine learning for remote sensing applications",
    "section": "The machine learning way",
    "text": "The machine learning way\n\nDefine CV folds\nLet’s use the NNDM cross-validation approach.\n\nnndm_folds &lt;- knndm(LAIdat,mof_sen,k=3)\n\nLet’s explore the geodistance\n\ngd &lt;- geodist(LAIdat,mof_sen,cvfolds = nndm_folds$indx_test)\nplot(gd)\n\n\n\n\n\n\n\n\n\n\nModel training\n\nctrl &lt;- trainControl(method=\"cv\",\n                     index=nndm_folds$indx_train,\n                     indexOut = nndm_folds$indx_test,\n                    savePredictions = \"all\")\n\n\nmodel &lt;- ffs(trainDat[,predictors],\n             trainDat$LAI,\n             method=\"rf\",\n             trControl = ctrl,\n             importance=TRUE,\n             verbose=FALSE)\n\n\nmodel\n\nSelected Variables: \nT32UMB_20170510T103031_B07 T32UMB_20170510T103031_B08 NDVI\n---\nRandom Forest \n\n67 samples\n 3 predictor\n\nNo pre-processing\nResampling: Cross-Validated (10 fold) \nSummary of sample sizes: 41, 44, 49 \nResampling results across tuning parameters:\n\n  mtry  RMSE       Rsquared   MAE      \n  2     0.8477533  0.2158171  0.7015259\n  3     0.8612522  0.2068717  0.7236176\n\nRMSE was used to select the optimal model using the smallest value.\nThe final value used for the model was mtry = 2.\n\n\n\n\nLAI prediction\nLet’s then use the trained model for prediction.\n\nLAIprediction &lt;- predict(mof_sen,model)\nplot(LAIprediction)\n\n\n\n\n\n\n\n\n\nQuestion?! Why does it look so different than the linear model?\n\n\n\nAOA estimation\n\nAOA &lt;- aoa(mof_sen,model,LPD = TRUE, verbose=FALSE)\nplot(AOA$AOA)\n\n\n\n\n\n\n\nplot(AOA$LPD)\n\n\n\n\n\n\n\n\n\n\nError profiles\nLet’s assume there is a relationship between the density of training data points in the predictor space (LPD) and the model performance. Let’s analyze that and use that to predict the prediction performance.\n\nep &lt;- errorProfiles(model,AOA,variable=\"DI\")\nplot(ep)\n\n\n\n\n\n\n\nplot(predict(AOA$DI,ep))",
    "crumbs": [
      "Block 3: Machine Learning",
      "Machine learning for remote sensing applications"
    ]
  },
  {
    "objectID": "block6/index.html",
    "href": "block6/index.html",
    "title": "Materials",
    "section": "",
    "text": "The workshop slides are at https://jakubnowosad.com/eon2025/.\n\n\n\nParticipants are expected to have a working recent version of R and RStudio installed, along with several R packages listed below.\n\nR: https://cloud.r-project.org/\nRStudio: https://posit.co/download/rstudio-desktop/#download\n\ninstall.packages(\"remotes\")\npkg_list = c(\"terra\", \"sf\", \"landscapemetrics\", \"motif\", \"tidyr\", \"dplyr\")\nremotes::install_cran(pkg_list)\n\n\n\nThe slides are accompanied by practical exercises. The best way to get them is to download the exercises repository as a ZIP file from https://github.com/Nowosad/eon2025-exercises/archive/refs/heads/main.zip and unpack it on your computer. Then, you can open the .Rproj file and start working on the exercises in RStudio.",
    "crumbs": [
      "Block 5: Understanding spatial patterns: how to measure and compare them",
      "Materials"
    ]
  },
  {
    "objectID": "block6/index.html#slides",
    "href": "block6/index.html#slides",
    "title": "Materials",
    "section": "",
    "text": "The workshop slides are at https://jakubnowosad.com/eon2025/.",
    "crumbs": [
      "Block 5: Understanding spatial patterns: how to measure and compare them",
      "Materials"
    ]
  },
  {
    "objectID": "block6/index.html#prerequisites",
    "href": "block6/index.html#prerequisites",
    "title": "Materials",
    "section": "",
    "text": "Participants are expected to have a working recent version of R and RStudio installed, along with several R packages listed below.\n\nR: https://cloud.r-project.org/\nRStudio: https://posit.co/download/rstudio-desktop/#download\n\ninstall.packages(\"remotes\")\npkg_list = c(\"terra\", \"sf\", \"landscapemetrics\", \"motif\", \"tidyr\", \"dplyr\")\nremotes::install_cran(pkg_list)",
    "crumbs": [
      "Block 5: Understanding spatial patterns: how to measure and compare them",
      "Materials"
    ]
  },
  {
    "objectID": "block6/index.html#exercises",
    "href": "block6/index.html#exercises",
    "title": "Materials",
    "section": "",
    "text": "The slides are accompanied by practical exercises. The best way to get them is to download the exercises repository as a ZIP file from https://github.com/Nowosad/eon2025-exercises/archive/refs/heads/main.zip and unpack it on your computer. Then, you can open the .Rproj file and start working on the exercises in RStudio.",
    "crumbs": [
      "Block 5: Understanding spatial patterns: how to measure and compare them",
      "Materials"
    ]
  },
  {
    "objectID": "block1_magdon/02_ValidationDataCollection.html",
    "href": "block1_magdon/02_ValidationDataCollection.html",
    "title": "Collection of validation data in the context of remote sensing based forest monitoring",
    "section": "",
    "text": "rm(list=ls())\nlibrary(sf)\n\nLinking to GEOS 3.13.1, GDAL 3.11.0, PROJ 9.6.0; sf_use_s2() is TRUE\n\nlibrary(terra)\n\nterra 1.8.60\n\nlibrary(ggplot2)\nlibrary(rprojroot)\nlibrary(patchwork)\n\n\nAttache Paket: 'patchwork'\n\n\nDas folgende Objekt ist maskiert 'package:terra':\n\n    area\n\ndata_dir=paste0(find_rstudio_root_file(),\"/block1_magdon/data/\")",
    "crumbs": [
      "Block 1: Reference Data Collection",
      "Validation of continuous maps using design-based sampling methods"
    ]
  },
  {
    "objectID": "block1_magdon/02_ValidationDataCollection.html#systematic-sample-to-collect-reference-data-for-map-validation",
    "href": "block1_magdon/02_ValidationDataCollection.html#systematic-sample-to-collect-reference-data-for-map-validation",
    "title": "Collection of validation data in the context of remote sensing based forest monitoring",
    "section": "Systematic sample to collect reference data for map validation",
    "text": "Systematic sample to collect reference data for map validation\nTo validate the map we use a systematic sample grid. In a real world application we do not know the true population values. Therefore, field work would be needed to collect reference data at the selected sample points. In this workshop we assume that the agp_pop map represents the true value without any errors. Thus, we don’t need to go to field but we can sample the data by extracting the true values from the map at the sample locations.\n\n# we will use n=100 sample plots\nn=100\np1 = sf::st_sample(np_boundary,size=n,type='regular')\n\nggplot()+geom_sf(data=np_boundary,fill=NA)+\n  geom_sf(data=p1)\n\n\n\n\n\n\n\n\nAt each sample point we extract the predicted and observed AGB value.\n\nobs &lt;- terra::extract(agb_pop,vect(p1))\nnames(obs)&lt;-c('ID','obs')\n\npred &lt;- terra::extract(agb_model,vect(p1))\nnames(pred)&lt;-c('ID','pred')\nvalidation&lt;-data.frame(observed=obs$obs, predicted=pred$pred)\n\n# we need to remove the na values from this dataframe. In real world applications\n# such NA values can,  occur for example at inaccessible field plots.\n\nvalidation&lt;-validation[complete.cases(validation),]",
    "crumbs": [
      "Block 1: Reference Data Collection",
      "Validation of continuous maps using design-based sampling methods"
    ]
  },
  {
    "objectID": "block1_magdon/02_ValidationDataCollection.html#assessment-of-the-abg-model-performance",
    "href": "block1_magdon/02_ValidationDataCollection.html#assessment-of-the-abg-model-performance",
    "title": "Collection of validation data in the context of remote sensing based forest monitoring",
    "section": "Assessment of the ABG-model performance",
    "text": "Assessment of the ABG-model performance\n\nggplot(data=validation,aes(x=observed, y=predicted))+\n  geom_point(alpha=0.5)+\n  xlab('Observed AGB t/ha')+ylab('Predicted AGB t/ha')\n\n\n\n\n\n\n\n\n\nSample RMSE\nAgain we can use the RMSE to express the mean difference between observed and predicted AGB.\n\nRMSE_sample = sqrt(sum((validation$observed-validation$predicted)^2)/nrow(validation))\n\nThe sample RMSE is 38.89* t/ha. To better compare the values between different target variables and models is can also express as a proportion relative to the mean value of the predictions.\n\nrRMSE = RMSE_sample/mean(validation$predicted)\n\nOn average we expect that the AGB estimate of our model has an error of 24.1 %.\n\n\nError distribution\nBut is this RMSE valid for the entire range of the observed values or do we expect higher errors for higher AGB values?\nTo see how the model performs over target value range we can use the following analysis plots.\n\nvalidation$resid&lt;-validation$observed-validation$predicted\n\np1&lt;-ggplot(data=validation,aes(x=observed, y=predicted))+\n  geom_point(alpha=0.5)+\n  xlab('Observed AGB t/ha')+ylab('Predicted AGB t/ha')+\n  xlim(0,250)+ylim(0,250)+\n  geom_abline(slope=1,intercept = 0)+\n  stat_summary(fun.data= mean_cl_normal) + \n  geom_smooth(method='lm')\n\n\n\np2&lt;-ggplot(data=validation,aes(x=observed, y=resid))+\n  geom_point(alpha=0.5)+\n  xlab('Observed AGB t/ha')+ylab('Residuals')+\n  xlim(0,250)+ylim(-50,+50)+\n  geom_abline(slope=0,intercept = 1)\n\np3&lt;-ggplot(data=validation,aes(x=resid))+\n  geom_histogram(aes(y=..density..),fill='grey',binwidth=10)+\n  xlab('Observed AGB t/ha')+ylab('Density')+\n  xlim(-150,150)+\n  stat_function(fun = dnorm, geom=\"polygon\",args = list(mean = mean(validation$resid), sd = sd(validation$resid)),color='blue',alpha=0.4,fill='blue')+\n  geom_vline(xintercept=0,color='blue')+\n  geom_vline(xintercept=mean(validation$resid),color='red')\np1+p2+p3+plot_layout(ncol=3)\n\nWarning: Removed 1 row containing non-finite outside the scale range\n(`stat_summary()`).\n\n\nWarning: Computation failed in `stat_summary()`.\nCaused by error in `fun.data()`:\n! The package \"Hmisc\" is required.\n\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\nWarning: Removed 1 row containing non-finite outside the scale range\n(`stat_smooth()`).\n\n\nWarning: Removed 1 row containing missing values or values outside the scale range\n(`geom_point()`).\n\n\nWarning: Removed 18 rows containing missing values or values outside the scale range\n(`geom_point()`).\n\n\nWarning: The dot-dot notation (`..density..`) was deprecated in ggplot2 3.4.0.\nℹ Please use `after_stat(density)` instead.\n\n\nWarning: Removed 2 rows containing missing values or values outside the scale range\n(`geom_bar()`).",
    "crumbs": [
      "Block 1: Reference Data Collection",
      "Validation of continuous maps using design-based sampling methods"
    ]
  },
  {
    "objectID": "block1_magdon/01_TrainingDataCollection.html",
    "href": "block1_magdon/01_TrainingDataCollection.html",
    "title": "Collection of training data for remote sensing model building",
    "section": "",
    "text": "#install.packages(\"devtools\")\n#devtools::install_github(\"bleutner/RStoolbox\")\nlibrary(sf)\n\nLinking to GEOS 3.13.1, GDAL 3.11.0, PROJ 9.6.0; sf_use_s2() is TRUE\n\nlibrary(RStoolbox)\n\nThis is version 1.0.2.1 of RStoolbox\n\nlibrary(terra)\n\nterra 1.8.60\n\nlibrary(ggplot2)\nlibrary(mapview)\nlibrary(kableExtra)\nlibrary(dplyr)\n\n\nAttache Paket: 'dplyr'\n\n\nDas folgende Objekt ist maskiert 'package:kableExtra':\n\n    group_rows\n\n\nDie folgenden Objekte sind maskiert von 'package:terra':\n\n    intersect, union\n\n\nDie folgenden Objekte sind maskiert von 'package:stats':\n\n    filter, lag\n\n\nDie folgenden Objekte sind maskiert von 'package:base':\n\n    intersect, setdiff, setequal, union\n\nlibrary(rprojroot)\nlibrary(patchwork)\n\n\nAttache Paket: 'patchwork'\n\n\nDas folgende Objekt ist maskiert 'package:terra':\n\n    area\n\nlibrary(rmarkdown)\nlibrary(tidyr)\n\n\nAttache Paket: 'tidyr'\n\n\nDas folgende Objekt ist maskiert 'package:terra':\n\n    extract\n\nlibrary(tibble)",
    "crumbs": [
      "Block 1: Reference Data Collection",
      "Collection of training data for remote sensing modelling based on spectral variability"
    ]
  },
  {
    "objectID": "block1_magdon/01_TrainingDataCollection.html#dimension-reduction-pca",
    "href": "block1_magdon/01_TrainingDataCollection.html#dimension-reduction-pca",
    "title": "Collection of training data for remote sensing model building",
    "section": "Dimension reduction (PCA)",
    "text": "Dimension reduction (PCA)\nIn a fist step we reduce the dimensions of the 9 Sentinel-2 bands while maintaining most of the information, using a principal component analysis (PCA).\n\n# Calculation of the principlal components using the RStoolbox\npca&lt;-RStoolbox::rasterPCA(s2,nSamples = 5000, spca=TRUE )\n\n\n|---------|---------|---------|---------|\n=========================================\n                                          \n\n# Extracting the first three components\nrgb_raster &lt;- subset(pca$map, 1:3)\n\n# Function to scale the pixel values to 0-255\nscale_fun &lt;- function(x) {\n  # Calculation of the 2% and 98% quantile\n  q &lt;- quantile(x, c(0.02, 0.98), na.rm = TRUE)\n  \n  # scaling the values\n  x &lt;- (x - q[1]) / (q[2] - q[1]) * 255\n  \n  # restrict the values to 0-255\n  x &lt;- pmin(pmax(x, 0), 255)\n  \n  return(x)\n}\n\n# Scaling of each band\nfor (i in 1:3) {\n  rgb_raster[[i]] &lt;- app(rgb_raster[[i]], scale_fun)\n}\n\n# Plot the first three principal components as RGB\nplotRGB(rgb_raster, r = 1, g = 2, b = 3)\n\n\n\n\n\n\n\n# Show importance of componentes\nsummary(pca$model)\n\nImportance of components:\n                          Comp.1    Comp.2    Comp.3     Comp.4     Comp.5\nStandard deviation     2.2346500 1.8079296 0.6737201 0.38366893 0.26048690\nProportion of Variance 0.5548512 0.3631788 0.0504332 0.01635576 0.00753927\nCumulative Proportion  0.5548512 0.9180300 0.9684632 0.98481898 0.99235825\n                            Comp.6      Comp.7       Comp.8       Comp.9\nStandard deviation     0.177485712 0.162607669 0.0812795411 0.0650151820\nProportion of Variance 0.003500131 0.002937917 0.0007340404 0.0004696638\nCumulative Proportion  0.995858379 0.998796296 0.9995303362 1.0000000000\n\n\nFrom the output of the PCA we see that we can capture 92% of the variability with the first two components. Thus we will only use the PC1 and PC2 for the subsequent analysis.",
    "crumbs": [
      "Block 1: Reference Data Collection",
      "Collection of training data for remote sensing modelling based on spectral variability"
    ]
  },
  {
    "objectID": "block1_magdon/01_TrainingDataCollection.html#unsupervised-clustering",
    "href": "block1_magdon/01_TrainingDataCollection.html#unsupervised-clustering",
    "title": "Collection of training data for remote sensing model building",
    "section": "Unsupervised clustering",
    "text": "Unsupervised clustering\nIn the next step we run an unsupervised classification of the PC1 and PC2 to get a clustered map. For the unsupervised classification we need to take a decision on the number of classes/clusters to be created. Here we will take \\(n=5\\) classes. However, depending on the target variable this value need to be adjusted.\n\nset.seed(2222)\ncluster &lt;- RStoolbox::unsuperClass(pca$map[[c('PC1','PC2')]], nSamples = 100, nClasses = 5, nStarts = 5)\n\n\n## Plots\ncolors &lt;- rainbow(5)\nplot(cluster$map, col = colors, legend = TRUE, axes = TRUE, box =TRUE)\n\n\n\n\n\n\n\n\nThe map shows a clear spatial patterns related to the elevation, tree species and vitality status of the National Park forests.",
    "crumbs": [
      "Block 1: Reference Data Collection",
      "Collection of training data for remote sensing modelling based on spectral variability"
    ]
  },
  {
    "objectID": "block1_magdon/01_TrainingDataCollection.html#implement-a-plot-design",
    "href": "block1_magdon/01_TrainingDataCollection.html#implement-a-plot-design",
    "title": "Collection of training data for remote sensing model building",
    "section": "Implement a plot design",
    "text": "Implement a plot design\nTo extract pixel values for the sample location we need to define a plot design. For this exercise we will simulate a circular fixed area plot with a radius of 13 m.\n\n# Create a training by extracting the mean value of all pixels touching\n# a buffered area with 13m around the plot center\n\nplots &lt;- sf::st_buffer(sf_samples,dist = 13)\ntrain&lt;-terra::extract(s2,plots,fun='mean',bind=FALSE,na.rm=TRUE)\n\nplots &lt;- plots %&gt;% mutate(ID=row_number())\ntrain &lt;- plots %&gt;% left_join(train, by= \"ID\")\nmapview::mapview(train, zcol=\"class_unsupervised\",\n        map.types = c(\"Esri.WorldShadedRelief\", \"OpenStreetMap.DE\"))+\n  mapview(np_boundary,alpha.regions = 0.2, aplha = 1)",
    "crumbs": [
      "Block 1: Reference Data Collection",
      "Collection of training data for remote sensing modelling based on spectral variability"
    ]
  },
  {
    "objectID": "block2_bas_dob/lidar_forest_structure.html",
    "href": "block2_bas_dob/lidar_forest_structure.html",
    "title": "Processing of 3D Lidar data to assess forest structure",
    "section": "",
    "text": "check out the manual of the lidR package: https://r-lidar.github.io/lidRbook/",
    "crumbs": [
      "Block 2: Processing of 3D LiDAR data to assess forest structure",
      "Processing of 3D Lidar data to assess forest structure"
    ]
  },
  {
    "objectID": "block2_bas_dob/lidar_forest_structure.html#setup",
    "href": "block2_bas_dob/lidar_forest_structure.html#setup",
    "title": "Processing of 3D Lidar data to assess forest structure",
    "section": "0. Setup",
    "text": "0. Setup\n\nInstall and load required libraries\n\n## install libraries\n# install.packges(\"lidR\")\n# install.packages(\"terra\")\n# install.packages(\"sf\")\n# install.packages(\"rTwig\")\n# install.packages('lidRviewer', repos = c('https://r-lidar.r-universe.dev'))\n\n\nlibrary(lidR)\nlibrary(terra)\nlibrary(sf)\nlibrary(lidRviewer)\nlibrary(dplyr)\nlibrary(rTwig)\n\n\n\ndownload the data\ndownload lidar point cloud and ground truth data (BI) from owncloud.\n\n# Data access\nurl_las &lt;- \"https://cloud.hawk.de/index.php/s/pB4RRmLb4Xxy4Qj/download\"\ndownload.file(url_las, destfile = \"uls_goewa.laz\", mode = \"wb\")\n\nurl_bi &lt;- \"https://cloud.hawk.de/index.php/s/5npprfZYLjg5ip5/download\"\ndownload.file(url_bi, destfile = \"trees_bi.gpkg\", mode = \"wb\")\n\n\n\nimport the data\n\nlas &lt;- readLAS(\"uls_goewa.laz\")\ntrees_bi &lt;- st_read(\"trees_bi.gpkg\")\n\nlet´s inspect the data. 1) whats the point density of the lidar data? 2) whats the total number of points and pulses and what is the difference between the two? 3) is there any classification in the point cloud? 4) how many trees were measured in the BI? 5) which tree species are present in the plot?\n\nprint(las)\n\nclass        : LAS (v1.2 format 3)\nmemory       : 313.2 Mb \nextent       : 572445.4, 572496.1, 5709020, 5709071 (xmin, xmax, ymin, ymax)\ncoord. ref.  : WGS 84 / UTM zone 32N \narea         : 2602 m²\npoints       : 5.13 million points\ntype         : terrestrial\ndensity      : 1971.94 points/m²\ndensity      : 1660.17 pulses/m²\n\nplot(las)\nprint(trees_bi)\n\nSimple feature collection with 69 features and 17 fields\nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: 572446.5 ymin: 5709021 xmax: 572495.8 ymax: 5709069\nProjected CRS: WGS 84 / UTM zone 32N\nFirst 10 features:\n   IDPlots          Name Plots.Bem ID      X_m     Y_m    Z_m Species Spec_txt\n1        1 GoeWald Fla 1      &lt;NA&gt;  1 572468.6 5709043 -0.448      22       Bu\n2        1 GoeWald Fla 1      &lt;NA&gt;  2 572467.4 5709044 -0.466      22       Bu\n3        1 GoeWald Fla 1      &lt;NA&gt;  3 572462.7 5709041 -0.443      22       Bu\n4        1 GoeWald Fla 1      &lt;NA&gt;  4 572459.8 5709039 -0.657      22       Bu\n5        1 GoeWald Fla 1      &lt;NA&gt;  5 572455.0 5709045 -0.971      22       Bu\n6        1 GoeWald Fla 1      &lt;NA&gt;  6 572455.1 5709048 -0.978      22       Bu\n7        1 GoeWald Fla 1      &lt;NA&gt;  7 572459.6 5709050 -0.723      22       Bu\n8        1 GoeWald Fla 1      &lt;NA&gt;  8 572460.5 5709052 -0.920      22       Bu\n9        1 GoeWald Fla 1      &lt;NA&gt;  9 572463.9 5709053 -0.747      22       Bu\n10       1 GoeWald Fla 1      &lt;NA&gt; 10 572468.1 5709048 -0.619      22       Bu\n   DBH_mm Vit Bruch                Schirm Schiefer.B Trees.Bem   x_lok  y_lok\n1     354 leb  nein geringe �berschirmung       Nein      &lt;NA&gt;  -2.197 -2.543\n2     345 leb  nein geringe �berschirmung       Nein      &lt;NA&gt;  -3.371 -1.518\n3     518 leb  nein geringe �berschirmung       Nein      &lt;NA&gt;  -8.087 -3.932\n4     373 leb  nein    Hohe �berschirmung       Nein      &lt;NA&gt; -11.067 -6.176\n5     350 leb  nein    Hohe �berschirmung       Nein      &lt;NA&gt; -15.776 -0.631\n6     388 leb  nein geringe �berschirmung       Nein      &lt;NA&gt; -15.576  2.703\n7     264 tot    ja                  &lt;NA&gt;       Nein      &lt;NA&gt; -11.070  4.367\n8     404 leb  nein geringe �berschirmung       Nein      &lt;NA&gt; -10.178  6.296\n9     464 leb  nein geringe �berschirmung       Nein      &lt;NA&gt;  -6.741  7.432\n10    291 leb    ja  Komplett �berschirmt       Nein      &lt;NA&gt;  -2.589  2.832\n                       geom\n1  POINT (572468.6 5709043)\n2  POINT (572467.4 5709044)\n3  POINT (572462.7 5709041)\n4  POINT (572459.8 5709039)\n5    POINT (572455 5709045)\n6  POINT (572455.1 5709048)\n7  POINT (572459.6 5709050)\n8  POINT (572460.5 5709052)\n9  POINT (572463.9 5709053)\n10 POINT (572468.1 5709048)",
    "crumbs": [
      "Block 2: Processing of 3D LiDAR data to assess forest structure",
      "Processing of 3D Lidar data to assess forest structure"
    ]
  },
  {
    "objectID": "block2_bas_dob/lidar_forest_structure.html#calculating-terrain-models",
    "href": "block2_bas_dob/lidar_forest_structure.html#calculating-terrain-models",
    "title": "Processing of 3D Lidar data to assess forest structure",
    "section": "1. Calculating Terrain Models",
    "text": "1. Calculating Terrain Models\nNext we are calculating terrain models, using triangulation (TIN = Triangulated Irregular Network). Since the data is already ground classified we can skip the classification step. The DEM is using the ground return points to interpolate the surface. In contrast, the digital surface model is using the highest lidar returns to represent the top of any object above the ground. Subtracting the two gives us the canopy height.\ncheck out the documentation of the rasterize_terrain algorithm. Try out different interpolation algorithms and different resolutions. Compare the results visually.\n\ndem &lt;- rasterize_terrain(las, res = 0.5, algorithm = tin())\ndsm &lt;- rasterize_canopy(las, res = 0.5, algorithm = dsmtin(max_edge = 8))\nchm &lt;- dsm - dem\nchm &lt;- terra::focal(chm, w = 3, fun = mean, na.rm = TRUE) # smoothing results \n\npar(mfrow = c(1,3))\nplot(dem, main = \"digital elevation model\")\nplot(dsm, main = \"digital surface model\")\nplot(chm, main = \"canopy heigt model\")\n\n\n\n\n\n\n\n#writeRaster(dem, \"./data/output/dem.tif\", overwrite=TRUE)\n#writeRaster(dsm, \"./data/output/dsm.tif\", overwrite=TRUE)\n#writeRaster(chm, \"./data/output/chm.tif\", overwrite=TRUE)",
    "crumbs": [
      "Block 2: Processing of 3D LiDAR data to assess forest structure",
      "Processing of 3D Lidar data to assess forest structure"
    ]
  },
  {
    "objectID": "block2_bas_dob/lidar_forest_structure.html#individual-tree-detection",
    "href": "block2_bas_dob/lidar_forest_structure.html#individual-tree-detection",
    "title": "Processing of 3D Lidar data to assess forest structure",
    "section": "2. Individual Tree Detection",
    "text": "2. Individual Tree Detection\nIndividual Tree Detection (ITD) is the process of spatially locating trees (f.i to extract height information). Tree tops can be detected by applying a Local Maximum Filter (LMF) on the loaded data set. The number of detected trees is correlated to the window size (ws) argument. Small windows sizes usually gives more trees, while large windows size generally miss smaller trees that are “hidden” by big trees that contain the highest points in the neighbourhood. We will use the Tree detection function with variable window size. Any points below 2 m will equate to a window size of 3 m, while points above 20 meters equate to a window size of 5 m. Anything between 2 and 20 meter will have a non-linear relationship.\n\n## Function for Local Maximum Filter with variable windows size\n\nf &lt;- function(x) {\n  y &lt;- 2.6 * (-(exp(-0.08*(x-2)) - 1)) + 3 \n  # from https://r-lidar.github.io/lidRbook/itd.html\n  y[x &lt; 2] &lt;- 3\n  y[x &gt; 20] &lt;- 5\n  return(y)\n}\n\nheights &lt;- seq(-5,35,0.5)\nws &lt;- f(heights)\n\nplot(heights, ws, type = \"l\",  ylim = c(0,5))\n\n\n\n\n\n\n\n\nLet´s run the tree detection algorithm using the user-defined ws function and the CHM created beforehand. Compare the results with the ground truth BI data. - How many of the trees could be detected in the lidar data?\n- what could be the reasons for that?\n\n#ttops &lt;- locate_trees(las, lmf(f)) # only run this if you have a fast computer! \nttops &lt;- locate_trees(chm, lmf(f)) \n\n# plot results \nplot(chm, col = height.colors(50))\nplot(sf::st_geometry(trees_bi), add = TRUE, pch = 2, col =\"blue\")\nplot(sf::st_geometry(ttops), add = TRUE, pch = 3, col = \"black\")\n\n\n\n\n\n\n\n# 3D plot\nlas_norm &lt;- normalize_height(las, knnidw()) # normalize point cloud for this vizualisation \n\nInverse distance weighting: [==================================----------------] 68% (6 threads)\nInverse distance weighting: [==================================----------------] 69% (6 threads)\nInverse distance weighting: [===================================---------------] 70% (6 threads)\nInverse distance weighting: [===================================---------------] 71% (6 threads)\nInverse distance weighting: [====================================--------------] 72% (6 threads)\nInverse distance weighting: [====================================--------------] 73% (6 threads)\nInverse distance weighting: [=====================================-------------] 74% (6 threads)\nInverse distance weighting: [=====================================-------------] 75% (6 threads)\nInverse distance weighting: [======================================------------] 76% (6 threads)\nInverse distance weighting: [======================================------------] 77% (6 threads)\nInverse distance weighting: [=======================================-----------] 78% (6 threads)\nInverse distance weighting: [=======================================-----------] 79% (6 threads)\nInverse distance weighting: [========================================----------] 80% (6 threads)\nInverse distance weighting: [========================================----------] 81% (6 threads)\nInverse distance weighting: [=========================================---------] 82% (6 threads)\nInverse distance weighting: [=========================================---------] 83% (6 threads)\nInverse distance weighting: [==========================================--------] 84% (6 threads)\nInverse distance weighting: [==========================================--------] 85% (6 threads)\nInverse distance weighting: [===========================================-------] 86% (6 threads)\nInverse distance weighting: [===========================================-------] 87% (6 threads)\nInverse distance weighting: [============================================------] 88% (6 threads)\nInverse distance weighting: [============================================------] 89% (6 threads)\nInverse distance weighting: [=============================================-----] 90% (6 threads)\nInverse distance weighting: [=============================================-----] 91% (6 threads)\nInverse distance weighting: [==============================================----] 92% (6 threads)\nInverse distance weighting: [==============================================----] 93% (6 threads)\nInverse distance weighting: [===============================================---] 94% (6 threads)\nInverse distance weighting: [===============================================---] 95% (6 threads)\nInverse distance weighting: [================================================--] 96% (6 threads)\nInverse distance weighting: [================================================--] 97% (6 threads)\nInverse distance weighting: [=================================================-] 98% (6 threads)\nInverse distance weighting: [=================================================-] 99% (6 threads)\nInverse distance weighting: [==================================================] 100% (6 threads)\n\nx &lt;- plot(las_norm, bg = \"white\", size = 4)\nadd_treetops3d(x, ttops)\n\n#writeVector(vect(ttops), \"./data/output/ttops_chm_.gpkg\", overwrite=TRUE)",
    "crumbs": [
      "Block 2: Processing of 3D LiDAR data to assess forest structure",
      "Processing of 3D Lidar data to assess forest structure"
    ]
  },
  {
    "objectID": "block2_bas_dob/lidar_forest_structure.html#individual-tree-segmentation",
    "href": "block2_bas_dob/lidar_forest_structure.html#individual-tree-segmentation",
    "title": "Processing of 3D Lidar data to assess forest structure",
    "section": "3. Individual Tree Segmentation",
    "text": "3. Individual Tree Segmentation\nIndividual Tree Segmentation (ITS) is the process of individually delineating detected trees. Even when the algorithm is raster-based (which is the case of dalponte2016()), lidR segments the point cloud and assigns an ID to each point by inserting a new attribute named treeID in the LAS object. This means that every point is associated with a particular tree.\n\nalgo &lt;- dalponte2016(chm, ttops)\nlas_seg &lt;- segment_trees(las_norm, algo) # segment point cloud\nx &lt;- plot(las_seg, bg = \"white\", size = 4, color = \"treeID\") # visualize trees\nadd_treetops3d(x, ttops)",
    "crumbs": [
      "Block 2: Processing of 3D LiDAR data to assess forest structure",
      "Processing of 3D Lidar data to assess forest structure"
    ]
  },
  {
    "objectID": "block2_bas_dob/lidar_forest_structure.html#deriving-metrics-using-the-area-based-approach",
    "href": "block2_bas_dob/lidar_forest_structure.html#deriving-metrics-using-the-area-based-approach",
    "title": "Processing of 3D Lidar data to assess forest structure",
    "section": "4. Deriving Metrics using the area-based approach",
    "text": "4. Deriving Metrics using the area-based approach\nthe Area-Based Approach (ABA) allows the creation of wall-to-wall predictions of forest inventory attributes (e.g. basal area or total volume per hectare) by linking ALS variables with field measured references.\n\nr_metr &lt;- pixel_metrics(las, res = 0.5, func = .stdmetrics)\nplot(r_metr)",
    "crumbs": [
      "Block 2: Processing of 3D LiDAR data to assess forest structure",
      "Processing of 3D Lidar data to assess forest structure"
    ]
  },
  {
    "objectID": "block2_bas_dob/lidar_forest_structure.html#forest-structural-complexity",
    "href": "block2_bas_dob/lidar_forest_structure.html#forest-structural-complexity",
    "title": "Processing of 3D Lidar data to assess forest structure",
    "section": "5.Forest structural complexity",
    "text": "5.Forest structural complexity\n(Fractal complexity analysis/ voxel-based box-count dimension or box dimension (Db) method)\nThe box dimension quantifies structural complexity of point clouds using a fractal box-counting approach.\nIt is defined as the slope of the regression between log box (voxel) count and log inverse box (voxel) size, with higher R² values indicating stronger self-similarity. Reliable estimates require high-resolution (≤1 cm) point clouds with minimal occlusion.\n\n# Read data, check and pre-process with lidR\n#data &lt;- readLAS(\"uls_goewa.laz\")\nprint(las)\n\nclass        : LAS (v1.2 format 3)\nmemory       : 313.2 Mb \nextent       : 572445.4, 572496.1, 5709020, 5709071 (xmin, xmax, ymin, ymax)\ncoord. ref.  : WGS 84 / UTM zone 32N \narea         : 2602 m²\npoints       : 5.13 million points\ntype         : terrestrial\ndensity      : 1971.94 points/m²\ndensity      : 1660.17 pulses/m²\n\nlas_check(las) \n\n\n Checking the data\n  - Checking coordinates...\u001b[0;32m ✓\u001b[0m\n  - Checking coordinates type...\u001b[0;32m ✓\u001b[0m\n  - Checking coordinates range...\u001b[0;32m ✓\u001b[0m\n  - Checking coordinates quantization...\u001b[0;32m ✓\u001b[0m\n  - Checking attributes type...\u001b[0;32m ✓\u001b[0m\n  - Checking ReturnNumber validity...\u001b[0;32m ✓\u001b[0m\n  - Checking NumberOfReturns validity...\u001b[0;32m ✓\u001b[0m\n  - Checking ReturnNumber vs. NumberOfReturns...\u001b[0;32m ✓\u001b[0m\n  - Checking RGB validity...\u001b[0;32m ✓\u001b[0m\n  - Checking absence of NAs...\u001b[0;32m ✓\u001b[0m\n  - Checking duplicated points...\u001b[0;32m ✓\u001b[0m\n  - Checking degenerated ground points...\u001b[0;32m ✓\u001b[0m\n  - Checking attribute population...\n \u001b[0;32m   🛈 'PointSourceID' attribute is not populated\u001b[0m\n \u001b[0;32m   🛈 'ScanDirectionFlag' attribute is not populated\u001b[0m\n \u001b[0;32m   🛈 'EdgeOfFlightline' attribute is not populated\u001b[0m\n  - Checking gpstime incoherances\u001b[0;32m ✓\u001b[0m\n  - Checking flag attributes...\u001b[0;32m ✓\u001b[0m\n  - Checking user data attribute...\u001b[0;32m ✓\u001b[0m\n Checking the header\n  - Checking header completeness...\u001b[0;32m ✓\u001b[0m\n  - Checking scale factor validity...\u001b[0;32m ✓\u001b[0m\n  - Checking point data format ID validity...\u001b[0;32m ✓\u001b[0m\n  - Checking extra bytes attributes validity...\u001b[0;32m ✓\u001b[0m\n  - Checking the bounding box validity...\u001b[0;32m ✓\u001b[0m\n  - Checking coordinate reference system...\u001b[0;32m ✓\u001b[0m\n Checking header vs data adequacy\n  - Checking attributes vs. point format...\u001b[0;32m ✓\u001b[0m\n  - Checking header bbox vs. actual content...\u001b[0;32m ✓\u001b[0m\n  - Checking header number of points vs. actual content...\u001b[0;32m ✓\u001b[0m\n  - Checking header return number vs. actual content...\u001b[0;32m ✓\u001b[0m\n Checking coordinate reference system...\n  - Checking if the CRS was understood by R...\u001b[0;32m ✓\u001b[0m\n Checking preprocessing already done \n  - Checking ground classification...\u001b[0;32m yes\u001b[0m\n  - Checking normalization...\u001b[0;31m no\u001b[0m\n  - Checking negative outliers...\u001b[0;32m ✓\u001b[0m\n  - Checking flightline classification...\u001b[0;31m no\u001b[0m\n Checking compression\n  - Checking attribute compression...\n   -  ScanDirectionFlag is compressed\n   -  EdgeOfFlightline is compressed\n   -  Synthetic_flag is compressed\n   -  Keypoint_flag is compressed\n   -  Withheld_flag is compressed\n   -  UserData is compressed\n   -  PointSourceID is compressed\n\nlas_norm &lt;- normalize_height(las = las, \n                        algorithm = tin(), \n                        use_class = 2)\n\nlas_check(las_norm) # check negative outliers\n\n\n Checking the data\n  - Checking coordinates...\u001b[0;32m ✓\u001b[0m\n  - Checking coordinates type...\u001b[0;32m ✓\u001b[0m\n  - Checking coordinates range...\u001b[0;32m ✓\u001b[0m\n  - Checking coordinates quantization...\u001b[0;32m ✓\u001b[0m\n  - Checking attributes type...\u001b[0;32m ✓\u001b[0m\n  - Checking ReturnNumber validity...\u001b[0;32m ✓\u001b[0m\n  - Checking NumberOfReturns validity...\u001b[0;32m ✓\u001b[0m\n  - Checking ReturnNumber vs. NumberOfReturns...\u001b[0;32m ✓\u001b[0m\n  - Checking RGB validity...\u001b[0;32m ✓\u001b[0m\n  - Checking absence of NAs...\u001b[0;32m ✓\u001b[0m\n  - Checking duplicated points...\u001b[0;32m ✓\u001b[0m\n  - Checking degenerated ground points...\u001b[0;32m ✓\u001b[0m\n  - Checking attribute population...\n \u001b[0;32m   🛈 'PointSourceID' attribute is not populated\u001b[0m\n \u001b[0;32m   🛈 'ScanDirectionFlag' attribute is not populated\u001b[0m\n \u001b[0;32m   🛈 'EdgeOfFlightline' attribute is not populated\u001b[0m\n  - Checking gpstime incoherances\u001b[0;32m ✓\u001b[0m\n  - Checking flag attributes...\u001b[0;32m ✓\u001b[0m\n  - Checking user data attribute...\u001b[0;32m ✓\u001b[0m\n Checking the header\n  - Checking header completeness...\u001b[0;32m ✓\u001b[0m\n  - Checking scale factor validity...\u001b[0;32m ✓\u001b[0m\n  - Checking point data format ID validity...\u001b[0;32m ✓\u001b[0m\n  - Checking extra bytes attributes validity...\u001b[0;32m ✓\u001b[0m\n  - Checking the bounding box validity...\u001b[0;32m ✓\u001b[0m\n  - Checking coordinate reference system...\u001b[0;32m ✓\u001b[0m\n Checking header vs data adequacy\n  - Checking attributes vs. point format...\u001b[0;32m ✓\u001b[0m\n  - Checking header bbox vs. actual content...\u001b[0;32m ✓\u001b[0m\n  - Checking header number of points vs. actual content...\u001b[0;32m ✓\u001b[0m\n  - Checking header return number vs. actual content...\u001b[0;32m ✓\u001b[0m\n Checking coordinate reference system...\n  - Checking if the CRS was understood by R...\u001b[0;32m ✓\u001b[0m\n Checking preprocessing already done \n  - Checking ground classification...\u001b[0;32m yes\u001b[0m\n  - Checking normalization...\u001b[0;32m yes\u001b[0m\n  - Checking negative outliers...\n \u001b[1;33m   ⚠ 51 points below 0\u001b[0m\n  - Checking flightline classification...\u001b[0;31m no\u001b[0m\n Checking compression\n  - Checking attribute compression...\n   -  ScanDirectionFlag is compressed\n   -  EdgeOfFlightline is compressed\n   -  Synthetic_flag is compressed\n   -  Keypoint_flag is compressed\n   -  Withheld_flag is compressed\n   -  UserData is compressed\n   -  PointSourceID is compressed\n\nview(las_norm)\n            #Rotate with left mouse button\n            #Zoom with mouse wheel\n            #Pan with right mouse button\n            #Keyboard r or g or b to color with RGB\n            #Keyboard z to color with Z\n            #Keyboard i to color with Intensity\n            #Keyboard c to color with Classification\n            #Keyboard + or - to change the point size\n            #Keyboard l to enable/disable eyes-dome lightning\n\n\nlas_norm@data[Z&lt;0, 1:3] # Here options are either remove all or assign all to 0, However...\n\n           X       Y       Z\n       &lt;num&gt;   &lt;num&gt;   &lt;num&gt;\n 1: 572471.7 5709038 -0.0183\n 2: 572469.0 5709021 -0.0044\n 3: 572475.5 5709041 -0.0315\n 4: 572477.3 5709041 -0.0004\n 5: 572475.2 5709040 -0.0013\n 6: 572476.9 5709041 -0.0088\n 7: 572459.1 5709064 -0.0035\n 8: 572477.3 5709041 -0.0041\n 9: 572460.3 5709058 -0.0034\n10: 572458.5 5709062 -0.0015\n11: 572462.5 5709046 -0.1952\n12: 572480.8 5709064 -0.0056\n13: 572483.4 5709069 -0.0163\n14: 572483.8 5709068 -0.0514\n15: 572483.8 5709068 -0.0330\n16: 572483.4 5709068 -0.0129\n17: 572488.9 5709023 -0.0133\n18: 572476.2 5709047 -0.0031\n19: 572451.9 5709020 -0.0029\n20: 572483.6 5709068 -0.0488\n21: 572484.9 5709062 -0.0040\n22: 572452.1 5709020 -0.0102\n23: 572452.0 5709021 -0.0030\n24: 572488.8 5709054 -0.0037\n25: 572476.1 5709032 -0.0070\n26: 572452.2 5709020 -0.0227\n27: 572491.1 5709069 -0.0163\n28: 572488.9 5709023 -0.0496\n29: 572483.9 5709068 -0.0246\n30: 572483.4 5709068 -0.0099\n31: 572484.7 5709063 -0.0017\n32: 572477.9 5709064 -0.0101\n33: 572478.2 5709065 -0.0129\n34: 572482.9 5709060 -0.0112\n35: 572494.0 5709053 -0.0001\n36: 572491.3 5709055 -0.0017\n37: 572476.1 5709047 -0.0046\n38: 572475.6 5709050 -0.0004\n39: 572491.0 5709039 -0.0019\n40: 572473.3 5709046 -0.0006\n41: 572478.1 5709039 -0.0100\n42: 572493.1 5709032 -0.0122\n43: 572470.5 5709037 -0.0157\n44: 572490.8 5709032 -0.0080\n45: 572477.5 5709041 -0.0002\n46: 572474.1 5709035 -0.0131\n47: 572493.0 5709032 -0.0166\n48: 572477.7 5709040 -0.0088\n49: 572476.5 5709052 -0.0075\n50: 572473.1 5709038 -0.0014\n51: 572474.0 5709035 -0.0297\n           X       Y       Z\n\n# Forest structural complexity (Box dimension)\n\ncloud = las_norm@data[Z&gt;0.5, 1:3] # Here, all points above 0.5 meter and only X,Y,z coordinates \n\ndb &lt;- box_dimension(cloud = cloud, \n                    lowercutoff = 0.01, \n                    rm_int_box = FALSE, \n                    plot = FALSE )\nstr(db)\n\nList of 2\n $ :Classes 'tidytable', 'tbl', 'data.table' and 'data.frame':  13 obs. of  2 variables:\n  ..$ log.box.size: num [1:13] 0 0.693 1.386 2.079 2.773 ...\n  ..$ log.voxels  : num [1:13] 1.39 2.89 4.32 6.04 7.56 ...\n  ..- attr(*, \".internal.selfref\")=&lt;externalptr&gt; \n $ :Classes 'tidytable', 'tbl', 'data.table' and 'data.frame':  1 obs. of  4 variables:\n  ..$ r.squared    : num 0.964\n  ..$ adj.r.squared: num 0.96\n  ..$ intercept    : num 2.24\n  ..$ slope        : num 1.84\n\n# Box Dimension (slope)\ndb[[2]]$slope\n\n[1] 1.838747\n\ndb[[2]]$r.squared # show similarity\n\n[1] 0.9636752\n\n\n\n# Visualization\n# 2D Plot\nbox_dimension(cloud[, 1:3], plot = \"2D\")\n\n\n\n\n\n\n\n\n[[1]]\n# A tidytable: 13 × 2\n   log.box.size log.voxels\n          &lt;dbl&gt;      &lt;dbl&gt;\n 1        0           1.39\n 2        0.693       2.89\n 3        1.39        4.32\n 4        2.08        6.04\n 5        2.77        7.56\n 6        3.47        9.11\n 7        4.16       10.6 \n 8        4.85       12.1 \n 9        5.55       13.6 \n10        6.24       14.8 \n11        6.93       15.3 \n12        7.62       15.4 \n13        8.32       15.4 \n\n[[2]]\n# A tidytable: 1 × 4\n  r.squared adj.r.squared intercept slope\n      &lt;dbl&gt;         &lt;dbl&gt;     &lt;dbl&gt; &lt;dbl&gt;\n1     0.964         0.960      2.24  1.84\n\n# 3D Plot\nbox_dimension(cloud[, 1:3], plot = \"3D\")\n\nPanning plot on rgl device: 4\n\n\n[[1]]\n# A tidytable: 13 × 2\n   log.box.size log.voxels\n          &lt;dbl&gt;      &lt;dbl&gt;\n 1        0           1.39\n 2        0.693       2.89\n 3        1.39        4.32\n 4        2.08        6.04\n 5        2.77        7.56\n 6        3.47        9.11\n 7        4.16       10.6 \n 8        4.85       12.1 \n 9        5.55       13.6 \n10        6.24       14.8 \n11        6.93       15.3 \n12        7.62       15.4 \n13        8.32       15.4 \n\n[[2]]\n# A tidytable: 1 × 4\n  r.squared adj.r.squared intercept slope\n      &lt;dbl&gt;         &lt;dbl&gt;     &lt;dbl&gt; &lt;dbl&gt;\n1     0.964         0.960      2.24  1.84",
    "crumbs": [
      "Block 2: Processing of 3D LiDAR data to assess forest structure",
      "Processing of 3D Lidar data to assess forest structure"
    ]
  },
  {
    "objectID": "index.html#sensors-equipment",
    "href": "index.html#sensors-equipment",
    "title": "Welcome EON Summer School 2025 (31.08-05.09.2025)",
    "section": "Sensors & Equipment",
    "text": "Sensors & Equipment\n\nHAWK:\n\nMavic 3 (Enterprise + Thermal + Multispectral)\nMatrics RTK350 incl. L1, P1 & Micasence\nGNSS (Emlid, Garmin)\nTablets (Android)\nForest Measurement Devices (diameter tapes, calipers, vertex, laser range finders, …)\nGeoSlam Mobile Laser Scanner\n\nUni Münster:\n\nDrill & Drop\nDii mini\n\nUni Marburg:\n\nMavic 3 (Enterprise + Thermal + Multispectral)\nMavic 3 Mini Pro\nLoRa based real time climate sensors",
    "crumbs": [
      "Welcome",
      "Welcome EON Summer School 2025 (31.08-05.09.2025)"
    ]
  }
]