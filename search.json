[
  {
    "objectID": "index.html#sensors-equipment",
    "href": "index.html#sensors-equipment",
    "title": "Welcome EON Summer School 2025 (31.08-05.09.2025)",
    "section": "Sensors & Equipment",
    "text": "Sensors & Equipment\n\nHAWK:\n\nMavic 3 (Enterprise + Thermal + Multispectral)\nMatrics RTK350 incl. L1, P1 & Micasence\nGNSS (Emlid, Garmin)\nTablets (Android)\nForest Measurement Devices (diameter tapes, calipers, vertex, laser range finders, …)\nGeoSlam Mobile Laser Scanner\n\nUni Münster:\n\nDrill & Drop\nDii mini\n\nUni Marburg:\n\nMavic 3 (Enterprise + Thermal + Multispectral)\nMavic 3 Mini Pro\nLoRa based real time climate sensors",
    "crumbs": [
      "Welcome",
      "Welcome EON Summer School 2025 (31.08-05.09.2025)"
    ]
  },
  {
    "objectID": "block2_bas_dob/lidar_forest_structure.html",
    "href": "block2_bas_dob/lidar_forest_structure.html",
    "title": "Processing of 3D Lidar data to assess forest structure",
    "section": "",
    "text": "check out the manual of the lidR package: https://r-lidar.github.io/lidRbook/",
    "crumbs": [
      "Block 2: Processing of 3D LiDAR data to assess forest structure",
      "Processing of 3D Lidar data to assess forest structure"
    ]
  },
  {
    "objectID": "block2_bas_dob/lidar_forest_structure.html#setup",
    "href": "block2_bas_dob/lidar_forest_structure.html#setup",
    "title": "Processing of 3D Lidar data to assess forest structure",
    "section": "0. Setup",
    "text": "0. Setup\n\nInstall and load required libraries\n\n## install libraries\n# install.packges(\"lidR\")\n# install.packages(\"terra\")\n# install.packages(\"sf\")\n# install.packages(\"rTwig\")\n# install.packages('lidRviewer', repos = c('https://r-lidar.r-universe.dev'))\n\n\nlibrary(lidR)\nlibrary(terra)\nlibrary(sf)\nlibrary(lidRviewer)\nlibrary(dplyr)\nlibrary(rTwig)\n\n\n\ndownload the data\ndownload lidar point cloud and ground truth data (BI) from owncloud.\n\n# Data access\nurl_las &lt;- \"https://cloud.hawk.de/index.php/s/pB4RRmLb4Xxy4Qj/download\"\ndownload.file(url_las, destfile = \"uls_goewa.laz\", mode = \"wb\")\n\nurl_bi &lt;- \"https://cloud.hawk.de/index.php/s/5npprfZYLjg5ip5/download\"\ndownload.file(url_bi, destfile = \"trees_bi.gpkg\", mode = \"wb\")\n\n\n\nimport the data\n\nlas &lt;- readLAS(\"uls_goewa.laz\")\ntrees_bi &lt;- st_read(\"trees_bi.gpkg\")\n\nlet´s inspect the data. 1) whats the point density of the lidar data? 2) whats the total number of points and pulses and what is the difference between the two? 3) is there any classification in the point cloud? 4) how many trees were measured in the BI? 5) which tree species are present in the plot?\n\nprint(las)\n\nclass        : LAS (v1.2 format 3)\nmemory       : 313.2 Mb \nextent       : 572445.4, 572496.1, 5709020, 5709071 (xmin, xmax, ymin, ymax)\ncoord. ref.  : WGS 84 / UTM zone 32N \narea         : 2602 m²\npoints       : 5.13 million points\ntype         : terrestrial\ndensity      : 1971.94 points/m²\ndensity      : 1660.17 pulses/m²\n\nplot(las)\nprint(trees_bi)\n\nSimple feature collection with 69 features and 17 fields\nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: 572446.5 ymin: 5709021 xmax: 572495.8 ymax: 5709069\nProjected CRS: WGS 84 / UTM zone 32N\nFirst 10 features:\n   IDPlots          Name Plots.Bem ID      X_m     Y_m    Z_m Species Spec_txt\n1        1 GoeWald Fla 1      &lt;NA&gt;  1 572468.6 5709043 -0.448      22       Bu\n2        1 GoeWald Fla 1      &lt;NA&gt;  2 572467.4 5709044 -0.466      22       Bu\n3        1 GoeWald Fla 1      &lt;NA&gt;  3 572462.7 5709041 -0.443      22       Bu\n4        1 GoeWald Fla 1      &lt;NA&gt;  4 572459.8 5709039 -0.657      22       Bu\n5        1 GoeWald Fla 1      &lt;NA&gt;  5 572455.0 5709045 -0.971      22       Bu\n6        1 GoeWald Fla 1      &lt;NA&gt;  6 572455.1 5709048 -0.978      22       Bu\n7        1 GoeWald Fla 1      &lt;NA&gt;  7 572459.6 5709050 -0.723      22       Bu\n8        1 GoeWald Fla 1      &lt;NA&gt;  8 572460.5 5709052 -0.920      22       Bu\n9        1 GoeWald Fla 1      &lt;NA&gt;  9 572463.9 5709053 -0.747      22       Bu\n10       1 GoeWald Fla 1      &lt;NA&gt; 10 572468.1 5709048 -0.619      22       Bu\n   DBH_mm Vit Bruch                Schirm Schiefer.B Trees.Bem   x_lok  y_lok\n1     354 leb  nein geringe �berschirmung       Nein      &lt;NA&gt;  -2.197 -2.543\n2     345 leb  nein geringe �berschirmung       Nein      &lt;NA&gt;  -3.371 -1.518\n3     518 leb  nein geringe �berschirmung       Nein      &lt;NA&gt;  -8.087 -3.932\n4     373 leb  nein    Hohe �berschirmung       Nein      &lt;NA&gt; -11.067 -6.176\n5     350 leb  nein    Hohe �berschirmung       Nein      &lt;NA&gt; -15.776 -0.631\n6     388 leb  nein geringe �berschirmung       Nein      &lt;NA&gt; -15.576  2.703\n7     264 tot    ja                  &lt;NA&gt;       Nein      &lt;NA&gt; -11.070  4.367\n8     404 leb  nein geringe �berschirmung       Nein      &lt;NA&gt; -10.178  6.296\n9     464 leb  nein geringe �berschirmung       Nein      &lt;NA&gt;  -6.741  7.432\n10    291 leb    ja  Komplett �berschirmt       Nein      &lt;NA&gt;  -2.589  2.832\n                       geom\n1  POINT (572468.6 5709043)\n2  POINT (572467.4 5709044)\n3  POINT (572462.7 5709041)\n4  POINT (572459.8 5709039)\n5    POINT (572455 5709045)\n6  POINT (572455.1 5709048)\n7  POINT (572459.6 5709050)\n8  POINT (572460.5 5709052)\n9  POINT (572463.9 5709053)\n10 POINT (572468.1 5709048)",
    "crumbs": [
      "Block 2: Processing of 3D LiDAR data to assess forest structure",
      "Processing of 3D Lidar data to assess forest structure"
    ]
  },
  {
    "objectID": "block2_bas_dob/lidar_forest_structure.html#calculating-terrain-models",
    "href": "block2_bas_dob/lidar_forest_structure.html#calculating-terrain-models",
    "title": "Processing of 3D Lidar data to assess forest structure",
    "section": "1. Calculating Terrain Models",
    "text": "1. Calculating Terrain Models\nNext we are calculating terrain models, using triangulation (TIN = Triangulated Irregular Network). Since the data is already ground classified we can skip the classification step. The DEM is using the ground return points to interpolate the surface. In contrast, the digital surface model is using the highest lidar returns to represent the top of any object above the ground. Subtracting the two gives us the canopy height.\ncheck out the documentation of the rasterize_terrain algorithm. Try out different interpolation algorithms and different resolutions. Compare the results visually.\n\ndem &lt;- rasterize_terrain(las, res = 0.5, algorithm = tin())\ndsm &lt;- rasterize_canopy(las, res = 0.5, algorithm = dsmtin(max_edge = 8))\nchm &lt;- dsm - dem\nchm &lt;- terra::focal(chm, w = 3, fun = mean, na.rm = TRUE) # smoothing results \n\npar(mfrow = c(1,3))\nplot(dem, main = \"digital elevation model\")\nplot(dsm, main = \"digital surface model\")\nplot(chm, main = \"canopy heigt model\")\n\n\n\n\n\n\n\n#writeRaster(dem, \"./data/output/dem.tif\", overwrite=TRUE)\n#writeRaster(dsm, \"./data/output/dsm.tif\", overwrite=TRUE)\n#writeRaster(chm, \"./data/output/chm.tif\", overwrite=TRUE)",
    "crumbs": [
      "Block 2: Processing of 3D LiDAR data to assess forest structure",
      "Processing of 3D Lidar data to assess forest structure"
    ]
  },
  {
    "objectID": "block2_bas_dob/lidar_forest_structure.html#individual-tree-detection",
    "href": "block2_bas_dob/lidar_forest_structure.html#individual-tree-detection",
    "title": "Processing of 3D Lidar data to assess forest structure",
    "section": "2. Individual Tree Detection",
    "text": "2. Individual Tree Detection\nIndividual Tree Detection (ITD) is the process of spatially locating trees (f.i to extract height information). Tree tops can be detected by applying a Local Maximum Filter (LMF) on the loaded data set. The number of detected trees is correlated to the window size (ws) argument. Small windows sizes usually gives more trees, while large windows size generally miss smaller trees that are “hidden” by big trees that contain the highest points in the neighbourhood. We will use the Tree detection function with variable window size. Any points below 2 m will equate to a window size of 3 m, while points above 20 meters equate to a window size of 5 m. Anything between 2 and 20 meter will have a non-linear relationship.\n\n## Function for Local Maximum Filter with variable windows size\n\nf &lt;- function(x) {\n  y &lt;- 2.6 * (-(exp(-0.08*(x-2)) - 1)) + 3 \n  # from https://r-lidar.github.io/lidRbook/itd.html\n  y[x &lt; 2] &lt;- 3\n  y[x &gt; 20] &lt;- 5\n  return(y)\n}\n\nheights &lt;- seq(-5,35,0.5)\nws &lt;- f(heights)\n\nplot(heights, ws, type = \"l\",  ylim = c(0,5))\n\n\n\n\n\n\n\n\nLet´s run the tree detection algorithm using the user-defined ws function and the CHM created beforehand. Compare the results with the ground truth BI data. - How many of the trees could be detected in the lidar data?\n- what could be the reasons for that?\n\n#ttops &lt;- locate_trees(las, lmf(f)) # only run this if you have a fast computer! \nttops &lt;- locate_trees(chm, lmf(f)) \n\n# plot results \nplot(chm, col = height.colors(50))\nplot(sf::st_geometry(trees_bi), add = TRUE, pch = 2, col =\"blue\")\nplot(sf::st_geometry(ttops), add = TRUE, pch = 3, col = \"black\")\n\n\n\n\n\n\n\n# 3D plot\nlas_norm &lt;- normalize_height(las, knnidw()) # normalize point cloud for this vizualisation \n\nInverse distance weighting: [==================================----------------] 68% (6 threads)\nInverse distance weighting: [==================================----------------] 69% (6 threads)\nInverse distance weighting: [===================================---------------] 70% (6 threads)\nInverse distance weighting: [===================================---------------] 71% (6 threads)\nInverse distance weighting: [====================================--------------] 72% (6 threads)\nInverse distance weighting: [====================================--------------] 73% (6 threads)\nInverse distance weighting: [=====================================-------------] 74% (6 threads)\nInverse distance weighting: [=====================================-------------] 75% (6 threads)\nInverse distance weighting: [======================================------------] 76% (6 threads)\nInverse distance weighting: [======================================------------] 77% (6 threads)\nInverse distance weighting: [=======================================-----------] 78% (6 threads)\nInverse distance weighting: [=======================================-----------] 79% (6 threads)\nInverse distance weighting: [========================================----------] 80% (6 threads)\nInverse distance weighting: [========================================----------] 81% (6 threads)\nInverse distance weighting: [=========================================---------] 82% (6 threads)\nInverse distance weighting: [=========================================---------] 83% (6 threads)\nInverse distance weighting: [==========================================--------] 84% (6 threads)\nInverse distance weighting: [==========================================--------] 85% (6 threads)\nInverse distance weighting: [===========================================-------] 86% (6 threads)\nInverse distance weighting: [===========================================-------] 87% (6 threads)\nInverse distance weighting: [============================================------] 88% (6 threads)\nInverse distance weighting: [============================================------] 89% (6 threads)\nInverse distance weighting: [=============================================-----] 90% (6 threads)\nInverse distance weighting: [=============================================-----] 91% (6 threads)\nInverse distance weighting: [==============================================----] 92% (6 threads)\nInverse distance weighting: [==============================================----] 93% (6 threads)\nInverse distance weighting: [===============================================---] 94% (6 threads)\nInverse distance weighting: [===============================================---] 95% (6 threads)\nInverse distance weighting: [================================================--] 96% (6 threads)\nInverse distance weighting: [================================================--] 97% (6 threads)\nInverse distance weighting: [=================================================-] 98% (6 threads)\nInverse distance weighting: [=================================================-] 99% (6 threads)\nInverse distance weighting: [==================================================] 100% (6 threads)\n\nx &lt;- plot(las_norm, bg = \"white\", size = 4)\nadd_treetops3d(x, ttops)\n\n#writeVector(vect(ttops), \"./data/output/ttops_chm_.gpkg\", overwrite=TRUE)",
    "crumbs": [
      "Block 2: Processing of 3D LiDAR data to assess forest structure",
      "Processing of 3D Lidar data to assess forest structure"
    ]
  },
  {
    "objectID": "block2_bas_dob/lidar_forest_structure.html#individual-tree-segmentation",
    "href": "block2_bas_dob/lidar_forest_structure.html#individual-tree-segmentation",
    "title": "Processing of 3D Lidar data to assess forest structure",
    "section": "3. Individual Tree Segmentation",
    "text": "3. Individual Tree Segmentation\nIndividual Tree Segmentation (ITS) is the process of individually delineating detected trees. Even when the algorithm is raster-based (which is the case of dalponte2016()), lidR segments the point cloud and assigns an ID to each point by inserting a new attribute named treeID in the LAS object. This means that every point is associated with a particular tree.\n\nalgo &lt;- dalponte2016(chm, ttops)\nlas_seg &lt;- segment_trees(las_norm, algo) # segment point cloud\nx &lt;- plot(las_seg, bg = \"white\", size = 4, color = \"treeID\") # visualize trees\nadd_treetops3d(x, ttops)",
    "crumbs": [
      "Block 2: Processing of 3D LiDAR data to assess forest structure",
      "Processing of 3D Lidar data to assess forest structure"
    ]
  },
  {
    "objectID": "block2_bas_dob/lidar_forest_structure.html#deriving-metrics-using-the-area-based-approach",
    "href": "block2_bas_dob/lidar_forest_structure.html#deriving-metrics-using-the-area-based-approach",
    "title": "Processing of 3D Lidar data to assess forest structure",
    "section": "4. Deriving Metrics using the area-based approach",
    "text": "4. Deriving Metrics using the area-based approach\nthe Area-Based Approach (ABA) allows the creation of wall-to-wall predictions of forest inventory attributes (e.g. basal area or total volume per hectare) by linking ALS variables with field measured references.\n\nr_metr &lt;- pixel_metrics(las, res = 0.5, func = .stdmetrics)\nplot(r_metr)",
    "crumbs": [
      "Block 2: Processing of 3D LiDAR data to assess forest structure",
      "Processing of 3D Lidar data to assess forest structure"
    ]
  },
  {
    "objectID": "block2_bas_dob/lidar_forest_structure.html#forest-structural-complexity",
    "href": "block2_bas_dob/lidar_forest_structure.html#forest-structural-complexity",
    "title": "Processing of 3D Lidar data to assess forest structure",
    "section": "5.Forest structural complexity",
    "text": "5.Forest structural complexity\n(Fractal complexity analysis/ voxel-based box-count dimension or box dimension (Db) method)\nThe box dimension quantifies structural complexity of point clouds using a fractal box-counting approach.\nIt is defined as the slope of the regression between log box (voxel) count and log inverse box (voxel) size, with higher R² values indicating stronger self-similarity. Reliable estimates require high-resolution (≤1 cm) point clouds with minimal occlusion.\n\n# Read data, check and pre-process with lidR\n#data &lt;- readLAS(\"uls_goewa.laz\")\nprint(las)\n\nclass        : LAS (v1.2 format 3)\nmemory       : 313.2 Mb \nextent       : 572445.4, 572496.1, 5709020, 5709071 (xmin, xmax, ymin, ymax)\ncoord. ref.  : WGS 84 / UTM zone 32N \narea         : 2602 m²\npoints       : 5.13 million points\ntype         : terrestrial\ndensity      : 1971.94 points/m²\ndensity      : 1660.17 pulses/m²\n\nlas_check(las) \n\n\n Checking the data\n  - Checking coordinates...\u001b[0;32m ✓\u001b[0m\n  - Checking coordinates type...\u001b[0;32m ✓\u001b[0m\n  - Checking coordinates range...\u001b[0;32m ✓\u001b[0m\n  - Checking coordinates quantization...\u001b[0;32m ✓\u001b[0m\n  - Checking attributes type...\u001b[0;32m ✓\u001b[0m\n  - Checking ReturnNumber validity...\u001b[0;32m ✓\u001b[0m\n  - Checking NumberOfReturns validity...\u001b[0;32m ✓\u001b[0m\n  - Checking ReturnNumber vs. NumberOfReturns...\u001b[0;32m ✓\u001b[0m\n  - Checking RGB validity...\u001b[0;32m ✓\u001b[0m\n  - Checking absence of NAs...\u001b[0;32m ✓\u001b[0m\n  - Checking duplicated points...\u001b[0;32m ✓\u001b[0m\n  - Checking degenerated ground points...\u001b[0;32m ✓\u001b[0m\n  - Checking attribute population...\n \u001b[0;32m   🛈 'PointSourceID' attribute is not populated\u001b[0m\n \u001b[0;32m   🛈 'ScanDirectionFlag' attribute is not populated\u001b[0m\n \u001b[0;32m   🛈 'EdgeOfFlightline' attribute is not populated\u001b[0m\n  - Checking gpstime incoherances\u001b[0;32m ✓\u001b[0m\n  - Checking flag attributes...\u001b[0;32m ✓\u001b[0m\n  - Checking user data attribute...\u001b[0;32m ✓\u001b[0m\n Checking the header\n  - Checking header completeness...\u001b[0;32m ✓\u001b[0m\n  - Checking scale factor validity...\u001b[0;32m ✓\u001b[0m\n  - Checking point data format ID validity...\u001b[0;32m ✓\u001b[0m\n  - Checking extra bytes attributes validity...\u001b[0;32m ✓\u001b[0m\n  - Checking the bounding box validity...\u001b[0;32m ✓\u001b[0m\n  - Checking coordinate reference system...\u001b[0;32m ✓\u001b[0m\n Checking header vs data adequacy\n  - Checking attributes vs. point format...\u001b[0;32m ✓\u001b[0m\n  - Checking header bbox vs. actual content...\u001b[0;32m ✓\u001b[0m\n  - Checking header number of points vs. actual content...\u001b[0;32m ✓\u001b[0m\n  - Checking header return number vs. actual content...\u001b[0;32m ✓\u001b[0m\n Checking coordinate reference system...\n  - Checking if the CRS was understood by R...\u001b[0;32m ✓\u001b[0m\n Checking preprocessing already done \n  - Checking ground classification...\u001b[0;32m yes\u001b[0m\n  - Checking normalization...\u001b[0;31m no\u001b[0m\n  - Checking negative outliers...\u001b[0;32m ✓\u001b[0m\n  - Checking flightline classification...\u001b[0;31m no\u001b[0m\n Checking compression\n  - Checking attribute compression...\n   -  ScanDirectionFlag is compressed\n   -  EdgeOfFlightline is compressed\n   -  Synthetic_flag is compressed\n   -  Keypoint_flag is compressed\n   -  Withheld_flag is compressed\n   -  UserData is compressed\n   -  PointSourceID is compressed\n\nlas_norm &lt;- normalize_height(las = las, \n                        algorithm = tin(), \n                        use_class = 2)\n\nlas_check(las_norm) # check negative outliers\n\n\n Checking the data\n  - Checking coordinates...\u001b[0;32m ✓\u001b[0m\n  - Checking coordinates type...\u001b[0;32m ✓\u001b[0m\n  - Checking coordinates range...\u001b[0;32m ✓\u001b[0m\n  - Checking coordinates quantization...\u001b[0;32m ✓\u001b[0m\n  - Checking attributes type...\u001b[0;32m ✓\u001b[0m\n  - Checking ReturnNumber validity...\u001b[0;32m ✓\u001b[0m\n  - Checking NumberOfReturns validity...\u001b[0;32m ✓\u001b[0m\n  - Checking ReturnNumber vs. NumberOfReturns...\u001b[0;32m ✓\u001b[0m\n  - Checking RGB validity...\u001b[0;32m ✓\u001b[0m\n  - Checking absence of NAs...\u001b[0;32m ✓\u001b[0m\n  - Checking duplicated points...\u001b[0;32m ✓\u001b[0m\n  - Checking degenerated ground points...\u001b[0;32m ✓\u001b[0m\n  - Checking attribute population...\n \u001b[0;32m   🛈 'PointSourceID' attribute is not populated\u001b[0m\n \u001b[0;32m   🛈 'ScanDirectionFlag' attribute is not populated\u001b[0m\n \u001b[0;32m   🛈 'EdgeOfFlightline' attribute is not populated\u001b[0m\n  - Checking gpstime incoherances\u001b[0;32m ✓\u001b[0m\n  - Checking flag attributes...\u001b[0;32m ✓\u001b[0m\n  - Checking user data attribute...\u001b[0;32m ✓\u001b[0m\n Checking the header\n  - Checking header completeness...\u001b[0;32m ✓\u001b[0m\n  - Checking scale factor validity...\u001b[0;32m ✓\u001b[0m\n  - Checking point data format ID validity...\u001b[0;32m ✓\u001b[0m\n  - Checking extra bytes attributes validity...\u001b[0;32m ✓\u001b[0m\n  - Checking the bounding box validity...\u001b[0;32m ✓\u001b[0m\n  - Checking coordinate reference system...\u001b[0;32m ✓\u001b[0m\n Checking header vs data adequacy\n  - Checking attributes vs. point format...\u001b[0;32m ✓\u001b[0m\n  - Checking header bbox vs. actual content...\u001b[0;32m ✓\u001b[0m\n  - Checking header number of points vs. actual content...\u001b[0;32m ✓\u001b[0m\n  - Checking header return number vs. actual content...\u001b[0;32m ✓\u001b[0m\n Checking coordinate reference system...\n  - Checking if the CRS was understood by R...\u001b[0;32m ✓\u001b[0m\n Checking preprocessing already done \n  - Checking ground classification...\u001b[0;32m yes\u001b[0m\n  - Checking normalization...\u001b[0;32m yes\u001b[0m\n  - Checking negative outliers...\n \u001b[1;33m   ⚠ 51 points below 0\u001b[0m\n  - Checking flightline classification...\u001b[0;31m no\u001b[0m\n Checking compression\n  - Checking attribute compression...\n   -  ScanDirectionFlag is compressed\n   -  EdgeOfFlightline is compressed\n   -  Synthetic_flag is compressed\n   -  Keypoint_flag is compressed\n   -  Withheld_flag is compressed\n   -  UserData is compressed\n   -  PointSourceID is compressed\n\nview(las_norm)\n            #Rotate with left mouse button\n            #Zoom with mouse wheel\n            #Pan with right mouse button\n            #Keyboard r or g or b to color with RGB\n            #Keyboard z to color with Z\n            #Keyboard i to color with Intensity\n            #Keyboard c to color with Classification\n            #Keyboard + or - to change the point size\n            #Keyboard l to enable/disable eyes-dome lightning\n\n\nlas_norm@data[Z&lt;0, 1:3] # Here options are either remove all or assign all to 0, However...\n\n           X       Y       Z\n       &lt;num&gt;   &lt;num&gt;   &lt;num&gt;\n 1: 572471.7 5709038 -0.0183\n 2: 572469.0 5709021 -0.0044\n 3: 572475.5 5709041 -0.0315\n 4: 572477.3 5709041 -0.0004\n 5: 572475.2 5709040 -0.0013\n 6: 572476.9 5709041 -0.0088\n 7: 572459.1 5709064 -0.0035\n 8: 572477.3 5709041 -0.0041\n 9: 572460.3 5709058 -0.0034\n10: 572458.5 5709062 -0.0015\n11: 572462.5 5709046 -0.1952\n12: 572480.8 5709064 -0.0056\n13: 572483.4 5709069 -0.0163\n14: 572483.8 5709068 -0.0514\n15: 572483.8 5709068 -0.0330\n16: 572483.4 5709068 -0.0129\n17: 572488.9 5709023 -0.0133\n18: 572476.2 5709047 -0.0031\n19: 572451.9 5709020 -0.0029\n20: 572483.6 5709068 -0.0488\n21: 572484.9 5709062 -0.0040\n22: 572452.1 5709020 -0.0102\n23: 572452.0 5709021 -0.0030\n24: 572488.8 5709054 -0.0037\n25: 572476.1 5709032 -0.0070\n26: 572452.2 5709020 -0.0227\n27: 572491.1 5709069 -0.0163\n28: 572488.9 5709023 -0.0496\n29: 572483.9 5709068 -0.0246\n30: 572483.4 5709068 -0.0099\n31: 572484.7 5709063 -0.0017\n32: 572477.9 5709064 -0.0101\n33: 572478.2 5709065 -0.0129\n34: 572482.9 5709060 -0.0112\n35: 572494.0 5709053 -0.0001\n36: 572491.3 5709055 -0.0017\n37: 572476.1 5709047 -0.0046\n38: 572475.6 5709050 -0.0004\n39: 572491.0 5709039 -0.0019\n40: 572473.3 5709046 -0.0006\n41: 572478.1 5709039 -0.0100\n42: 572493.1 5709032 -0.0122\n43: 572470.5 5709037 -0.0157\n44: 572490.8 5709032 -0.0080\n45: 572477.5 5709041 -0.0002\n46: 572474.1 5709035 -0.0131\n47: 572493.0 5709032 -0.0166\n48: 572477.7 5709040 -0.0088\n49: 572476.5 5709052 -0.0075\n50: 572473.1 5709038 -0.0014\n51: 572474.0 5709035 -0.0297\n           X       Y       Z\n\n# Forest structural complexity (Box dimension)\n\ncloud = las_norm@data[Z&gt;0.5, 1:3] # Here, all points above 0.5 meter and only X,Y,z coordinates \n\ndb &lt;- box_dimension(cloud = cloud, \n                    lowercutoff = 0.01, \n                    rm_int_box = FALSE, \n                    plot = FALSE )\nstr(db)\n\nList of 2\n $ :Classes 'tidytable', 'tbl', 'data.table' and 'data.frame':  13 obs. of  2 variables:\n  ..$ log.box.size: num [1:13] 0 0.693 1.386 2.079 2.773 ...\n  ..$ log.voxels  : num [1:13] 1.39 2.89 4.32 6.04 7.56 ...\n  ..- attr(*, \".internal.selfref\")=&lt;externalptr&gt; \n $ :Classes 'tidytable', 'tbl', 'data.table' and 'data.frame':  1 obs. of  4 variables:\n  ..$ r.squared    : num 0.964\n  ..$ adj.r.squared: num 0.96\n  ..$ intercept    : num 2.24\n  ..$ slope        : num 1.84\n\n# Box Dimension (slope)\ndb[[2]]$slope\n\n[1] 1.838747\n\ndb[[2]]$r.squared # show similarity\n\n[1] 0.9636752\n\n\n\n# Visualization\n# 2D Plot\nbox_dimension(cloud[, 1:3], plot = \"2D\")\n\n\n\n\n\n\n\n\n[[1]]\n# A tidytable: 13 × 2\n   log.box.size log.voxels\n          &lt;dbl&gt;      &lt;dbl&gt;\n 1        0           1.39\n 2        0.693       2.89\n 3        1.39        4.32\n 4        2.08        6.04\n 5        2.77        7.56\n 6        3.47        9.11\n 7        4.16       10.6 \n 8        4.85       12.1 \n 9        5.55       13.6 \n10        6.24       14.8 \n11        6.93       15.3 \n12        7.62       15.4 \n13        8.32       15.4 \n\n[[2]]\n# A tidytable: 1 × 4\n  r.squared adj.r.squared intercept slope\n      &lt;dbl&gt;         &lt;dbl&gt;     &lt;dbl&gt; &lt;dbl&gt;\n1     0.964         0.960      2.24  1.84\n\n# 3D Plot\nbox_dimension(cloud[, 1:3], plot = \"3D\")\n\nPanning plot on rgl device: 4\n\n\n[[1]]\n# A tidytable: 13 × 2\n   log.box.size log.voxels\n          &lt;dbl&gt;      &lt;dbl&gt;\n 1        0           1.39\n 2        0.693       2.89\n 3        1.39        4.32\n 4        2.08        6.04\n 5        2.77        7.56\n 6        3.47        9.11\n 7        4.16       10.6 \n 8        4.85       12.1 \n 9        5.55       13.6 \n10        6.24       14.8 \n11        6.93       15.3 \n12        7.62       15.4 \n13        8.32       15.4 \n\n[[2]]\n# A tidytable: 1 × 4\n  r.squared adj.r.squared intercept slope\n      &lt;dbl&gt;         &lt;dbl&gt;     &lt;dbl&gt; &lt;dbl&gt;\n1     0.964         0.960      2.24  1.84",
    "crumbs": [
      "Block 2: Processing of 3D LiDAR data to assess forest structure",
      "Processing of 3D Lidar data to assess forest structure"
    ]
  },
  {
    "objectID": "block1_magdon/01_TrainingDataCollection.html",
    "href": "block1_magdon/01_TrainingDataCollection.html",
    "title": "Collection of training data for remote sensing model building",
    "section": "",
    "text": "#install.packages(\"devtools\")\n#devtools::install_github(\"bleutner/RStoolbox\")\nlibrary(sf)\n\nLinking to GEOS 3.13.1, GDAL 3.11.0, PROJ 9.6.0; sf_use_s2() is TRUE\n\nlibrary(RStoolbox)\n\nThis is version 1.0.2.1 of RStoolbox\n\nlibrary(terra)\n\nterra 1.8.60\n\nlibrary(ggplot2)\nlibrary(mapview)\nlibrary(kableExtra)\nlibrary(dplyr)\n\n\nAttache Paket: 'dplyr'\n\n\nDas folgende Objekt ist maskiert 'package:kableExtra':\n\n    group_rows\n\n\nDie folgenden Objekte sind maskiert von 'package:terra':\n\n    intersect, union\n\n\nDie folgenden Objekte sind maskiert von 'package:stats':\n\n    filter, lag\n\n\nDie folgenden Objekte sind maskiert von 'package:base':\n\n    intersect, setdiff, setequal, union\n\nlibrary(rprojroot)\nlibrary(patchwork)\n\n\nAttache Paket: 'patchwork'\n\n\nDas folgende Objekt ist maskiert 'package:terra':\n\n    area\n\nlibrary(rmarkdown)\nlibrary(tidyr)\n\n\nAttache Paket: 'tidyr'\n\n\nDas folgende Objekt ist maskiert 'package:terra':\n\n    extract\n\nlibrary(tibble)",
    "crumbs": [
      "Block 1: Reference Data Collection",
      "Collection of training data for remote sensing modelling based on spectral variability"
    ]
  },
  {
    "objectID": "block1_magdon/01_TrainingDataCollection.html#dimension-reduction-pca",
    "href": "block1_magdon/01_TrainingDataCollection.html#dimension-reduction-pca",
    "title": "Collection of training data for remote sensing model building",
    "section": "Dimension reduction (PCA)",
    "text": "Dimension reduction (PCA)\nIn a fist step we reduce the dimensions of the 9 Sentinel-2 bands while maintaining most of the information, using a principal component analysis (PCA).\n\n# Calculation of the principlal components using the RStoolbox\npca&lt;-RStoolbox::rasterPCA(s2,nSamples = 5000, spca=TRUE )\n\n\n|---------|---------|---------|---------|\n=========================================\n                                          \n\n# Extracting the first three components\nrgb_raster &lt;- subset(pca$map, 1:3)\n\n# Function to scale the pixel values to 0-255\nscale_fun &lt;- function(x) {\n  # Calculation of the 2% and 98% quantile\n  q &lt;- quantile(x, c(0.02, 0.98), na.rm = TRUE)\n  \n  # scaling the values\n  x &lt;- (x - q[1]) / (q[2] - q[1]) * 255\n  \n  # restrict the values to 0-255\n  x &lt;- pmin(pmax(x, 0), 255)\n  \n  return(x)\n}\n\n# Scaling of each band\nfor (i in 1:3) {\n  rgb_raster[[i]] &lt;- app(rgb_raster[[i]], scale_fun)\n}\n\n# Plot the first three principal components as RGB\nplotRGB(rgb_raster, r = 1, g = 2, b = 3)\n\n\n\n\n\n\n\n# Show importance of componentes\nsummary(pca$model)\n\nImportance of components:\n                          Comp.1    Comp.2    Comp.3     Comp.4     Comp.5\nStandard deviation     2.2346500 1.8079296 0.6737201 0.38366893 0.26048690\nProportion of Variance 0.5548512 0.3631788 0.0504332 0.01635576 0.00753927\nCumulative Proportion  0.5548512 0.9180300 0.9684632 0.98481898 0.99235825\n                            Comp.6      Comp.7       Comp.8       Comp.9\nStandard deviation     0.177485712 0.162607669 0.0812795411 0.0650151820\nProportion of Variance 0.003500131 0.002937917 0.0007340404 0.0004696638\nCumulative Proportion  0.995858379 0.998796296 0.9995303362 1.0000000000\n\n\nFrom the output of the PCA we see that we can capture 92% of the variability with the first two components. Thus we will only use the PC1 and PC2 for the subsequent analysis.",
    "crumbs": [
      "Block 1: Reference Data Collection",
      "Collection of training data for remote sensing modelling based on spectral variability"
    ]
  },
  {
    "objectID": "block1_magdon/01_TrainingDataCollection.html#unsupervised-clustering",
    "href": "block1_magdon/01_TrainingDataCollection.html#unsupervised-clustering",
    "title": "Collection of training data for remote sensing model building",
    "section": "Unsupervised clustering",
    "text": "Unsupervised clustering\nIn the next step we run an unsupervised classification of the PC1 and PC2 to get a clustered map. For the unsupervised classification we need to take a decision on the number of classes/clusters to be created. Here we will take \\(n=5\\) classes. However, depending on the target variable this value need to be adjusted.\n\nset.seed(2222)\ncluster &lt;- RStoolbox::unsuperClass(pca$map[[c('PC1','PC2')]], nSamples = 100, nClasses = 5, nStarts = 5)\n\n\n## Plots\ncolors &lt;- rainbow(5)\nplot(cluster$map, col = colors, legend = TRUE, axes = TRUE, box =TRUE)\n\n\n\n\n\n\n\n\nThe map shows a clear spatial patterns related to the elevation, tree species and vitality status of the National Park forests.",
    "crumbs": [
      "Block 1: Reference Data Collection",
      "Collection of training data for remote sensing modelling based on spectral variability"
    ]
  },
  {
    "objectID": "block1_magdon/01_TrainingDataCollection.html#implement-a-plot-design",
    "href": "block1_magdon/01_TrainingDataCollection.html#implement-a-plot-design",
    "title": "Collection of training data for remote sensing model building",
    "section": "Implement a plot design",
    "text": "Implement a plot design\nTo extract pixel values for the sample location we need to define a plot design. For this exercise we will simulate a circular fixed area plot with a radius of 13 m.\n\n# Create a training by extracting the mean value of all pixels touching\n# a buffered area with 13m around the plot center\n\nplots &lt;- sf::st_buffer(sf_samples,dist = 13)\ntrain&lt;-terra::extract(s2,plots,fun='mean',bind=FALSE,na.rm=TRUE)\n\nplots &lt;- plots %&gt;% mutate(ID=row_number())\ntrain &lt;- plots %&gt;% left_join(train, by= \"ID\")\nmapview::mapview(train, zcol=\"class_unsupervised\",\n        map.types = c(\"Esri.WorldShadedRelief\", \"OpenStreetMap.DE\"))+\n  mapview(np_boundary,alpha.regions = 0.2, aplha = 1)",
    "crumbs": [
      "Block 1: Reference Data Collection",
      "Collection of training data for remote sensing modelling based on spectral variability"
    ]
  },
  {
    "objectID": "block1_magdon/02_ValidationDataCollection.html",
    "href": "block1_magdon/02_ValidationDataCollection.html",
    "title": "Collection of validation data in the context of remote sensing based forest monitoring",
    "section": "",
    "text": "rm(list=ls())\nlibrary(sf)\n\nLinking to GEOS 3.13.1, GDAL 3.11.0, PROJ 9.6.0; sf_use_s2() is TRUE\n\nlibrary(terra)\n\nterra 1.8.60\n\nlibrary(ggplot2)\nlibrary(rprojroot)\nlibrary(patchwork)\n\n\nAttache Paket: 'patchwork'\n\n\nDas folgende Objekt ist maskiert 'package:terra':\n\n    area\n\ndata_dir=paste0(find_rstudio_root_file(),\"/block1_magdon/data/\")",
    "crumbs": [
      "Block 1: Reference Data Collection",
      "Validation of continuous maps using design-based sampling methods"
    ]
  },
  {
    "objectID": "block1_magdon/02_ValidationDataCollection.html#systematic-sample-to-collect-reference-data-for-map-validation",
    "href": "block1_magdon/02_ValidationDataCollection.html#systematic-sample-to-collect-reference-data-for-map-validation",
    "title": "Collection of validation data in the context of remote sensing based forest monitoring",
    "section": "Systematic sample to collect reference data for map validation",
    "text": "Systematic sample to collect reference data for map validation\nTo validate the map we use a systematic sample grid. In a real world application we do not know the true population values. Therefore, field work would be needed to collect reference data at the selected sample points. In this workshop we assume that the agp_pop map represents the true value without any errors. Thus, we don’t need to go to field but we can sample the data by extracting the true values from the map at the sample locations.\n\n# we will use n=100 sample plots\nn=100\np1 = sf::st_sample(np_boundary,size=n,type='regular')\n\nggplot()+geom_sf(data=np_boundary,fill=NA)+\n  geom_sf(data=p1)\n\n\n\n\n\n\n\n\nAt each sample point we extract the predicted and observed AGB value.\n\nobs &lt;- terra::extract(agb_pop,vect(p1))\nnames(obs)&lt;-c('ID','obs')\n\npred &lt;- terra::extract(agb_model,vect(p1))\nnames(pred)&lt;-c('ID','pred')\nvalidation&lt;-data.frame(observed=obs$obs, predicted=pred$pred)\n\n# we need to remove the na values from this dataframe. In real world applications\n# such NA values can,  occur for example at inaccessible field plots.\n\nvalidation&lt;-validation[complete.cases(validation),]",
    "crumbs": [
      "Block 1: Reference Data Collection",
      "Validation of continuous maps using design-based sampling methods"
    ]
  },
  {
    "objectID": "block1_magdon/02_ValidationDataCollection.html#assessment-of-the-abg-model-performance",
    "href": "block1_magdon/02_ValidationDataCollection.html#assessment-of-the-abg-model-performance",
    "title": "Collection of validation data in the context of remote sensing based forest monitoring",
    "section": "Assessment of the ABG-model performance",
    "text": "Assessment of the ABG-model performance\n\nggplot(data=validation,aes(x=observed, y=predicted))+\n  geom_point(alpha=0.5)+\n  xlab('Observed AGB t/ha')+ylab('Predicted AGB t/ha')\n\n\n\n\n\n\n\n\n\nSample RMSE\nAgain we can use the RMSE to express the mean difference between observed and predicted AGB.\n\nRMSE_sample = sqrt(sum((validation$observed-validation$predicted)^2)/nrow(validation))\n\nThe sample RMSE is 38.89* t/ha. To better compare the values between different target variables and models is can also express as a proportion relative to the mean value of the predictions.\n\nrRMSE = RMSE_sample/mean(validation$predicted)\n\nOn average we expect that the AGB estimate of our model has an error of 24.1 %.\n\n\nError distribution\nBut is this RMSE valid for the entire range of the observed values or do we expect higher errors for higher AGB values?\nTo see how the model performs over target value range we can use the following analysis plots.\n\nvalidation$resid&lt;-validation$observed-validation$predicted\n\np1&lt;-ggplot(data=validation,aes(x=observed, y=predicted))+\n  geom_point(alpha=0.5)+\n  xlab('Observed AGB t/ha')+ylab('Predicted AGB t/ha')+\n  xlim(0,250)+ylim(0,250)+\n  geom_abline(slope=1,intercept = 0)+\n  stat_summary(fun.data= mean_cl_normal) + \n  geom_smooth(method='lm')\n\n\n\np2&lt;-ggplot(data=validation,aes(x=observed, y=resid))+\n  geom_point(alpha=0.5)+\n  xlab('Observed AGB t/ha')+ylab('Residuals')+\n  xlim(0,250)+ylim(-50,+50)+\n  geom_abline(slope=0,intercept = 1)\n\np3&lt;-ggplot(data=validation,aes(x=resid))+\n  geom_histogram(aes(y=..density..),fill='grey',binwidth=10)+\n  xlab('Observed AGB t/ha')+ylab('Density')+\n  xlim(-150,150)+\n  stat_function(fun = dnorm, geom=\"polygon\",args = list(mean = mean(validation$resid), sd = sd(validation$resid)),color='blue',alpha=0.4,fill='blue')+\n  geom_vline(xintercept=0,color='blue')+\n  geom_vline(xintercept=mean(validation$resid),color='red')\np1+p2+p3+plot_layout(ncol=3)\n\nWarning: Removed 1 row containing non-finite outside the scale range\n(`stat_summary()`).\n\n\nWarning: Computation failed in `stat_summary()`.\nCaused by error in `fun.data()`:\n! The package \"Hmisc\" is required.\n\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\nWarning: Removed 1 row containing non-finite outside the scale range\n(`stat_smooth()`).\n\n\nWarning: Removed 1 row containing missing values or values outside the scale range\n(`geom_point()`).\n\n\nWarning: Removed 18 rows containing missing values or values outside the scale range\n(`geom_point()`).\n\n\nWarning: The dot-dot notation (`..density..`) was deprecated in ggplot2 3.4.0.\nℹ Please use `after_stat(density)` instead.\n\n\nWarning: Removed 2 rows containing missing values or values outside the scale range\n(`geom_bar()`).",
    "crumbs": [
      "Block 1: Reference Data Collection",
      "Validation of continuous maps using design-based sampling methods"
    ]
  },
  {
    "objectID": "block6/index.html",
    "href": "block6/index.html",
    "title": "Materials",
    "section": "",
    "text": "The workshop slides are at https://jakubnowosad.com/eon2025/.\n\n\n\nParticipants are expected to have a working recent version of R and RStudio installed, along with several R packages listed below.\n\nR: https://cloud.r-project.org/\nRStudio: https://posit.co/download/rstudio-desktop/#download\n\ninstall.packages(\"remotes\")\npkg_list = c(\"terra\", \"sf\", \"landscapemetrics\", \"motif\", \"tidyr\", \"dplyr\")\nremotes::install_cran(pkg_list)\n\n\n\nThe slides are accompanied by practical exercises. The best way to get them is to download the exercises repository as a ZIP file from https://github.com/Nowosad/eon2025-exercises/archive/refs/heads/main.zip and unpack it on your computer. Then, you can open the .Rproj file and start working on the exercises in RStudio.",
    "crumbs": [
      "Block 5: Understanding spatial patterns: how to measure and compare them",
      "Materials"
    ]
  },
  {
    "objectID": "block6/index.html#slides",
    "href": "block6/index.html#slides",
    "title": "Materials",
    "section": "",
    "text": "The workshop slides are at https://jakubnowosad.com/eon2025/.",
    "crumbs": [
      "Block 5: Understanding spatial patterns: how to measure and compare them",
      "Materials"
    ]
  },
  {
    "objectID": "block6/index.html#prerequisites",
    "href": "block6/index.html#prerequisites",
    "title": "Materials",
    "section": "",
    "text": "Participants are expected to have a working recent version of R and RStudio installed, along with several R packages listed below.\n\nR: https://cloud.r-project.org/\nRStudio: https://posit.co/download/rstudio-desktop/#download\n\ninstall.packages(\"remotes\")\npkg_list = c(\"terra\", \"sf\", \"landscapemetrics\", \"motif\", \"tidyr\", \"dplyr\")\nremotes::install_cran(pkg_list)",
    "crumbs": [
      "Block 5: Understanding spatial patterns: how to measure and compare them",
      "Materials"
    ]
  },
  {
    "objectID": "block6/index.html#exercises",
    "href": "block6/index.html#exercises",
    "title": "Materials",
    "section": "",
    "text": "The slides are accompanied by practical exercises. The best way to get them is to download the exercises repository as a ZIP file from https://github.com/Nowosad/eon2025-exercises/archive/refs/heads/main.zip and unpack it on your computer. Then, you can open the .Rproj file and start working on the exercises in RStudio.",
    "crumbs": [
      "Block 5: Understanding spatial patterns: how to measure and compare them",
      "Materials"
    ]
  }
]